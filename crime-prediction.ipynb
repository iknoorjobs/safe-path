{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('COBRA-2019.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original unprocessed Dataset\n",
    "\n",
    "##### Orginal Features include:\n",
    "\n",
    "Report Number\n",
    "Occur Date\t\n",
    "Occur Time\t\n",
    "Possible Date\t\n",
    "Possible Time\t\n",
    "Beat\t\n",
    "Apartment Office Prefix\t\n",
    "Apartment Number\t\n",
    "Location\t\n",
    "Shift Occurrence\t\n",
    "Location Type\t\n",
    "UCR Literal\t\n",
    "UCR #\t\n",
    "IBR Code\t\n",
    "Neighborhood\t\n",
    "NPU\t\n",
    "Latitude\t\n",
    "Longitude\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Report Number</th>\n",
       "      <th>Report Date</th>\n",
       "      <th>Occur Date</th>\n",
       "      <th>Occur Time</th>\n",
       "      <th>Possible Date</th>\n",
       "      <th>Possible Time</th>\n",
       "      <th>Beat</th>\n",
       "      <th>Apartment Office Prefix</th>\n",
       "      <th>Apartment Number</th>\n",
       "      <th>Location</th>\n",
       "      <th>Shift Occurrence</th>\n",
       "      <th>Location Type</th>\n",
       "      <th>UCR Literal</th>\n",
       "      <th>UCR #</th>\n",
       "      <th>IBR Code</th>\n",
       "      <th>Neighborhood</th>\n",
       "      <th>NPU</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10898</th>\n",
       "      <td>191710478</td>\n",
       "      <td>2019-06-20</td>\n",
       "      <td>2019-06-20</td>\n",
       "      <td>0646</td>\n",
       "      <td>2019-06-20</td>\n",
       "      <td>650</td>\n",
       "      <td>309.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2461 METROPOLITAN PKWY SW</td>\n",
       "      <td>Day Watch</td>\n",
       "      <td>23</td>\n",
       "      <td>LARCENY-FROM VEHICLE</td>\n",
       "      <td>640</td>\n",
       "      <td>2305</td>\n",
       "      <td>Perkerson</td>\n",
       "      <td>X</td>\n",
       "      <td>33.68783</td>\n",
       "      <td>-84.40888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10899</th>\n",
       "      <td>191710383</td>\n",
       "      <td>2019-06-20</td>\n",
       "      <td>2019-06-20</td>\n",
       "      <td>0448</td>\n",
       "      <td>2019-06-20</td>\n",
       "      <td>448</td>\n",
       "      <td>406.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>1135 DOLPHIN DR SW</td>\n",
       "      <td>Morning Watch</td>\n",
       "      <td>18</td>\n",
       "      <td>AUTO THEFT</td>\n",
       "      <td>710</td>\n",
       "      <td>2404</td>\n",
       "      <td>Cascade Avenue/Road</td>\n",
       "      <td>S</td>\n",
       "      <td>33.72393</td>\n",
       "      <td>-84.46186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10900</th>\n",
       "      <td>191701424</td>\n",
       "      <td>2019-06-20</td>\n",
       "      <td>2019-06-19</td>\n",
       "      <td>1530</td>\n",
       "      <td>2019-06-19</td>\n",
       "      <td>1545</td>\n",
       "      <td>103.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1736 MARIETTA RD NW</td>\n",
       "      <td>Evening Watch</td>\n",
       "      <td>18</td>\n",
       "      <td>HOMICIDE</td>\n",
       "      <td>110</td>\n",
       "      <td>0903</td>\n",
       "      <td>Hills Park</td>\n",
       "      <td>D</td>\n",
       "      <td>33.80272</td>\n",
       "      <td>-84.44344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10901</th>\n",
       "      <td>191710357</td>\n",
       "      <td>2019-06-20</td>\n",
       "      <td>2019-06-20</td>\n",
       "      <td>0407</td>\n",
       "      <td>2019-06-20</td>\n",
       "      <td>407</td>\n",
       "      <td>413.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2310 FAIRWAY CIR SW</td>\n",
       "      <td>Morning Watch</td>\n",
       "      <td>20</td>\n",
       "      <td>AGG ASSAULT</td>\n",
       "      <td>410</td>\n",
       "      <td>1314</td>\n",
       "      <td>Fairway Acres</td>\n",
       "      <td>P</td>\n",
       "      <td>33.68852</td>\n",
       "      <td>-84.53597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10902</th>\n",
       "      <td>191710284</td>\n",
       "      <td>2019-06-20</td>\n",
       "      <td>2019-06-20</td>\n",
       "      <td>0249</td>\n",
       "      <td>2019-06-20</td>\n",
       "      <td>253</td>\n",
       "      <td>613.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C-</td>\n",
       "      <td>1529 PIEDMONT AVE NE</td>\n",
       "      <td>Morning Watch</td>\n",
       "      <td>9</td>\n",
       "      <td>BURGLARY-NONRES</td>\n",
       "      <td>512</td>\n",
       "      <td>2203</td>\n",
       "      <td>Morningside/Lenox Park</td>\n",
       "      <td>F</td>\n",
       "      <td>33.79634</td>\n",
       "      <td>-84.36956</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Report Number Report Date  Occur Date Occur Time Possible Date  \\\n",
       "10898      191710478  2019-06-20  2019-06-20       0646    2019-06-20   \n",
       "10899      191710383  2019-06-20  2019-06-20       0448    2019-06-20   \n",
       "10900      191701424  2019-06-20  2019-06-19       1530    2019-06-19   \n",
       "10901      191710357  2019-06-20  2019-06-20       0407    2019-06-20   \n",
       "10902      191710284  2019-06-20  2019-06-20       0249    2019-06-20   \n",
       "\n",
       "       Possible Time   Beat Apartment Office Prefix Apartment Number  \\\n",
       "10898            650  309.0                     NaN              NaN   \n",
       "10899            448  406.0                     NaN               10   \n",
       "10900           1545  103.0                     NaN              NaN   \n",
       "10901            407  413.0                     NaN              NaN   \n",
       "10902            253  613.0                     NaN               C-   \n",
       "\n",
       "                        Location Shift Occurrence Location Type  \\\n",
       "10898  2461 METROPOLITAN PKWY SW        Day Watch            23   \n",
       "10899         1135 DOLPHIN DR SW    Morning Watch            18   \n",
       "10900        1736 MARIETTA RD NW    Evening Watch            18   \n",
       "10901       2310 FAIRWAY CIR SW     Morning Watch            20   \n",
       "10902       1529 PIEDMONT AVE NE    Morning Watch             9   \n",
       "\n",
       "                UCR Literal  UCR # IBR Code            Neighborhood NPU  \\\n",
       "10898  LARCENY-FROM VEHICLE    640     2305               Perkerson   X   \n",
       "10899            AUTO THEFT    710     2404     Cascade Avenue/Road   S   \n",
       "10900              HOMICIDE    110     0903              Hills Park   D   \n",
       "10901           AGG ASSAULT    410     1314           Fairway Acres   P   \n",
       "10902       BURGLARY-NONRES    512     2203  Morningside/Lenox Park   F   \n",
       "\n",
       "       Latitude  Longitude  \n",
       "10898  33.68783  -84.40888  \n",
       "10899  33.72393  -84.46186  \n",
       "10900  33.80272  -84.44344  \n",
       "10901  33.68852  -84.53597  \n",
       "10902  33.79634  -84.36956  "
      ]
     },
     "execution_count": 406,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useless features Removal\n",
    "Following are the useless features which needs to be removed from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df[\"Report Number\"]\n",
    "del df[\"Possible Time\"]\n",
    "del df[\"Apartment Office Prefix\"]\n",
    "del df[\"Report Date\"]\n",
    "del df[\"Apartment Number\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df[\"Possible Date\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Garbage value \"T\" is removed from the Occur Time feature column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df[\"Occur Time\"] != \"T\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time conversion \n",
    "Time is not given in proper format as it is in the form of string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_sting_middle(str):\n",
    "    return str[:2] + \":\" + str[2:] + \":00\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Occur Time\"]=df[\"Occur Time\"].apply(insert_sting_middle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timestamp conversion of both Date and Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Occur Time\"]=pd.to_datetime(df[\"Occur Time\"], format='%H:%M:%S').dt.time\n",
    "# df[\"Possible Date\"]=pd.to_datetime(df[\"Possible Date\"],format = '%Y/%m/%d')\n",
    "df[\"Occur Date\"]=pd.to_datetime(df[\"Occur Date\"],format = '%Y/%m/%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final table below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Occur Date</th>\n",
       "      <th>Occur Time</th>\n",
       "      <th>Beat</th>\n",
       "      <th>Location</th>\n",
       "      <th>Shift Occurrence</th>\n",
       "      <th>Location Type</th>\n",
       "      <th>UCR Literal</th>\n",
       "      <th>UCR #</th>\n",
       "      <th>IBR Code</th>\n",
       "      <th>Neighborhood</th>\n",
       "      <th>NPU</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>00:20:00</td>\n",
       "      <td>511.0</td>\n",
       "      <td>50 UPPER ALABAMA ST SW</td>\n",
       "      <td>Morning Watch</td>\n",
       "      <td>13</td>\n",
       "      <td>LARCENY-NON VEHICLE</td>\n",
       "      <td>620</td>\n",
       "      <td>2302</td>\n",
       "      <td>Downtown</td>\n",
       "      <td>M</td>\n",
       "      <td>33.75194</td>\n",
       "      <td>-84.38964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>01:20:00</td>\n",
       "      <td>511.0</td>\n",
       "      <td>20 BROAD ST</td>\n",
       "      <td>Morning Watch</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LARCENY-NON VEHICLE</td>\n",
       "      <td>620</td>\n",
       "      <td>2302</td>\n",
       "      <td>Downtown</td>\n",
       "      <td>M</td>\n",
       "      <td>33.75312</td>\n",
       "      <td>-84.39208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>17:40:00</td>\n",
       "      <td>411.0</td>\n",
       "      <td>3000 CONTINENTAL COLONY PKWY SW</td>\n",
       "      <td>Evening Watch</td>\n",
       "      <td>26</td>\n",
       "      <td>LARCENY-NON VEHICLE</td>\n",
       "      <td>620</td>\n",
       "      <td>2302</td>\n",
       "      <td>Greenbriar</td>\n",
       "      <td>R</td>\n",
       "      <td>33.68077</td>\n",
       "      <td>-84.49370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>04:15:00</td>\n",
       "      <td>607.0</td>\n",
       "      <td>1362 BOULEVARD SE</td>\n",
       "      <td>Morning Watch</td>\n",
       "      <td>23</td>\n",
       "      <td>LARCENY-NON VEHICLE</td>\n",
       "      <td>630</td>\n",
       "      <td>2303</td>\n",
       "      <td>Benteen Park</td>\n",
       "      <td>W</td>\n",
       "      <td>33.71744</td>\n",
       "      <td>-84.36818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>14:00:00</td>\n",
       "      <td>210.0</td>\n",
       "      <td>3393 PEACHTREE RD NE @LENOX MALL</td>\n",
       "      <td>Evening Watch</td>\n",
       "      <td>8</td>\n",
       "      <td>LARCENY-NON VEHICLE</td>\n",
       "      <td>630</td>\n",
       "      <td>2303</td>\n",
       "      <td>Lenox</td>\n",
       "      <td>B</td>\n",
       "      <td>33.84676</td>\n",
       "      <td>-84.36212</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Occur Date Occur Time   Beat                          Location  \\\n",
       "0 2019-01-01   00:20:00  511.0            50 UPPER ALABAMA ST SW   \n",
       "1 2019-01-01   01:20:00  511.0                       20 BROAD ST   \n",
       "2 2019-01-01   17:40:00  411.0   3000 CONTINENTAL COLONY PKWY SW   \n",
       "3 2019-01-01   04:15:00  607.0                 1362 BOULEVARD SE   \n",
       "4 2019-01-01   14:00:00  210.0  3393 PEACHTREE RD NE @LENOX MALL   \n",
       "\n",
       "  Shift Occurrence Location Type          UCR Literal  UCR # IBR Code  \\\n",
       "0    Morning Watch            13  LARCENY-NON VEHICLE    620     2302   \n",
       "1    Morning Watch           NaN  LARCENY-NON VEHICLE    620     2302   \n",
       "2    Evening Watch            26  LARCENY-NON VEHICLE    620     2302   \n",
       "3    Morning Watch            23  LARCENY-NON VEHICLE    630     2303   \n",
       "4    Evening Watch             8  LARCENY-NON VEHICLE    630     2303   \n",
       "\n",
       "   Neighborhood NPU  Latitude  Longitude  \n",
       "0      Downtown   M  33.75194  -84.38964  \n",
       "1      Downtown   M  33.75312  -84.39208  \n",
       "2    Greenbriar   R  33.68077  -84.49370  \n",
       "3  Benteen Park   W  33.71744  -84.36818  \n",
       "4         Lenox   B  33.84676  -84.36212  "
      ]
     },
     "execution_count": 416,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation of the number of crimes of each category\n",
    "Bargraph representation of all the crimes in Atlanta in 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LARCENY-FROM VEHICLE    3907\n",
       "LARCENY-NON VEHICLE     2781\n",
       "AUTO THEFT              1399\n",
       "BURGLARY-RESIDENCE       975\n",
       "AGG ASSAULT              934\n",
       "BURGLARY-NONRES          374\n",
       "ROBBERY-PEDESTRIAN       361\n",
       "ROBBERY-COMMERCIAL        69\n",
       "ROBBERY-RESIDENCE         51\n",
       "HOMICIDE                  50\n",
       "MANSLAUGHTER               1\n",
       "Name: UCR Literal, dtype: int64"
      ]
     },
     "execution_count": 417,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"UCR Literal\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['UCR Literal'].value_counts().plot(kind='bar')\n",
    "plt.title('Crimes Committed by Type')\n",
    "plt.xlabel('Offense Type')\n",
    "plt.ylabel('Total Crimes (2019)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stats of number of incidents happening during different periods of the day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Evening Watch    3974\n",
       "Day Watch        2959\n",
       "Morning Watch    2606\n",
       "Unknown           251\n",
       "Name: Shift Occurrence, dtype: int64"
      ]
     },
     "execution_count": 446,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Shift Occurrence\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering of DateTime\n",
    "Hour, Minute, Day of the Month, Day of the Week are extracted from DateTime feature using Pandas library. All these features can independently contribute as usefull features for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"DateTime\"] = pd.to_datetime(df[\"Occur Date\"].astype(str)+' '+df[\"Occur Time\"].astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['DateTime'] = pd.to_datetime(df.DateTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Hour'] = df.DateTime.dt.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Minute'] = df.DateTime.dt.minute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Month'] = df.DateTime.dt.month\n",
    "#df['Year'] = df.DateTime.dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Occur Date</th>\n",
       "      <th>Occur Time</th>\n",
       "      <th>Beat</th>\n",
       "      <th>Location</th>\n",
       "      <th>Shift Occurrence</th>\n",
       "      <th>Location Type</th>\n",
       "      <th>UCR Literal</th>\n",
       "      <th>UCR #</th>\n",
       "      <th>IBR Code</th>\n",
       "      <th>Neighborhood</th>\n",
       "      <th>NPU</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>DateTime</th>\n",
       "      <th>Hour</th>\n",
       "      <th>Minute</th>\n",
       "      <th>Month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>00:20:00</td>\n",
       "      <td>511.0</td>\n",
       "      <td>50 UPPER ALABAMA ST SW</td>\n",
       "      <td>Morning Watch</td>\n",
       "      <td>13</td>\n",
       "      <td>LARCENY-NON VEHICLE</td>\n",
       "      <td>620</td>\n",
       "      <td>2302</td>\n",
       "      <td>Downtown</td>\n",
       "      <td>M</td>\n",
       "      <td>33.75194</td>\n",
       "      <td>-84.38964</td>\n",
       "      <td>2019-01-01 00:20:00</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>01:20:00</td>\n",
       "      <td>511.0</td>\n",
       "      <td>20 BROAD ST</td>\n",
       "      <td>Morning Watch</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LARCENY-NON VEHICLE</td>\n",
       "      <td>620</td>\n",
       "      <td>2302</td>\n",
       "      <td>Downtown</td>\n",
       "      <td>M</td>\n",
       "      <td>33.75312</td>\n",
       "      <td>-84.39208</td>\n",
       "      <td>2019-01-01 01:20:00</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>17:40:00</td>\n",
       "      <td>411.0</td>\n",
       "      <td>3000 CONTINENTAL COLONY PKWY SW</td>\n",
       "      <td>Evening Watch</td>\n",
       "      <td>26</td>\n",
       "      <td>LARCENY-NON VEHICLE</td>\n",
       "      <td>620</td>\n",
       "      <td>2302</td>\n",
       "      <td>Greenbriar</td>\n",
       "      <td>R</td>\n",
       "      <td>33.68077</td>\n",
       "      <td>-84.49370</td>\n",
       "      <td>2019-01-01 17:40:00</td>\n",
       "      <td>17</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>04:15:00</td>\n",
       "      <td>607.0</td>\n",
       "      <td>1362 BOULEVARD SE</td>\n",
       "      <td>Morning Watch</td>\n",
       "      <td>23</td>\n",
       "      <td>LARCENY-NON VEHICLE</td>\n",
       "      <td>630</td>\n",
       "      <td>2303</td>\n",
       "      <td>Benteen Park</td>\n",
       "      <td>W</td>\n",
       "      <td>33.71744</td>\n",
       "      <td>-84.36818</td>\n",
       "      <td>2019-01-01 04:15:00</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>14:00:00</td>\n",
       "      <td>210.0</td>\n",
       "      <td>3393 PEACHTREE RD NE @LENOX MALL</td>\n",
       "      <td>Evening Watch</td>\n",
       "      <td>8</td>\n",
       "      <td>LARCENY-NON VEHICLE</td>\n",
       "      <td>630</td>\n",
       "      <td>2303</td>\n",
       "      <td>Lenox</td>\n",
       "      <td>B</td>\n",
       "      <td>33.84676</td>\n",
       "      <td>-84.36212</td>\n",
       "      <td>2019-01-01 14:00:00</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Occur Date Occur Time   Beat                          Location  \\\n",
       "0 2019-01-01   00:20:00  511.0            50 UPPER ALABAMA ST SW   \n",
       "1 2019-01-01   01:20:00  511.0                       20 BROAD ST   \n",
       "2 2019-01-01   17:40:00  411.0   3000 CONTINENTAL COLONY PKWY SW   \n",
       "3 2019-01-01   04:15:00  607.0                 1362 BOULEVARD SE   \n",
       "4 2019-01-01   14:00:00  210.0  3393 PEACHTREE RD NE @LENOX MALL   \n",
       "\n",
       "  Shift Occurrence Location Type          UCR Literal  UCR # IBR Code  \\\n",
       "0    Morning Watch            13  LARCENY-NON VEHICLE    620     2302   \n",
       "1    Morning Watch           NaN  LARCENY-NON VEHICLE    620     2302   \n",
       "2    Evening Watch            26  LARCENY-NON VEHICLE    620     2302   \n",
       "3    Morning Watch            23  LARCENY-NON VEHICLE    630     2303   \n",
       "4    Evening Watch             8  LARCENY-NON VEHICLE    630     2303   \n",
       "\n",
       "   Neighborhood NPU  Latitude  Longitude            DateTime  Hour  Minute  \\\n",
       "0      Downtown   M  33.75194  -84.38964 2019-01-01 00:20:00     0      20   \n",
       "1      Downtown   M  33.75312  -84.39208 2019-01-01 01:20:00     1      20   \n",
       "2    Greenbriar   R  33.68077  -84.49370 2019-01-01 17:40:00    17      40   \n",
       "3  Benteen Park   W  33.71744  -84.36818 2019-01-01 04:15:00     4      15   \n",
       "4         Lenox   B  33.84676  -84.36212 2019-01-01 14:00:00    14       0   \n",
       "\n",
       "   Month  \n",
       "0      1  \n",
       "1      1  \n",
       "2      1  \n",
       "3      1  \n",
       "4      1  "
      ]
     },
     "execution_count": 426,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['DayOfWeek'] = df['DateTime'].dt.dayofweek\n",
    "df['Day'] = df['DateTime'].dt.day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Null Values\n",
    "Checking the null values in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Occur Date            0\n",
       "Occur Time            0\n",
       "Beat                  9\n",
       "Location              0\n",
       "Shift Occurrence      0\n",
       "Location Type       732\n",
       "UCR Literal           0\n",
       "UCR #                 0\n",
       "IBR Code              0\n",
       "Neighborhood        405\n",
       "NPU                   6\n",
       "Latitude              0\n",
       "Longitude             0\n",
       "DateTime              0\n",
       "Hour                  0\n",
       "Minute                0\n",
       "Month                 0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 428,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of unique Values of each feature column\n",
    "This done to know which all columns need to be one hot encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Occur Date           252\n",
       "Occur Time          1142\n",
       "Beat                  80\n",
       "Location            6584\n",
       "Shift Occurrence       4\n",
       "Location Type         34\n",
       "UCR Literal           11\n",
       "UCR #                 48\n",
       "IBR Code              60\n",
       "Neighborhood         228\n",
       "NPU                   25\n",
       "Latitude            5225\n",
       "Longitude           5252\n",
       "DateTime            8247\n",
       "Hour                  24\n",
       "Minute                60\n",
       "Month                 12\n",
       "DayOfWeek              7\n",
       "Day                   31\n",
       "dtype: int64"
      ]
     },
     "execution_count": 431,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As all the relevant features are already extracted in separate columns, therefore there is no need of Date time columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df['Occur Date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df['Occur Time']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the datatype of various feature columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Beat                       float64\n",
       "Location                    object\n",
       "Shift Occurrence            object\n",
       "Location Type               object\n",
       "UCR Literal                 object\n",
       "UCR #                        int64\n",
       "IBR Code                    object\n",
       "Neighborhood                object\n",
       "NPU                         object\n",
       "Latitude                   float64\n",
       "Longitude                  float64\n",
       "DateTime            datetime64[ns]\n",
       "Hour                         int64\n",
       "Minute                       int64\n",
       "Month                        int64\n",
       "DayOfWeek                    int64\n",
       "Day                          int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 434,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.dropna(how='any') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Beat</th>\n",
       "      <th>Location</th>\n",
       "      <th>Shift Occurrence</th>\n",
       "      <th>Location Type</th>\n",
       "      <th>UCR Literal</th>\n",
       "      <th>UCR #</th>\n",
       "      <th>IBR Code</th>\n",
       "      <th>Neighborhood</th>\n",
       "      <th>NPU</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>DateTime</th>\n",
       "      <th>Hour</th>\n",
       "      <th>Minute</th>\n",
       "      <th>Month</th>\n",
       "      <th>DayOfWeek</th>\n",
       "      <th>Day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>511.0</td>\n",
       "      <td>50 UPPER ALABAMA ST SW</td>\n",
       "      <td>Morning Watch</td>\n",
       "      <td>13</td>\n",
       "      <td>LARCENY-NON VEHICLE</td>\n",
       "      <td>620</td>\n",
       "      <td>2302</td>\n",
       "      <td>Downtown</td>\n",
       "      <td>M</td>\n",
       "      <td>33.75194</td>\n",
       "      <td>-84.38964</td>\n",
       "      <td>2019-01-01 00:20:00</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>411.0</td>\n",
       "      <td>3000 CONTINENTAL COLONY PKWY SW</td>\n",
       "      <td>Evening Watch</td>\n",
       "      <td>26</td>\n",
       "      <td>LARCENY-NON VEHICLE</td>\n",
       "      <td>620</td>\n",
       "      <td>2302</td>\n",
       "      <td>Greenbriar</td>\n",
       "      <td>R</td>\n",
       "      <td>33.68077</td>\n",
       "      <td>-84.49370</td>\n",
       "      <td>2019-01-01 17:40:00</td>\n",
       "      <td>17</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Beat                         Location Shift Occurrence Location Type  \\\n",
       "0  511.0           50 UPPER ALABAMA ST SW    Morning Watch            13   \n",
       "2  411.0  3000 CONTINENTAL COLONY PKWY SW    Evening Watch            26   \n",
       "\n",
       "           UCR Literal  UCR # IBR Code Neighborhood NPU  Latitude  Longitude  \\\n",
       "0  LARCENY-NON VEHICLE    620     2302     Downtown   M  33.75194  -84.38964   \n",
       "2  LARCENY-NON VEHICLE    620     2302   Greenbriar   R  33.68077  -84.49370   \n",
       "\n",
       "             DateTime  Hour  Minute  Month  DayOfWeek  Day  \n",
       "0 2019-01-01 00:20:00     0      20      1          1    1  \n",
       "2 2019-01-01 17:40:00    17      40      1          1    1  "
      ]
     },
     "execution_count": 438,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Absurd Values removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df[df['Location Type'] != 'E']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['13', '26', '23', '8', '24', '12', '20', '18', '21', '99', '31',\n",
       "       '3', '2', '14', '6', '5', '28', '4', '11', '9', '7', '29', '17',\n",
       "       '1', '10', '19', '35', '36', '30', '15', '34', '33', '16'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 440,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Location Type'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separating out label of our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = df['UCR Literal'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['LARCENY-NON VEHICLE', 'LARCENY-NON VEHICLE',\n",
       "       'LARCENY-NON VEHICLE', ..., 'HOMICIDE', 'AGG ASSAULT',\n",
       "       'BURGLARY-NONRES'], dtype=object)"
      ]
     },
     "execution_count": 442,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = np.reshape(Y, (-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['LARCENY-NON VEHICLE'],\n",
       "       ['LARCENY-NON VEHICLE'],\n",
       "       ['LARCENY-NON VEHICLE'],\n",
       "       ...,\n",
       "       ['HOMICIDE'],\n",
       "       ['AGG ASSAULT'],\n",
       "       ['BURGLARY-NONRES']], dtype=object)"
      ]
     },
     "execution_count": 444,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separating out the final features of our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 654,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df[['Latitude','Longitude','Shift Occurrence','Location Type','Neighborhood','Hour','Minute','Month','DayOfWeek','Day']]\n",
    "# df1 = df[['Latitude','Longitude','Shift Occurrence','Hour','Minute','Month','DayOfWeek','Day']]\n",
    "# df1 = df[['Latitude','Longitude']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 655,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Shift Occurrence</th>\n",
       "      <th>Location Type</th>\n",
       "      <th>Neighborhood</th>\n",
       "      <th>Hour</th>\n",
       "      <th>Minute</th>\n",
       "      <th>Month</th>\n",
       "      <th>DayOfWeek</th>\n",
       "      <th>Day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>33.75194</td>\n",
       "      <td>-84.38964</td>\n",
       "      <td>Morning Watch</td>\n",
       "      <td>13</td>\n",
       "      <td>Downtown</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33.68077</td>\n",
       "      <td>-84.49370</td>\n",
       "      <td>Evening Watch</td>\n",
       "      <td>26</td>\n",
       "      <td>Greenbriar</td>\n",
       "      <td>17</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33.71744</td>\n",
       "      <td>-84.36818</td>\n",
       "      <td>Morning Watch</td>\n",
       "      <td>23</td>\n",
       "      <td>Benteen Park</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33.84676</td>\n",
       "      <td>-84.36212</td>\n",
       "      <td>Evening Watch</td>\n",
       "      <td>8</td>\n",
       "      <td>Lenox</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>33.84676</td>\n",
       "      <td>-84.36212</td>\n",
       "      <td>Evening Watch</td>\n",
       "      <td>24</td>\n",
       "      <td>Lenox</td>\n",
       "      <td>15</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Latitude  Longitude Shift Occurrence Location Type  Neighborhood  Hour  \\\n",
       "0  33.75194  -84.38964    Morning Watch            13      Downtown     0   \n",
       "2  33.68077  -84.49370    Evening Watch            26    Greenbriar    17   \n",
       "3  33.71744  -84.36818    Morning Watch            23  Benteen Park     4   \n",
       "4  33.84676  -84.36212    Evening Watch             8         Lenox    14   \n",
       "5  33.84676  -84.36212    Evening Watch            24         Lenox    15   \n",
       "\n",
       "   Minute  Month  DayOfWeek  Day  \n",
       "0      20      1          1    1  \n",
       "2      40      1          1    1  \n",
       "3      15      1          1    1  \n",
       "4       0      1          1    1  \n",
       "5      28      1          1    1  "
      ]
     },
     "execution_count": 655,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 656,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Latitude            float64\n",
       "Longitude           float64\n",
       "Shift Occurrence     object\n",
       "Location Type        object\n",
       "Neighborhood         object\n",
       "Hour                  int64\n",
       "Minute                int64\n",
       "Month                 int64\n",
       "DayOfWeek             int64\n",
       "Day                   int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 656,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot encoding\n",
    "All the categorical data is one hot encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 657,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=pd.get_dummies(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 658,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9790, 270)"
      ]
     },
     "execution_count": 658,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 659,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df1.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling the features\n",
    "Using Standard Scaler for scaling all the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 660,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler(copy=True, with_mean=True, with_std=True)"
      ]
     },
     "execution_count": 660,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 661,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=scaler.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Encoding and One Hot Encoding of the Label column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Integer encode\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit(Y)\n",
    "labels = integer_encoded.classes_\n",
    "integer_encoded = integer_encoded.transform(Y)\n",
    "\n",
    "# binary encode\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "onehot_encoded = onehot_encoder.fit_transform(integer_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 663,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 663,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onehot_encoded[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 664,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y=onehot_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 665,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9790, 11)"
      ]
     },
     "execution_count": 665,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 666,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Size: (7832, 270)\n",
      "Testing Data Size:(1958, 270)\n",
      "Training Label Size: (7832, 11)\n",
      "Testing Label Size: (1958, 11)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.20, random_state=42)\n",
    "\n",
    "print(\"Training Data Size: \" + str(X_train.shape))\n",
    "print(\"Testing Data Size:\" + str(X_test.shape))\n",
    "print(\"Training Label Size: \" + str(Y_train.shape))\n",
    "print(\"Testing Label Size: \" + str(Y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Model\n",
    "The following model uses 6 dense layers with softmax on the output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_95 (Dense)             (None, 64)                17344     \n",
      "_________________________________________________________________\n",
      "dropout_65 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_96 (Dense)             (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dropout_66 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_97 (Dense)             (None, 512)               66048     \n",
      "_________________________________________________________________\n",
      "dropout_67 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_98 (Dense)             (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "dropout_68 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_99 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_69 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_100 (Dense)            (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_70 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_101 (Dense)            (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_102 (Dense)            (None, 11)                715       \n",
      "=================================================================\n",
      "Total params: 199,371\n",
      "Trainable params: 199,371\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense, Dropout\n",
    "from keras.models import Sequential\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(64, activation='tanh', input_shape=(X_train.shape[1], ) ) )\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(128, activation='tanh') )\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(512, activation='tanh') )\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(128, activation='tanh') )\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(128, activation='tanh') )\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(128, activation='tanh') )\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(64, activation='tanh') )\n",
    "\n",
    "\n",
    "model.add(Dense(Y_train.shape[1], activation='softmax') )\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7048 samples, validate on 784 samples\n",
      "Epoch 1/1000\n",
      "7048/7048 [==============================] - 2s 284us/step - loss: 1.8001 - acc: 0.3214 - val_loss: 1.6969 - val_acc: 0.3622\n",
      "Epoch 2/1000\n",
      "7048/7048 [==============================] - 0s 55us/step - loss: 1.7390 - acc: 0.3434 - val_loss: 1.7006 - val_acc: 0.3622\n",
      "Epoch 3/1000\n",
      "7048/7048 [==============================] - 0s 56us/step - loss: 1.7255 - acc: 0.3463 - val_loss: 1.7027 - val_acc: 0.3597\n",
      "Epoch 4/1000\n",
      "7048/7048 [==============================] - 0s 53us/step - loss: 1.7189 - acc: 0.3482 - val_loss: 1.6878 - val_acc: 0.3622\n",
      "Epoch 5/1000\n",
      "7048/7048 [==============================] - 0s 54us/step - loss: 1.6931 - acc: 0.3635 - val_loss: 1.6103 - val_acc: 0.3839\n",
      "Epoch 6/1000\n",
      "7048/7048 [==============================] - 0s 54us/step - loss: 1.6240 - acc: 0.4127 - val_loss: 1.4966 - val_acc: 0.4541\n",
      "Epoch 7/1000\n",
      "7048/7048 [==============================] - 0s 62us/step - loss: 1.5288 - acc: 0.4706 - val_loss: 1.4371 - val_acc: 0.5153\n",
      "Epoch 8/1000\n",
      "7048/7048 [==============================] - 0s 55us/step - loss: 1.4973 - acc: 0.4823 - val_loss: 1.4741 - val_acc: 0.5038\n",
      "Epoch 9/1000\n",
      "7048/7048 [==============================] - 0s 56us/step - loss: 1.4697 - acc: 0.4987 - val_loss: 1.3806 - val_acc: 0.5332\n",
      "Epoch 10/1000\n",
      "7048/7048 [==============================] - 0s 56us/step - loss: 1.4587 - acc: 0.4966 - val_loss: 1.3691 - val_acc: 0.5319\n",
      "Epoch 11/1000\n",
      "7048/7048 [==============================] - 0s 54us/step - loss: 1.3958 - acc: 0.5200 - val_loss: 1.3395 - val_acc: 0.5446\n",
      "Epoch 12/1000\n",
      "7048/7048 [==============================] - 0s 53us/step - loss: 1.3824 - acc: 0.5149 - val_loss: 1.3104 - val_acc: 0.5472\n",
      "Epoch 13/1000\n",
      "7048/7048 [==============================] - 0s 54us/step - loss: 1.3658 - acc: 0.5176 - val_loss: 1.3668 - val_acc: 0.5370\n",
      "Epoch 14/1000\n",
      "7048/7048 [==============================] - 0s 54us/step - loss: 1.3770 - acc: 0.5148 - val_loss: 1.3094 - val_acc: 0.5510\n",
      "Epoch 15/1000\n",
      "7048/7048 [==============================] - 0s 54us/step - loss: 1.3420 - acc: 0.5220 - val_loss: 1.2914 - val_acc: 0.5510\n",
      "Epoch 16/1000\n",
      "7048/7048 [==============================] - 0s 54us/step - loss: 1.3362 - acc: 0.5257 - val_loss: 1.3434 - val_acc: 0.5434\n",
      "Epoch 17/1000\n",
      "7048/7048 [==============================] - 0s 57us/step - loss: 1.3484 - acc: 0.5265 - val_loss: 1.3019 - val_acc: 0.5408\n",
      "Epoch 18/1000\n",
      "7048/7048 [==============================] - 0s 54us/step - loss: 1.3261 - acc: 0.5271 - val_loss: 1.3018 - val_acc: 0.5446\n",
      "Epoch 19/1000\n",
      "7048/7048 [==============================] - 0s 54us/step - loss: 1.3335 - acc: 0.5203 - val_loss: 1.2981 - val_acc: 0.5408\n",
      "Epoch 20/1000\n",
      "7048/7048 [==============================] - 0s 54us/step - loss: 1.3367 - acc: 0.5294 - val_loss: 1.2923 - val_acc: 0.5536\n",
      "Epoch 21/1000\n",
      "7048/7048 [==============================] - 0s 54us/step - loss: 1.3310 - acc: 0.5284 - val_loss: 1.2877 - val_acc: 0.5472\n",
      "Epoch 22/1000\n",
      "7048/7048 [==============================] - 0s 54us/step - loss: 1.3195 - acc: 0.5359 - val_loss: 1.2953 - val_acc: 0.5523\n",
      "Epoch 23/1000\n",
      "7048/7048 [==============================] - 0s 64us/step - loss: 1.3188 - acc: 0.5312 - val_loss: 1.3071 - val_acc: 0.5485\n",
      "Epoch 24/1000\n",
      "7048/7048 [==============================] - 0s 58us/step - loss: 1.3203 - acc: 0.5301 - val_loss: 1.2982 - val_acc: 0.5370\n",
      "Epoch 25/1000\n",
      "7048/7048 [==============================] - 0s 55us/step - loss: 1.3204 - acc: 0.5262 - val_loss: 1.2993 - val_acc: 0.5510\n",
      "Epoch 26/1000\n",
      "7048/7048 [==============================] - 0s 55us/step - loss: 1.3127 - acc: 0.5312 - val_loss: 1.2763 - val_acc: 0.5574\n",
      "Epoch 27/1000\n",
      "7048/7048 [==============================] - 0s 55us/step - loss: 1.3066 - acc: 0.5353 - val_loss: 1.2866 - val_acc: 0.5459\n",
      "Epoch 28/1000\n",
      "7048/7048 [==============================] - 0s 63us/step - loss: 1.3199 - acc: 0.5369 - val_loss: 1.3024 - val_acc: 0.5536\n",
      "Epoch 29/1000\n",
      "7048/7048 [==============================] - 0s 55us/step - loss: 1.3011 - acc: 0.5355 - val_loss: 1.3021 - val_acc: 0.5408\n",
      "Epoch 30/1000\n",
      "7048/7048 [==============================] - 0s 54us/step - loss: 1.2994 - acc: 0.5383 - val_loss: 1.3056 - val_acc: 0.5587\n",
      "Epoch 31/1000\n",
      "7048/7048 [==============================] - 0s 55us/step - loss: 1.2950 - acc: 0.5350 - val_loss: 1.3072 - val_acc: 0.5421\n",
      "Epoch 32/1000\n",
      "7048/7048 [==============================] - 0s 54us/step - loss: 1.2985 - acc: 0.5387 - val_loss: 1.2978 - val_acc: 0.5574\n",
      "Epoch 33/1000\n",
      "7048/7048 [==============================] - 0s 58us/step - loss: 1.2914 - acc: 0.5424 - val_loss: 1.3328 - val_acc: 0.5536\n",
      "Epoch 34/1000\n",
      "7048/7048 [==============================] - 0s 60us/step - loss: 1.2784 - acc: 0.5465 - val_loss: 1.3303 - val_acc: 0.5459\n",
      "Epoch 35/1000\n",
      "7048/7048 [==============================] - 0s 54us/step - loss: 1.2799 - acc: 0.5433 - val_loss: 1.3121 - val_acc: 0.5587\n",
      "Epoch 36/1000\n",
      "7048/7048 [==============================] - 0s 55us/step - loss: 1.2945 - acc: 0.5350 - val_loss: 1.2827 - val_acc: 0.5612\n",
      "Epoch 37/1000\n",
      "7048/7048 [==============================] - 0s 55us/step - loss: 1.2825 - acc: 0.5420 - val_loss: 1.3055 - val_acc: 0.5510\n",
      "Epoch 38/1000\n",
      "7048/7048 [==============================] - 0s 55us/step - loss: 1.2803 - acc: 0.5467 - val_loss: 1.2777 - val_acc: 0.5599\n",
      "Epoch 39/1000\n",
      "7048/7048 [==============================] - 0s 61us/step - loss: 1.2825 - acc: 0.5490 - val_loss: 1.2858 - val_acc: 0.5485\n",
      "Epoch 40/1000\n",
      "7048/7048 [==============================] - 0s 57us/step - loss: 1.2904 - acc: 0.5410 - val_loss: 1.2866 - val_acc: 0.5625\n",
      "Epoch 41/1000\n",
      "7048/7048 [==============================] - 0s 58us/step - loss: 1.2749 - acc: 0.5443 - val_loss: 1.2971 - val_acc: 0.5523\n",
      "Epoch 42/1000\n",
      "7048/7048 [==============================] - 1s 72us/step - loss: 1.2720 - acc: 0.5471 - val_loss: 1.2747 - val_acc: 0.5625\n",
      "Epoch 43/1000\n",
      "7048/7048 [==============================] - 0s 64us/step - loss: 1.2706 - acc: 0.5492 - val_loss: 1.2807 - val_acc: 0.5536\n",
      "Epoch 44/1000\n",
      "7048/7048 [==============================] - 1s 77us/step - loss: 1.2671 - acc: 0.5504 - val_loss: 1.3092 - val_acc: 0.5395\n",
      "Epoch 45/1000\n",
      "7048/7048 [==============================] - 0s 64us/step - loss: 1.2678 - acc: 0.5518 - val_loss: 1.2898 - val_acc: 0.5472\n",
      "Epoch 46/1000\n",
      "7048/7048 [==============================] - 0s 56us/step - loss: 1.2803 - acc: 0.5458 - val_loss: 1.2714 - val_acc: 0.5574\n",
      "Epoch 47/1000\n",
      "7048/7048 [==============================] - 0s 66us/step - loss: 1.2606 - acc: 0.5484 - val_loss: 1.2925 - val_acc: 0.5612\n",
      "Epoch 48/1000\n",
      "7048/7048 [==============================] - 0s 63us/step - loss: 1.2628 - acc: 0.5475 - val_loss: 1.2728 - val_acc: 0.5472\n",
      "Epoch 49/1000\n",
      "7048/7048 [==============================] - 0s 69us/step - loss: 1.2628 - acc: 0.5465 - val_loss: 1.2829 - val_acc: 0.5625\n",
      "Epoch 50/1000\n",
      "7048/7048 [==============================] - 0s 57us/step - loss: 1.2603 - acc: 0.5521 - val_loss: 1.2989 - val_acc: 0.5217\n",
      "Epoch 51/1000\n",
      "7048/7048 [==============================] - 0s 61us/step - loss: 1.2722 - acc: 0.5434 - val_loss: 1.2847 - val_acc: 0.5510\n",
      "Epoch 52/1000\n",
      "7048/7048 [==============================] - 0s 55us/step - loss: 1.2553 - acc: 0.5542 - val_loss: 1.2802 - val_acc: 0.5536\n",
      "Epoch 53/1000\n",
      "7048/7048 [==============================] - 0s 55us/step - loss: 1.2656 - acc: 0.5526 - val_loss: 1.2846 - val_acc: 0.5612\n",
      "Epoch 54/1000\n",
      "7048/7048 [==============================] - 0s 63us/step - loss: 1.2646 - acc: 0.5509 - val_loss: 1.3263 - val_acc: 0.5293\n",
      "Epoch 55/1000\n",
      "7048/7048 [==============================] - 0s 61us/step - loss: 1.3445 - acc: 0.5230 - val_loss: 1.2914 - val_acc: 0.5523\n",
      "Epoch 56/1000\n",
      "7048/7048 [==============================] - 0s 60us/step - loss: 1.2710 - acc: 0.5484 - val_loss: 1.2781 - val_acc: 0.5612\n",
      "Epoch 57/1000\n",
      "7048/7048 [==============================] - 0s 57us/step - loss: 1.2593 - acc: 0.5509 - val_loss: 1.2922 - val_acc: 0.5625\n",
      "Epoch 58/1000\n",
      "7048/7048 [==============================] - 0s 57us/step - loss: 1.2566 - acc: 0.5485 - val_loss: 1.2761 - val_acc: 0.5599\n",
      "Epoch 59/1000\n",
      "7048/7048 [==============================] - 0s 61us/step - loss: 1.2604 - acc: 0.5526 - val_loss: 1.2765 - val_acc: 0.5536\n",
      "Epoch 60/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7048/7048 [==============================] - 0s 55us/step - loss: 1.2518 - acc: 0.5535 - val_loss: 1.2740 - val_acc: 0.5548\n",
      "Epoch 61/1000\n",
      "7048/7048 [==============================] - 0s 53us/step - loss: 1.2628 - acc: 0.5498 - val_loss: 1.2790 - val_acc: 0.5638\n",
      "Epoch 62/1000\n",
      "7048/7048 [==============================] - 0s 61us/step - loss: 1.2390 - acc: 0.5600 - val_loss: 1.2911 - val_acc: 0.5548\n",
      "Epoch 63/1000\n",
      "7048/7048 [==============================] - 0s 59us/step - loss: 1.2436 - acc: 0.5548 - val_loss: 1.2655 - val_acc: 0.5689\n",
      "Epoch 64/1000\n",
      "7048/7048 [==============================] - 0s 64us/step - loss: 1.2426 - acc: 0.5585 - val_loss: 1.2678 - val_acc: 0.5599\n",
      "Epoch 65/1000\n",
      "7048/7048 [==============================] - 0s 56us/step - loss: 1.2440 - acc: 0.5526 - val_loss: 1.2842 - val_acc: 0.5548\n",
      "Epoch 66/1000\n",
      "7048/7048 [==============================] - 0s 56us/step - loss: 1.2379 - acc: 0.5604 - val_loss: 1.2780 - val_acc: 0.5523\n",
      "Epoch 67/1000\n",
      "7048/7048 [==============================] - 0s 55us/step - loss: 1.2485 - acc: 0.5563 - val_loss: 1.2852 - val_acc: 0.5612\n",
      "Epoch 68/1000\n",
      "7048/7048 [==============================] - 0s 55us/step - loss: 1.2345 - acc: 0.5582 - val_loss: 1.2699 - val_acc: 0.5497\n",
      "Epoch 69/1000\n",
      "7048/7048 [==============================] - 0s 55us/step - loss: 1.2387 - acc: 0.5603 - val_loss: 1.2808 - val_acc: 0.5612\n",
      "Epoch 70/1000\n",
      "7048/7048 [==============================] - 0s 54us/step - loss: 1.2566 - acc: 0.5558 - val_loss: 1.2666 - val_acc: 0.5689\n",
      "Epoch 71/1000\n",
      "7048/7048 [==============================] - 0s 54us/step - loss: 1.2481 - acc: 0.5575 - val_loss: 1.2867 - val_acc: 0.5638\n",
      "Epoch 72/1000\n",
      "7048/7048 [==============================] - 0s 63us/step - loss: 1.2321 - acc: 0.5573 - val_loss: 1.2766 - val_acc: 0.5599\n",
      "Epoch 73/1000\n",
      "7048/7048 [==============================] - 0s 67us/step - loss: 1.2374 - acc: 0.5570 - val_loss: 1.2645 - val_acc: 0.5663\n",
      "Epoch 74/1000\n",
      "7048/7048 [==============================] - 0s 58us/step - loss: 1.2360 - acc: 0.5594 - val_loss: 1.2769 - val_acc: 0.5536\n",
      "Epoch 75/1000\n",
      "7048/7048 [==============================] - 0s 56us/step - loss: 1.2374 - acc: 0.5570 - val_loss: 1.2743 - val_acc: 0.5638\n",
      "Epoch 76/1000\n",
      "7048/7048 [==============================] - 0s 57us/step - loss: 1.2337 - acc: 0.5573 - val_loss: 1.2938 - val_acc: 0.5689\n",
      "Epoch 77/1000\n",
      "7048/7048 [==============================] - 0s 60us/step - loss: 1.2364 - acc: 0.5607 - val_loss: 1.2996 - val_acc: 0.5651\n",
      "Epoch 78/1000\n",
      "7048/7048 [==============================] - 0s 57us/step - loss: 1.2445 - acc: 0.5577 - val_loss: 1.2755 - val_acc: 0.5574\n",
      "Epoch 79/1000\n",
      "7048/7048 [==============================] - 0s 58us/step - loss: 1.2289 - acc: 0.5623 - val_loss: 1.2696 - val_acc: 0.5497\n",
      "Epoch 80/1000\n",
      "7048/7048 [==============================] - 0s 56us/step - loss: 1.2320 - acc: 0.5620 - val_loss: 1.2805 - val_acc: 0.5638\n",
      "Epoch 81/1000\n",
      "7048/7048 [==============================] - 0s 60us/step - loss: 1.2177 - acc: 0.5621 - val_loss: 1.2901 - val_acc: 0.5497\n",
      "Epoch 82/1000\n",
      "7048/7048 [==============================] - 0s 58us/step - loss: 1.2256 - acc: 0.5586 - val_loss: 1.2891 - val_acc: 0.5472\n",
      "Epoch 83/1000\n",
      "7048/7048 [==============================] - 0s 57us/step - loss: 1.2377 - acc: 0.5590 - val_loss: 1.2808 - val_acc: 0.5574\n",
      "Epoch 84/1000\n",
      "7048/7048 [==============================] - 0s 58us/step - loss: 1.2260 - acc: 0.5690 - val_loss: 1.2793 - val_acc: 0.5714\n",
      "Epoch 85/1000\n",
      "7048/7048 [==============================] - 0s 57us/step - loss: 1.2150 - acc: 0.5668 - val_loss: 1.2656 - val_acc: 0.5651\n",
      "Epoch 86/1000\n",
      "7048/7048 [==============================] - 0s 57us/step - loss: 1.2180 - acc: 0.5653 - val_loss: 1.2674 - val_acc: 0.5612\n",
      "Epoch 87/1000\n",
      "7048/7048 [==============================] - 0s 57us/step - loss: 1.2176 - acc: 0.5673 - val_loss: 1.2803 - val_acc: 0.5574\n",
      "Epoch 88/1000\n",
      "7048/7048 [==============================] - 0s 57us/step - loss: 1.2204 - acc: 0.5640 - val_loss: 1.2644 - val_acc: 0.5612\n",
      "Epoch 89/1000\n",
      "7048/7048 [==============================] - 0s 55us/step - loss: 1.2179 - acc: 0.5675 - val_loss: 1.2785 - val_acc: 0.5561\n",
      "Epoch 90/1000\n",
      "7048/7048 [==============================] - 0s 55us/step - loss: 1.2192 - acc: 0.5620 - val_loss: 1.2646 - val_acc: 0.5561\n",
      "Epoch 91/1000\n",
      "7048/7048 [==============================] - 0s 55us/step - loss: 1.2161 - acc: 0.5681 - val_loss: 1.2748 - val_acc: 0.5625\n",
      "Epoch 92/1000\n",
      "7048/7048 [==============================] - 0s 54us/step - loss: 1.2059 - acc: 0.5705 - val_loss: 1.2508 - val_acc: 0.5612\n",
      "Epoch 93/1000\n",
      "7048/7048 [==============================] - 0s 54us/step - loss: 1.2310 - acc: 0.5593 - val_loss: 1.2735 - val_acc: 0.5485\n",
      "Epoch 94/1000\n",
      "7048/7048 [==============================] - 0s 55us/step - loss: 1.2294 - acc: 0.5596 - val_loss: 1.2462 - val_acc: 0.5651\n",
      "Epoch 95/1000\n",
      "7048/7048 [==============================] - 0s 54us/step - loss: 1.2132 - acc: 0.5657 - val_loss: 1.3154 - val_acc: 0.5612\n",
      "Epoch 96/1000\n",
      "7048/7048 [==============================] - 0s 55us/step - loss: 1.2219 - acc: 0.5594 - val_loss: 1.2412 - val_acc: 0.5561\n",
      "Epoch 97/1000\n",
      "7048/7048 [==============================] - 0s 54us/step - loss: 1.2180 - acc: 0.5617 - val_loss: 1.2725 - val_acc: 0.5548\n",
      "Epoch 98/1000\n",
      "7048/7048 [==============================] - 0s 55us/step - loss: 1.2001 - acc: 0.5681 - val_loss: 1.2630 - val_acc: 0.5574\n",
      "Epoch 99/1000\n",
      "7048/7048 [==============================] - 0s 54us/step - loss: 1.2079 - acc: 0.5691 - val_loss: 1.2791 - val_acc: 0.5612\n",
      "Epoch 100/1000\n",
      "7048/7048 [==============================] - 0s 58us/step - loss: 1.2092 - acc: 0.5671 - val_loss: 1.2779 - val_acc: 0.5676\n",
      "Epoch 101/1000\n",
      "7048/7048 [==============================] - 0s 63us/step - loss: 1.2113 - acc: 0.5647 - val_loss: 1.2788 - val_acc: 0.5625\n",
      "Epoch 102/1000\n",
      "7048/7048 [==============================] - 0s 64us/step - loss: 1.2083 - acc: 0.5681 - val_loss: 1.2574 - val_acc: 0.5702\n",
      "Epoch 103/1000\n",
      "7048/7048 [==============================] - 0s 57us/step - loss: 1.2048 - acc: 0.5658 - val_loss: 1.2649 - val_acc: 0.5676\n",
      "Epoch 104/1000\n",
      "7048/7048 [==============================] - 0s 57us/step - loss: 1.2260 - acc: 0.5658 - val_loss: 1.2569 - val_acc: 0.5714\n",
      "Epoch 105/1000\n",
      "7048/7048 [==============================] - 0s 56us/step - loss: 1.2022 - acc: 0.5667 - val_loss: 1.2672 - val_acc: 0.5561\n",
      "Epoch 106/1000\n",
      "7048/7048 [==============================] - 0s 55us/step - loss: 1.2245 - acc: 0.5612 - val_loss: 1.2978 - val_acc: 0.5536\n",
      "Epoch 107/1000\n",
      "7048/7048 [==============================] - 0s 55us/step - loss: 1.2274 - acc: 0.5620 - val_loss: 1.2633 - val_acc: 0.5497\n",
      "Epoch 108/1000\n",
      "7048/7048 [==============================] - 0s 55us/step - loss: 1.2109 - acc: 0.5664 - val_loss: 1.2532 - val_acc: 0.5625\n",
      "Epoch 109/1000\n",
      "7048/7048 [==============================] - 0s 55us/step - loss: 1.2043 - acc: 0.5648 - val_loss: 1.2640 - val_acc: 0.5753\n",
      "Epoch 110/1000\n",
      "7048/7048 [==============================] - 0s 55us/step - loss: 1.2064 - acc: 0.5667 - val_loss: 1.2924 - val_acc: 0.5587\n",
      "Epoch 111/1000\n",
      "7048/7048 [==============================] - 0s 55us/step - loss: 1.2014 - acc: 0.5736 - val_loss: 1.2594 - val_acc: 0.5727\n",
      "Epoch 112/1000\n",
      "7048/7048 [==============================] - 0s 55us/step - loss: 1.1905 - acc: 0.5790 - val_loss: 1.2671 - val_acc: 0.5689\n",
      "Epoch 113/1000\n",
      "7048/7048 [==============================] - 0s 55us/step - loss: 1.2131 - acc: 0.5699 - val_loss: 1.2503 - val_acc: 0.5676\n",
      "Epoch 114/1000\n",
      "7048/7048 [==============================] - 0s 55us/step - loss: 1.1985 - acc: 0.5684 - val_loss: 1.3102 - val_acc: 0.5332\n",
      "Epoch 115/1000\n",
      "7048/7048 [==============================] - 0s 54us/step - loss: 1.2019 - acc: 0.5673 - val_loss: 1.2542 - val_acc: 0.5753\n",
      "Epoch 116/1000\n",
      "7048/7048 [==============================] - 0s 55us/step - loss: 1.1940 - acc: 0.5743 - val_loss: 1.2625 - val_acc: 0.5689\n",
      "Epoch 117/1000\n",
      "7048/7048 [==============================] - 0s 62us/step - loss: 1.2050 - acc: 0.5663 - val_loss: 1.2547 - val_acc: 0.5561\n",
      "Epoch 118/1000\n",
      "7048/7048 [==============================] - 0s 65us/step - loss: 1.1972 - acc: 0.5734 - val_loss: 1.2719 - val_acc: 0.5548\n",
      "Epoch 119/1000\n",
      "7048/7048 [==============================] - 1s 80us/step - loss: 1.2148 - acc: 0.5671 - val_loss: 1.2864 - val_acc: 0.5714\n",
      "Epoch 120/1000\n",
      "7048/7048 [==============================] - 1s 80us/step - loss: 1.2129 - acc: 0.5660 - val_loss: 1.2916 - val_acc: 0.5740\n",
      "Epoch 121/1000\n",
      "7048/7048 [==============================] - 0s 61us/step - loss: 1.2000 - acc: 0.5721 - val_loss: 1.2663 - val_acc: 0.5459\n",
      "Epoch 122/1000\n",
      "7048/7048 [==============================] - 0s 49us/step - loss: 1.1962 - acc: 0.5748 - val_loss: 1.2772 - val_acc: 0.5740\n",
      "Epoch 123/1000\n",
      "7048/7048 [==============================] - 0s 54us/step - loss: 1.2052 - acc: 0.5680 - val_loss: 1.2934 - val_acc: 0.5548\n",
      "Epoch 124/1000\n",
      "7048/7048 [==============================] - 0s 64us/step - loss: 1.1964 - acc: 0.5714 - val_loss: 1.2715 - val_acc: 0.5714\n",
      "Epoch 125/1000\n",
      "7048/7048 [==============================] - 0s 49us/step - loss: 1.1807 - acc: 0.5790 - val_loss: 1.2505 - val_acc: 0.5651\n",
      "Epoch 126/1000\n",
      "7048/7048 [==============================] - 0s 48us/step - loss: 1.1869 - acc: 0.5776 - val_loss: 1.2837 - val_acc: 0.5612\n",
      "Epoch 127/1000\n",
      "7048/7048 [==============================] - 0s 53us/step - loss: 1.1943 - acc: 0.5755 - val_loss: 1.2513 - val_acc: 0.5523\n",
      "Epoch 128/1000\n",
      "7048/7048 [==============================] - 0s 51us/step - loss: 1.1989 - acc: 0.5749 - val_loss: 1.2511 - val_acc: 0.5663\n",
      "Epoch 129/1000\n",
      "7048/7048 [==============================] - 0s 52us/step - loss: 1.1903 - acc: 0.5756 - val_loss: 1.2781 - val_acc: 0.5638\n",
      "Epoch 130/1000\n",
      "7048/7048 [==============================] - 0s 49us/step - loss: 1.2079 - acc: 0.5651 - val_loss: 1.2514 - val_acc: 0.5536\n",
      "Epoch 131/1000\n",
      "7048/7048 [==============================] - 0s 51us/step - loss: 1.1776 - acc: 0.5732 - val_loss: 1.2671 - val_acc: 0.5714\n",
      "Epoch 132/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 1.1795 - acc: 0.5766 - val_loss: 1.2868 - val_acc: 0.5523\n",
      "Epoch 133/1000\n",
      "7048/7048 [==============================] - 0s 52us/step - loss: 1.1893 - acc: 0.5763 - val_loss: 1.3170 - val_acc: 0.5523\n",
      "Epoch 134/1000\n",
      "7048/7048 [==============================] - 0s 49us/step - loss: 1.1886 - acc: 0.5743 - val_loss: 1.2508 - val_acc: 0.5663\n",
      "Epoch 135/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 1.2086 - acc: 0.5702 - val_loss: 1.2611 - val_acc: 0.5651\n",
      "Epoch 136/1000\n",
      "7048/7048 [==============================] - 0s 53us/step - loss: 1.1825 - acc: 0.5775 - val_loss: 1.2851 - val_acc: 0.5587\n",
      "Epoch 137/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 1.1866 - acc: 0.5741 - val_loss: 1.2701 - val_acc: 0.5765\n",
      "Epoch 138/1000\n",
      "7048/7048 [==============================] - 0s 51us/step - loss: 1.1884 - acc: 0.5694 - val_loss: 1.2646 - val_acc: 0.5727\n",
      "Epoch 139/1000\n",
      "7048/7048 [==============================] - 0s 51us/step - loss: 1.1825 - acc: 0.5743 - val_loss: 1.2804 - val_acc: 0.5689\n",
      "Epoch 140/1000\n",
      "7048/7048 [==============================] - 0s 51us/step - loss: 1.1848 - acc: 0.5741 - val_loss: 1.2903 - val_acc: 0.5638\n",
      "Epoch 141/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 1.1787 - acc: 0.5772 - val_loss: 1.2790 - val_acc: 0.5612\n",
      "Epoch 142/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 1.1820 - acc: 0.5769 - val_loss: 1.2852 - val_acc: 0.5472\n",
      "Epoch 143/1000\n",
      "7048/7048 [==============================] - 0s 52us/step - loss: 1.1786 - acc: 0.5831 - val_loss: 1.3089 - val_acc: 0.5612\n",
      "Epoch 144/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 1.1760 - acc: 0.5729 - val_loss: 1.2515 - val_acc: 0.5651\n",
      "Epoch 145/1000\n",
      "7048/7048 [==============================] - 0s 48us/step - loss: 1.1792 - acc: 0.5817 - val_loss: 1.2726 - val_acc: 0.5612\n",
      "Epoch 146/1000\n",
      "7048/7048 [==============================] - 0s 48us/step - loss: 1.1807 - acc: 0.5729 - val_loss: 1.2668 - val_acc: 0.5625\n",
      "Epoch 147/1000\n",
      "7048/7048 [==============================] - 0s 49us/step - loss: 1.1744 - acc: 0.5779 - val_loss: 1.3009 - val_acc: 0.5587\n",
      "Epoch 148/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 1.1935 - acc: 0.5726 - val_loss: 1.2820 - val_acc: 0.5740\n",
      "Epoch 149/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 1.1738 - acc: 0.5790 - val_loss: 1.2847 - val_acc: 0.5561\n",
      "Epoch 150/1000\n",
      "7048/7048 [==============================] - 0s 47us/step - loss: 1.1882 - acc: 0.5766 - val_loss: 1.2772 - val_acc: 0.5676\n",
      "Epoch 151/1000\n",
      "7048/7048 [==============================] - 0s 52us/step - loss: 1.1714 - acc: 0.5871 - val_loss: 1.2787 - val_acc: 0.5625\n",
      "Epoch 152/1000\n",
      "7048/7048 [==============================] - 0s 54us/step - loss: 1.1780 - acc: 0.5807 - val_loss: 1.2790 - val_acc: 0.5714\n",
      "Epoch 153/1000\n",
      "7048/7048 [==============================] - 0s 53us/step - loss: 1.1864 - acc: 0.5755 - val_loss: 1.2480 - val_acc: 0.5587\n",
      "Epoch 154/1000\n",
      "7048/7048 [==============================] - 0s 52us/step - loss: 1.1772 - acc: 0.5775 - val_loss: 1.2593 - val_acc: 0.5587\n",
      "Epoch 155/1000\n",
      "7048/7048 [==============================] - 0s 52us/step - loss: 1.1714 - acc: 0.5841 - val_loss: 1.2598 - val_acc: 0.5663\n",
      "Epoch 156/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 1.1775 - acc: 0.5813 - val_loss: 1.2618 - val_acc: 0.5663\n",
      "Epoch 157/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 1.1800 - acc: 0.5770 - val_loss: 1.2869 - val_acc: 0.5625\n",
      "Epoch 158/1000\n",
      "7048/7048 [==============================] - 0s 47us/step - loss: 1.1909 - acc: 0.5722 - val_loss: 1.2555 - val_acc: 0.5689\n",
      "Epoch 159/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 1.1705 - acc: 0.5749 - val_loss: 1.2616 - val_acc: 0.5663\n",
      "Epoch 160/1000\n",
      "7048/7048 [==============================] - 0s 64us/step - loss: 1.1742 - acc: 0.5786 - val_loss: 1.2550 - val_acc: 0.5599\n",
      "Epoch 161/1000\n",
      "7048/7048 [==============================] - 1s 76us/step - loss: 1.1719 - acc: 0.5802 - val_loss: 1.3029 - val_acc: 0.5625\n",
      "Epoch 162/1000\n",
      "7048/7048 [==============================] - 0s 62us/step - loss: 1.1755 - acc: 0.5789 - val_loss: 1.2424 - val_acc: 0.5561\n",
      "Epoch 163/1000\n",
      "7048/7048 [==============================] - 0s 57us/step - loss: 1.1656 - acc: 0.5773 - val_loss: 1.2841 - val_acc: 0.5676\n",
      "Epoch 164/1000\n",
      "7048/7048 [==============================] - 0s 63us/step - loss: 1.1737 - acc: 0.5738 - val_loss: 1.2858 - val_acc: 0.5727\n",
      "Epoch 165/1000\n",
      "7048/7048 [==============================] - 0s 62us/step - loss: 1.1722 - acc: 0.5812 - val_loss: 1.2851 - val_acc: 0.5561\n",
      "Epoch 166/1000\n",
      "7048/7048 [==============================] - 0s 57us/step - loss: 1.1790 - acc: 0.5718 - val_loss: 1.2729 - val_acc: 0.5459\n",
      "Epoch 167/1000\n",
      "7048/7048 [==============================] - 0s 63us/step - loss: 1.1901 - acc: 0.5778 - val_loss: 1.2890 - val_acc: 0.5638\n",
      "Epoch 168/1000\n",
      "7048/7048 [==============================] - 0s 71us/step - loss: 1.1688 - acc: 0.5759 - val_loss: 1.2692 - val_acc: 0.5587\n",
      "Epoch 169/1000\n",
      "7048/7048 [==============================] - 0s 68us/step - loss: 1.1625 - acc: 0.5844 - val_loss: 1.2902 - val_acc: 0.5689\n",
      "Epoch 170/1000\n",
      "7048/7048 [==============================] - 0s 60us/step - loss: 1.1753 - acc: 0.5809 - val_loss: 1.2933 - val_acc: 0.5574\n",
      "Epoch 171/1000\n",
      "7048/7048 [==============================] - 0s 58us/step - loss: 1.1934 - acc: 0.5721 - val_loss: 1.2812 - val_acc: 0.5574\n",
      "Epoch 172/1000\n",
      "7048/7048 [==============================] - 0s 58us/step - loss: 1.1637 - acc: 0.5802 - val_loss: 1.3021 - val_acc: 0.5548\n",
      "Epoch 173/1000\n",
      "7048/7048 [==============================] - 0s 58us/step - loss: 1.1648 - acc: 0.5816 - val_loss: 1.2581 - val_acc: 0.5421\n",
      "Epoch 174/1000\n",
      "7048/7048 [==============================] - 0s 64us/step - loss: 1.1795 - acc: 0.5738 - val_loss: 1.2576 - val_acc: 0.5561\n",
      "Epoch 175/1000\n",
      "7048/7048 [==============================] - 0s 65us/step - loss: 1.1605 - acc: 0.5792 - val_loss: 1.2676 - val_acc: 0.5651\n",
      "Epoch 176/1000\n",
      "7048/7048 [==============================] - 0s 49us/step - loss: 1.1724 - acc: 0.5786 - val_loss: 1.2746 - val_acc: 0.5816\n",
      "Epoch 177/1000\n",
      "7048/7048 [==============================] - 0s 56us/step - loss: 1.1522 - acc: 0.5863 - val_loss: 1.2783 - val_acc: 0.5638\n",
      "Epoch 178/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7048/7048 [==============================] - 0s 49us/step - loss: 1.1574 - acc: 0.5895 - val_loss: 1.2717 - val_acc: 0.5714\n",
      "Epoch 179/1000\n",
      "7048/7048 [==============================] - 0s 49us/step - loss: 1.1581 - acc: 0.5857 - val_loss: 1.3063 - val_acc: 0.5536\n",
      "Epoch 180/1000\n",
      "7048/7048 [==============================] - 0s 49us/step - loss: 1.1597 - acc: 0.5816 - val_loss: 1.2848 - val_acc: 0.5778\n",
      "Epoch 181/1000\n",
      "7048/7048 [==============================] - 0s 49us/step - loss: 1.1718 - acc: 0.5804 - val_loss: 1.2728 - val_acc: 0.5638\n",
      "Epoch 182/1000\n",
      "7048/7048 [==============================] - 0s 48us/step - loss: 1.1574 - acc: 0.5839 - val_loss: 1.2751 - val_acc: 0.5651\n",
      "Epoch 183/1000\n",
      "7048/7048 [==============================] - 0s 47us/step - loss: 1.1623 - acc: 0.5861 - val_loss: 1.2766 - val_acc: 0.5536\n",
      "Epoch 184/1000\n",
      "7048/7048 [==============================] - 0s 49us/step - loss: 1.1540 - acc: 0.5820 - val_loss: 1.2639 - val_acc: 0.5497\n",
      "Epoch 185/1000\n",
      "7048/7048 [==============================] - 0s 56us/step - loss: 1.1639 - acc: 0.5820 - val_loss: 1.2662 - val_acc: 0.5740\n",
      "Epoch 186/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 1.1627 - acc: 0.5780 - val_loss: 1.2629 - val_acc: 0.5612\n",
      "Epoch 187/1000\n",
      "7048/7048 [==============================] - 0s 49us/step - loss: 1.1745 - acc: 0.5766 - val_loss: 1.2580 - val_acc: 0.5485\n",
      "Epoch 188/1000\n",
      "7048/7048 [==============================] - 0s 53us/step - loss: 1.1793 - acc: 0.5796 - val_loss: 1.2627 - val_acc: 0.5765\n",
      "Epoch 189/1000\n",
      "7048/7048 [==============================] - 0s 55us/step - loss: 1.2078 - acc: 0.5745 - val_loss: 1.3176 - val_acc: 0.5625\n",
      "Epoch 190/1000\n",
      "7048/7048 [==============================] - 0s 54us/step - loss: 1.1788 - acc: 0.5778 - val_loss: 1.2748 - val_acc: 0.5625\n",
      "Epoch 191/1000\n",
      "7048/7048 [==============================] - 0s 51us/step - loss: 1.1650 - acc: 0.5810 - val_loss: 1.2628 - val_acc: 0.5651\n",
      "Epoch 192/1000\n",
      "7048/7048 [==============================] - 0s 51us/step - loss: 1.1623 - acc: 0.5867 - val_loss: 1.2520 - val_acc: 0.5561\n",
      "Epoch 193/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 1.1653 - acc: 0.5848 - val_loss: 1.2974 - val_acc: 0.5523\n",
      "Epoch 194/1000\n",
      "7048/7048 [==============================] - 0s 52us/step - loss: 1.1615 - acc: 0.5824 - val_loss: 1.2814 - val_acc: 0.5663\n",
      "Epoch 195/1000\n",
      "7048/7048 [==============================] - 0s 49us/step - loss: 1.1605 - acc: 0.5860 - val_loss: 1.2642 - val_acc: 0.5574\n",
      "Epoch 196/1000\n",
      "7048/7048 [==============================] - 0s 48us/step - loss: 1.1542 - acc: 0.5833 - val_loss: 1.2699 - val_acc: 0.5485\n",
      "Epoch 197/1000\n",
      "7048/7048 [==============================] - 0s 49us/step - loss: 1.1537 - acc: 0.5839 - val_loss: 1.2760 - val_acc: 0.5510\n",
      "Epoch 198/1000\n",
      "7048/7048 [==============================] - 0s 49us/step - loss: 1.1649 - acc: 0.5795 - val_loss: 1.2742 - val_acc: 0.5625\n",
      "Epoch 199/1000\n",
      "7048/7048 [==============================] - 0s 49us/step - loss: 1.1517 - acc: 0.5827 - val_loss: 1.2882 - val_acc: 0.5599\n",
      "Epoch 200/1000\n",
      "7048/7048 [==============================] - 0s 49us/step - loss: 1.1495 - acc: 0.5857 - val_loss: 1.2715 - val_acc: 0.5587\n",
      "Epoch 201/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 1.1449 - acc: 0.5895 - val_loss: 1.2948 - val_acc: 0.5612\n",
      "Epoch 202/1000\n",
      "7048/7048 [==============================] - 0s 62us/step - loss: 1.1493 - acc: 0.5919 - val_loss: 1.2876 - val_acc: 0.5548\n",
      "Epoch 203/1000\n",
      "7048/7048 [==============================] - 0s 56us/step - loss: 1.1614 - acc: 0.5877 - val_loss: 1.2589 - val_acc: 0.5625\n",
      "Epoch 204/1000\n",
      "7048/7048 [==============================] - 0s 61us/step - loss: 1.1423 - acc: 0.5911 - val_loss: 1.2832 - val_acc: 0.5663\n",
      "Epoch 205/1000\n",
      "7048/7048 [==============================] - 0s 53us/step - loss: 1.1541 - acc: 0.5870 - val_loss: 1.2961 - val_acc: 0.5714\n",
      "Epoch 206/1000\n",
      "7048/7048 [==============================] - 0s 59us/step - loss: 1.1682 - acc: 0.5840 - val_loss: 1.2724 - val_acc: 0.5574\n",
      "Epoch 207/1000\n",
      "7048/7048 [==============================] - 0s 69us/step - loss: 1.1520 - acc: 0.5898 - val_loss: 1.2754 - val_acc: 0.5599\n",
      "Epoch 208/1000\n",
      "7048/7048 [==============================] - 0s 71us/step - loss: 1.1441 - acc: 0.5936 - val_loss: 1.3447 - val_acc: 0.5714\n",
      "Epoch 209/1000\n",
      "7048/7048 [==============================] - 1s 80us/step - loss: 1.1593 - acc: 0.5854 - val_loss: 1.3000 - val_acc: 0.5625\n",
      "Epoch 210/1000\n",
      "7048/7048 [==============================] - 1s 75us/step - loss: 1.1469 - acc: 0.5878 - val_loss: 1.2821 - val_acc: 0.5548\n",
      "Epoch 211/1000\n",
      "7048/7048 [==============================] - 1s 77us/step - loss: 1.2549 - acc: 0.5501 - val_loss: 1.3009 - val_acc: 0.5612\n",
      "Epoch 212/1000\n",
      "7048/7048 [==============================] - 0s 60us/step - loss: 1.1677 - acc: 0.5826 - val_loss: 1.2655 - val_acc: 0.5638\n",
      "Epoch 213/1000\n",
      "7048/7048 [==============================] - 1s 78us/step - loss: 1.1633 - acc: 0.5839 - val_loss: 1.2812 - val_acc: 0.5638\n",
      "Epoch 214/1000\n",
      "7048/7048 [==============================] - 0s 67us/step - loss: 1.1544 - acc: 0.5857 - val_loss: 1.2917 - val_acc: 0.5599\n",
      "Epoch 215/1000\n",
      "7048/7048 [==============================] - 1s 75us/step - loss: 1.1416 - acc: 0.5928 - val_loss: 1.3190 - val_acc: 0.5651\n",
      "Epoch 216/1000\n",
      "7048/7048 [==============================] - 0s 67us/step - loss: 1.1591 - acc: 0.5891 - val_loss: 1.3276 - val_acc: 0.5676\n",
      "Epoch 217/1000\n",
      "7048/7048 [==============================] - 0s 66us/step - loss: 1.1550 - acc: 0.5841 - val_loss: 1.2876 - val_acc: 0.5587\n",
      "Epoch 218/1000\n",
      "7048/7048 [==============================] - 0s 67us/step - loss: 1.1424 - acc: 0.5912 - val_loss: 1.2808 - val_acc: 0.5485\n",
      "Epoch 219/1000\n",
      "7048/7048 [==============================] - 0s 59us/step - loss: 1.1390 - acc: 0.5922 - val_loss: 1.2840 - val_acc: 0.5689\n",
      "Epoch 220/1000\n",
      "7048/7048 [==============================] - 0s 58us/step - loss: 1.1530 - acc: 0.5863 - val_loss: 1.2979 - val_acc: 0.5561\n",
      "Epoch 221/1000\n",
      "7048/7048 [==============================] - 0s 60us/step - loss: 1.1442 - acc: 0.5928 - val_loss: 1.3012 - val_acc: 0.5612\n",
      "Epoch 222/1000\n",
      "7048/7048 [==============================] - 0s 61us/step - loss: 1.1422 - acc: 0.5890 - val_loss: 1.2903 - val_acc: 0.5497\n",
      "Epoch 223/1000\n",
      "7048/7048 [==============================] - 0s 60us/step - loss: 1.1501 - acc: 0.5890 - val_loss: 1.2892 - val_acc: 0.5599\n",
      "Epoch 224/1000\n",
      "7048/7048 [==============================] - 0s 59us/step - loss: 1.1430 - acc: 0.5863 - val_loss: 1.2917 - val_acc: 0.5523\n",
      "Epoch 225/1000\n",
      "7048/7048 [==============================] - 0s 58us/step - loss: 1.1466 - acc: 0.5875 - val_loss: 1.2896 - val_acc: 0.5663\n",
      "Epoch 226/1000\n",
      "7048/7048 [==============================] - 0s 60us/step - loss: 1.1436 - acc: 0.5854 - val_loss: 1.2913 - val_acc: 0.5676\n",
      "Epoch 227/1000\n",
      "7048/7048 [==============================] - 0s 63us/step - loss: 1.1317 - acc: 0.5955 - val_loss: 1.3122 - val_acc: 0.5638\n",
      "Epoch 228/1000\n",
      "7048/7048 [==============================] - 0s 58us/step - loss: 1.1395 - acc: 0.5891 - val_loss: 1.2907 - val_acc: 0.5651\n",
      "Epoch 229/1000\n",
      "7048/7048 [==============================] - 0s 57us/step - loss: 1.1357 - acc: 0.5885 - val_loss: 1.3293 - val_acc: 0.5574\n",
      "Epoch 230/1000\n",
      "7048/7048 [==============================] - 0s 61us/step - loss: 1.1425 - acc: 0.5904 - val_loss: 1.2968 - val_acc: 0.5625\n",
      "Epoch 231/1000\n",
      "7048/7048 [==============================] - 0s 53us/step - loss: 1.1355 - acc: 0.5921 - val_loss: 1.2787 - val_acc: 0.5625\n",
      "Epoch 232/1000\n",
      "7048/7048 [==============================] - 0s 56us/step - loss: 1.1418 - acc: 0.5888 - val_loss: 1.3053 - val_acc: 0.5612\n",
      "Epoch 233/1000\n",
      "7048/7048 [==============================] - 0s 60us/step - loss: 1.1378 - acc: 0.5905 - val_loss: 1.3039 - val_acc: 0.5574\n",
      "Epoch 234/1000\n",
      "7048/7048 [==============================] - 0s 53us/step - loss: 1.1435 - acc: 0.5942 - val_loss: 1.2810 - val_acc: 0.5651\n",
      "Epoch 235/1000\n",
      "7048/7048 [==============================] - 0s 55us/step - loss: 1.2265 - acc: 0.5638 - val_loss: 1.2870 - val_acc: 0.5625\n",
      "Epoch 236/1000\n",
      "7048/7048 [==============================] - 0s 55us/step - loss: 1.1419 - acc: 0.5908 - val_loss: 1.2895 - val_acc: 0.5561\n",
      "Epoch 237/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7048/7048 [==============================] - 0s 65us/step - loss: 1.1360 - acc: 0.5928 - val_loss: 1.2843 - val_acc: 0.5676\n",
      "Epoch 238/1000\n",
      "7048/7048 [==============================] - 0s 52us/step - loss: 1.1420 - acc: 0.5853 - val_loss: 1.2791 - val_acc: 0.5625\n",
      "Epoch 239/1000\n",
      "7048/7048 [==============================] - 0s 52us/step - loss: 1.1485 - acc: 0.5827 - val_loss: 1.2739 - val_acc: 0.5612\n",
      "Epoch 240/1000\n",
      "7048/7048 [==============================] - 0s 51us/step - loss: 1.1373 - acc: 0.5873 - val_loss: 1.2859 - val_acc: 0.5599\n",
      "Epoch 241/1000\n",
      "7048/7048 [==============================] - 0s 53us/step - loss: 1.1611 - acc: 0.5780 - val_loss: 1.2875 - val_acc: 0.5638\n",
      "Epoch 242/1000\n",
      "7048/7048 [==============================] - 0s 51us/step - loss: 1.1352 - acc: 0.5911 - val_loss: 1.2814 - val_acc: 0.5612\n",
      "Epoch 243/1000\n",
      "7048/7048 [==============================] - 0s 62us/step - loss: 1.1415 - acc: 0.5902 - val_loss: 1.3046 - val_acc: 0.5599\n",
      "Epoch 244/1000\n",
      "7048/7048 [==============================] - 0s 52us/step - loss: 1.1385 - acc: 0.5858 - val_loss: 1.2905 - val_acc: 0.5599\n",
      "Epoch 245/1000\n",
      "7048/7048 [==============================] - 0s 59us/step - loss: 1.1259 - acc: 0.5951 - val_loss: 1.2896 - val_acc: 0.5663\n",
      "Epoch 246/1000\n",
      "7048/7048 [==============================] - 0s 52us/step - loss: 1.1321 - acc: 0.5915 - val_loss: 1.3002 - val_acc: 0.5548\n",
      "Epoch 247/1000\n",
      "7048/7048 [==============================] - 0s 61us/step - loss: 1.1424 - acc: 0.5846 - val_loss: 1.3035 - val_acc: 0.5651\n",
      "Epoch 248/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 1.1330 - acc: 0.5936 - val_loss: 1.2755 - val_acc: 0.5625\n",
      "Epoch 249/1000\n",
      "7048/7048 [==============================] - 0s 53us/step - loss: 1.1460 - acc: 0.5881 - val_loss: 1.3005 - val_acc: 0.5651\n",
      "Epoch 250/1000\n",
      "7048/7048 [==============================] - 0s 65us/step - loss: 1.1250 - acc: 0.5905 - val_loss: 1.3058 - val_acc: 0.5612\n",
      "Epoch 251/1000\n",
      "7048/7048 [==============================] - 0s 67us/step - loss: 1.1307 - acc: 0.5949 - val_loss: 1.3045 - val_acc: 0.5740\n",
      "Epoch 252/1000\n",
      "7048/7048 [==============================] - 0s 69us/step - loss: 1.1295 - acc: 0.5996 - val_loss: 1.2701 - val_acc: 0.5587\n",
      "Epoch 253/1000\n",
      "7048/7048 [==============================] - 0s 63us/step - loss: 1.1368 - acc: 0.5867 - val_loss: 1.2765 - val_acc: 0.5625\n",
      "Epoch 254/1000\n",
      "7048/7048 [==============================] - 0s 52us/step - loss: 1.1312 - acc: 0.5911 - val_loss: 1.2725 - val_acc: 0.5651\n",
      "Epoch 255/1000\n",
      "7048/7048 [==============================] - 0s 62us/step - loss: 1.1335 - acc: 0.5955 - val_loss: 1.2769 - val_acc: 0.5714\n",
      "Epoch 256/1000\n",
      "7048/7048 [==============================] - 0s 57us/step - loss: 1.1216 - acc: 0.6023 - val_loss: 1.2921 - val_acc: 0.5497\n",
      "Epoch 257/1000\n",
      "7048/7048 [==============================] - 0s 65us/step - loss: 1.1381 - acc: 0.5919 - val_loss: 1.2728 - val_acc: 0.5753\n",
      "Epoch 258/1000\n",
      "7048/7048 [==============================] - 0s 54us/step - loss: 1.1205 - acc: 0.5963 - val_loss: 1.2914 - val_acc: 0.5727\n",
      "Epoch 259/1000\n",
      "7048/7048 [==============================] - 0s 68us/step - loss: 1.1186 - acc: 0.5990 - val_loss: 1.2905 - val_acc: 0.5574\n",
      "Epoch 260/1000\n",
      "7048/7048 [==============================] - 0s 63us/step - loss: 1.1208 - acc: 0.5961 - val_loss: 1.3064 - val_acc: 0.5459\n",
      "Epoch 261/1000\n",
      "7048/7048 [==============================] - 0s 64us/step - loss: 1.1142 - acc: 0.6044 - val_loss: 1.2820 - val_acc: 0.5714\n",
      "Epoch 262/1000\n",
      "7048/7048 [==============================] - 1s 73us/step - loss: 1.1232 - acc: 0.5931 - val_loss: 1.2835 - val_acc: 0.5625\n",
      "Epoch 263/1000\n",
      "7048/7048 [==============================] - 0s 68us/step - loss: 1.1215 - acc: 0.5973 - val_loss: 1.2835 - val_acc: 0.5638\n",
      "Epoch 264/1000\n",
      "7048/7048 [==============================] - 0s 48us/step - loss: 1.1659 - acc: 0.5793 - val_loss: 1.2958 - val_acc: 0.5765\n",
      "Epoch 265/1000\n",
      "7048/7048 [==============================] - 0s 63us/step - loss: 1.1218 - acc: 0.5983 - val_loss: 1.3099 - val_acc: 0.5651\n",
      "Epoch 266/1000\n",
      "7048/7048 [==============================] - 0s 52us/step - loss: 1.1243 - acc: 0.5970 - val_loss: 1.2716 - val_acc: 0.5561\n",
      "Epoch 267/1000\n",
      "7048/7048 [==============================] - 0s 59us/step - loss: 1.1274 - acc: 0.5939 - val_loss: 1.2613 - val_acc: 0.5561\n",
      "Epoch 268/1000\n",
      "7048/7048 [==============================] - 0s 54us/step - loss: 1.1364 - acc: 0.5929 - val_loss: 1.3299 - val_acc: 0.5638\n",
      "Epoch 269/1000\n",
      "7048/7048 [==============================] - 0s 61us/step - loss: 1.1467 - acc: 0.5895 - val_loss: 1.2895 - val_acc: 0.5689\n",
      "Epoch 270/1000\n",
      "7048/7048 [==============================] - 0s 60us/step - loss: 1.1145 - acc: 0.6023 - val_loss: 1.2779 - val_acc: 0.5778\n",
      "Epoch 271/1000\n",
      "7048/7048 [==============================] - 0s 69us/step - loss: 1.1251 - acc: 0.5915 - val_loss: 1.3347 - val_acc: 0.5574\n",
      "Epoch 272/1000\n",
      "7048/7048 [==============================] - 0s 52us/step - loss: 1.1297 - acc: 0.5912 - val_loss: 1.2731 - val_acc: 0.5612\n",
      "Epoch 273/1000\n",
      "7048/7048 [==============================] - 0s 63us/step - loss: 1.1108 - acc: 0.6020 - val_loss: 1.2625 - val_acc: 0.5612\n",
      "Epoch 274/1000\n",
      "7048/7048 [==============================] - 0s 66us/step - loss: 1.1157 - acc: 0.5992 - val_loss: 1.2780 - val_acc: 0.5485\n",
      "Epoch 275/1000\n",
      "7048/7048 [==============================] - 0s 58us/step - loss: 1.1221 - acc: 0.5982 - val_loss: 1.3172 - val_acc: 0.5587\n",
      "Epoch 276/1000\n",
      "7048/7048 [==============================] - 0s 64us/step - loss: 1.1218 - acc: 0.5951 - val_loss: 1.2927 - val_acc: 0.5523\n",
      "Epoch 277/1000\n",
      "7048/7048 [==============================] - 0s 59us/step - loss: 1.1154 - acc: 0.5975 - val_loss: 1.2870 - val_acc: 0.5702\n",
      "Epoch 278/1000\n",
      "7048/7048 [==============================] - 0s 57us/step - loss: 1.1090 - acc: 0.6040 - val_loss: 1.2865 - val_acc: 0.5676\n",
      "Epoch 279/1000\n",
      "7048/7048 [==============================] - 0s 52us/step - loss: 1.1144 - acc: 0.5961 - val_loss: 1.3005 - val_acc: 0.5625\n",
      "Epoch 280/1000\n",
      "7048/7048 [==============================] - 0s 49us/step - loss: 1.1165 - acc: 0.5989 - val_loss: 1.2773 - val_acc: 0.5612\n",
      "Epoch 281/1000\n",
      "7048/7048 [==============================] - 0s 49us/step - loss: 1.1217 - acc: 0.5997 - val_loss: 1.2733 - val_acc: 0.5676\n",
      "Epoch 282/1000\n",
      "7048/7048 [==============================] - 0s 52us/step - loss: 1.1152 - acc: 0.5963 - val_loss: 1.3319 - val_acc: 0.5485\n",
      "Epoch 283/1000\n",
      "7048/7048 [==============================] - 0s 49us/step - loss: 1.1247 - acc: 0.5953 - val_loss: 1.2715 - val_acc: 0.5689\n",
      "Epoch 284/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 1.1192 - acc: 0.5961 - val_loss: 1.3091 - val_acc: 0.5561\n",
      "Epoch 285/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 1.1167 - acc: 0.6009 - val_loss: 1.3088 - val_acc: 0.5574\n",
      "Epoch 286/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 1.1184 - acc: 0.5988 - val_loss: 1.3158 - val_acc: 0.5536\n",
      "Epoch 287/1000\n",
      "7048/7048 [==============================] - 0s 49us/step - loss: 1.1147 - acc: 0.6006 - val_loss: 1.2914 - val_acc: 0.5510\n",
      "Epoch 288/1000\n",
      "7048/7048 [==============================] - 0s 49us/step - loss: 1.1172 - acc: 0.5952 - val_loss: 1.2969 - val_acc: 0.5625\n",
      "Epoch 289/1000\n",
      "7048/7048 [==============================] - 0s 49us/step - loss: 1.1122 - acc: 0.5978 - val_loss: 1.2962 - val_acc: 0.5702\n",
      "Epoch 290/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 1.1212 - acc: 0.5961 - val_loss: 1.2915 - val_acc: 0.5702\n",
      "Epoch 291/1000\n",
      "7048/7048 [==============================] - 0s 49us/step - loss: 1.1040 - acc: 0.6029 - val_loss: 1.3205 - val_acc: 0.5612\n",
      "Epoch 292/1000\n",
      "7048/7048 [==============================] - 0s 49us/step - loss: 1.1228 - acc: 0.6068 - val_loss: 1.3222 - val_acc: 0.5638\n",
      "Epoch 293/1000\n",
      "7048/7048 [==============================] - 0s 48us/step - loss: 1.1121 - acc: 0.6000 - val_loss: 1.3072 - val_acc: 0.5612\n",
      "Epoch 294/1000\n",
      "7048/7048 [==============================] - 0s 49us/step - loss: 1.1207 - acc: 0.6019 - val_loss: 1.2978 - val_acc: 0.5561\n",
      "Epoch 295/1000\n",
      "7048/7048 [==============================] - 0s 51us/step - loss: 1.1229 - acc: 0.5968 - val_loss: 1.2878 - val_acc: 0.5753\n",
      "Epoch 296/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7048/7048 [==============================] - 0s 49us/step - loss: 1.1052 - acc: 0.6057 - val_loss: 1.3307 - val_acc: 0.5421\n",
      "Epoch 297/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 1.1119 - acc: 0.6002 - val_loss: 1.3212 - val_acc: 0.5612\n",
      "Epoch 298/1000\n",
      "7048/7048 [==============================] - 0s 49us/step - loss: 1.1084 - acc: 0.5970 - val_loss: 1.2897 - val_acc: 0.5625\n",
      "Epoch 299/1000\n",
      "7048/7048 [==============================] - ETA: 0s - loss: 1.1089 - acc: 0.602 - 0s 49us/step - loss: 1.1084 - acc: 0.6026 - val_loss: 1.2934 - val_acc: 0.5536\n",
      "Epoch 300/1000\n",
      "7048/7048 [==============================] - 0s 49us/step - loss: 1.1271 - acc: 0.5907 - val_loss: 1.3197 - val_acc: 0.5778\n",
      "Epoch 301/1000\n",
      "7048/7048 [==============================] - 0s 49us/step - loss: 1.1171 - acc: 0.6009 - val_loss: 1.2909 - val_acc: 0.5676\n",
      "Epoch 302/1000\n",
      "7048/7048 [==============================] - 0s 49us/step - loss: 1.0998 - acc: 0.6031 - val_loss: 1.3138 - val_acc: 0.5663\n",
      "Epoch 303/1000\n",
      "7048/7048 [==============================] - 0s 49us/step - loss: 1.0996 - acc: 0.6033 - val_loss: 1.2847 - val_acc: 0.5663\n",
      "Epoch 304/1000\n",
      "7048/7048 [==============================] - 0s 49us/step - loss: 1.1016 - acc: 0.6056 - val_loss: 1.2629 - val_acc: 0.5599\n",
      "Epoch 305/1000\n",
      "7048/7048 [==============================] - 0s 51us/step - loss: 1.1481 - acc: 0.5927 - val_loss: 1.3437 - val_acc: 0.5357\n",
      "Epoch 306/1000\n",
      "7048/7048 [==============================] - 0s 48us/step - loss: 1.1594 - acc: 0.5839 - val_loss: 1.2819 - val_acc: 0.5663\n",
      "Epoch 307/1000\n",
      "7048/7048 [==============================] - 0s 51us/step - loss: 1.1143 - acc: 0.6005 - val_loss: 1.2844 - val_acc: 0.5625\n",
      "Epoch 308/1000\n",
      "7048/7048 [==============================] - 0s 51us/step - loss: 1.1116 - acc: 0.6024 - val_loss: 1.2716 - val_acc: 0.5485\n",
      "Epoch 309/1000\n",
      "7048/7048 [==============================] - 0s 51us/step - loss: 1.1062 - acc: 0.6066 - val_loss: 1.2812 - val_acc: 0.5561\n",
      "Epoch 310/1000\n",
      "7048/7048 [==============================] - 0s 49us/step - loss: 1.1094 - acc: 0.6002 - val_loss: 1.2840 - val_acc: 0.5638\n",
      "Epoch 311/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 1.1108 - acc: 0.6024 - val_loss: 1.2962 - val_acc: 0.5612\n",
      "Epoch 312/1000\n",
      "7048/7048 [==============================] - 0s 49us/step - loss: 1.1016 - acc: 0.6051 - val_loss: 1.2921 - val_acc: 0.5625\n",
      "Epoch 313/1000\n",
      "7048/7048 [==============================] - 0s 49us/step - loss: 1.1084 - acc: 0.5959 - val_loss: 1.3206 - val_acc: 0.5714\n",
      "Epoch 314/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 1.0987 - acc: 0.6068 - val_loss: 1.3089 - val_acc: 0.5587\n",
      "Epoch 315/1000\n",
      "7048/7048 [==============================] - 0s 49us/step - loss: 1.1013 - acc: 0.6078 - val_loss: 1.3256 - val_acc: 0.5638\n",
      "Epoch 316/1000\n",
      "7048/7048 [==============================] - 0s 49us/step - loss: 1.1030 - acc: 0.6037 - val_loss: 1.3109 - val_acc: 0.5676\n",
      "Epoch 317/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 1.1059 - acc: 0.6009 - val_loss: 1.2922 - val_acc: 0.5638\n",
      "Epoch 318/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 1.1053 - acc: 0.6026 - val_loss: 1.2937 - val_acc: 0.5536\n",
      "Epoch 319/1000\n",
      "7048/7048 [==============================] - 0s 49us/step - loss: 1.1037 - acc: 0.6073 - val_loss: 1.2937 - val_acc: 0.5663\n",
      "Epoch 320/1000\n",
      "7048/7048 [==============================] - 0s 51us/step - loss: 1.1032 - acc: 0.6017 - val_loss: 1.2751 - val_acc: 0.5548\n",
      "Epoch 321/1000\n",
      "7048/7048 [==============================] - 0s 65us/step - loss: 1.1680 - acc: 0.5895 - val_loss: 1.3164 - val_acc: 0.5599\n",
      "Epoch 322/1000\n",
      "7048/7048 [==============================] - 0s 62us/step - loss: 1.1319 - acc: 0.5968 - val_loss: 1.2866 - val_acc: 0.5612\n",
      "Epoch 323/1000\n",
      "7048/7048 [==============================] - 0s 57us/step - loss: 1.1372 - acc: 0.5966 - val_loss: 1.2881 - val_acc: 0.5612\n",
      "Epoch 324/1000\n",
      "7048/7048 [==============================] - 0s 53us/step - loss: 1.1203 - acc: 0.5980 - val_loss: 1.2937 - val_acc: 0.5676\n",
      "Epoch 325/1000\n",
      "7048/7048 [==============================] - 0s 51us/step - loss: 1.1118 - acc: 0.6012 - val_loss: 1.3026 - val_acc: 0.5561\n",
      "Epoch 326/1000\n",
      "7048/7048 [==============================] - 0s 54us/step - loss: 1.1467 - acc: 0.5865 - val_loss: 1.2738 - val_acc: 0.5765\n",
      "Epoch 327/1000\n",
      "7048/7048 [==============================] - 0s 54us/step - loss: 1.1061 - acc: 0.6041 - val_loss: 1.3028 - val_acc: 0.5689\n",
      "Epoch 328/1000\n",
      "7048/7048 [==============================] - 0s 53us/step - loss: 1.1018 - acc: 0.6064 - val_loss: 1.2838 - val_acc: 0.5651\n",
      "Epoch 329/1000\n",
      "7048/7048 [==============================] - 0s 52us/step - loss: 1.1040 - acc: 0.6017 - val_loss: 1.2754 - val_acc: 0.5753\n",
      "Epoch 330/1000\n",
      "7048/7048 [==============================] - 0s 51us/step - loss: 1.1153 - acc: 0.5979 - val_loss: 1.2848 - val_acc: 0.5791\n",
      "Epoch 331/1000\n",
      "7048/7048 [==============================] - 0s 63us/step - loss: 1.1136 - acc: 0.6005 - val_loss: 1.2893 - val_acc: 0.5599\n",
      "Epoch 332/1000\n",
      "7048/7048 [==============================] - 0s 68us/step - loss: 1.1063 - acc: 0.6071 - val_loss: 1.2769 - val_acc: 0.5689\n",
      "Epoch 333/1000\n",
      "7048/7048 [==============================] - 0s 63us/step - loss: 1.1017 - acc: 0.6019 - val_loss: 1.3326 - val_acc: 0.5663\n",
      "Epoch 334/1000\n",
      "7048/7048 [==============================] - 0s 54us/step - loss: 1.1057 - acc: 0.6012 - val_loss: 1.2963 - val_acc: 0.5561\n",
      "Epoch 335/1000\n",
      "7048/7048 [==============================] - 0s 56us/step - loss: 1.1125 - acc: 0.6027 - val_loss: 1.2772 - val_acc: 0.5638\n",
      "Epoch 336/1000\n",
      "7048/7048 [==============================] - 0s 64us/step - loss: 1.0884 - acc: 0.6087 - val_loss: 1.2789 - val_acc: 0.5599\n",
      "Epoch 337/1000\n",
      "7048/7048 [==============================] - 0s 62us/step - loss: 1.1176 - acc: 0.6023 - val_loss: 1.3243 - val_acc: 0.5434\n",
      "Epoch 338/1000\n",
      "7048/7048 [==============================] - 0s 58us/step - loss: 1.1093 - acc: 0.6014 - val_loss: 1.2962 - val_acc: 0.5702\n",
      "Epoch 339/1000\n",
      "7048/7048 [==============================] - 0s 51us/step - loss: 1.1061 - acc: 0.6037 - val_loss: 1.3090 - val_acc: 0.5574\n",
      "Epoch 340/1000\n",
      "7048/7048 [==============================] - 0s 51us/step - loss: 1.1083 - acc: 0.5999 - val_loss: 1.3035 - val_acc: 0.5574\n",
      "Epoch 341/1000\n",
      "7048/7048 [==============================] - 0s 51us/step - loss: 1.1186 - acc: 0.6030 - val_loss: 1.2883 - val_acc: 0.5587\n",
      "Epoch 342/1000\n",
      "7048/7048 [==============================] - 0s 53us/step - loss: 1.1057 - acc: 0.6033 - val_loss: 1.2905 - val_acc: 0.5638\n",
      "Epoch 343/1000\n",
      "7048/7048 [==============================] - 0s 51us/step - loss: 1.1090 - acc: 0.6012 - val_loss: 1.3317 - val_acc: 0.5574\n",
      "Epoch 344/1000\n",
      "7048/7048 [==============================] - 0s 55us/step - loss: 1.1105 - acc: 0.6034 - val_loss: 1.2973 - val_acc: 0.5587\n",
      "Epoch 345/1000\n",
      "7048/7048 [==============================] - 0s 56us/step - loss: 1.1165 - acc: 0.5979 - val_loss: 1.3242 - val_acc: 0.5638\n",
      "Epoch 346/1000\n",
      "7048/7048 [==============================] - 0s 53us/step - loss: 1.1039 - acc: 0.6030 - val_loss: 1.3006 - val_acc: 0.5523\n",
      "Epoch 347/1000\n",
      "7048/7048 [==============================] - 0s 52us/step - loss: 1.0929 - acc: 0.6058 - val_loss: 1.2934 - val_acc: 0.5548\n",
      "Epoch 348/1000\n",
      "7048/7048 [==============================] - 0s 54us/step - loss: 1.1031 - acc: 0.5990 - val_loss: 1.3107 - val_acc: 0.5561\n",
      "Epoch 349/1000\n",
      "7048/7048 [==============================] - 0s 55us/step - loss: 1.0894 - acc: 0.6102 - val_loss: 1.3047 - val_acc: 0.5689\n",
      "Epoch 350/1000\n",
      "7048/7048 [==============================] - 0s 55us/step - loss: 1.0897 - acc: 0.6067 - val_loss: 1.2986 - val_acc: 0.5625\n",
      "Epoch 351/1000\n",
      "7048/7048 [==============================] - 0s 55us/step - loss: 1.0866 - acc: 0.6080 - val_loss: 1.2815 - val_acc: 0.5510\n",
      "Epoch 352/1000\n",
      "7048/7048 [==============================] - 0s 55us/step - loss: 1.1002 - acc: 0.6060 - val_loss: 1.3005 - val_acc: 0.5536\n",
      "Epoch 353/1000\n",
      "7048/7048 [==============================] - 0s 53us/step - loss: 1.1067 - acc: 0.6057 - val_loss: 1.3218 - val_acc: 0.5587\n",
      "Epoch 354/1000\n",
      "7048/7048 [==============================] - 0s 55us/step - loss: 1.0826 - acc: 0.6122 - val_loss: 1.2946 - val_acc: 0.5651\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 355/1000\n",
      "7048/7048 [==============================] - 0s 53us/step - loss: 1.0970 - acc: 0.6068 - val_loss: 1.3096 - val_acc: 0.5599\n",
      "Epoch 356/1000\n",
      "7048/7048 [==============================] - 0s 58us/step - loss: 1.0913 - acc: 0.6056 - val_loss: 1.3277 - val_acc: 0.5676\n",
      "Epoch 357/1000\n",
      "7048/7048 [==============================] - 0s 53us/step - loss: 1.0956 - acc: 0.6105 - val_loss: 1.3147 - val_acc: 0.5663\n",
      "Epoch 358/1000\n",
      "7048/7048 [==============================] - 0s 56us/step - loss: 1.1023 - acc: 0.5965 - val_loss: 1.3012 - val_acc: 0.5651\n",
      "Epoch 359/1000\n",
      "7048/7048 [==============================] - 0s 52us/step - loss: 1.0875 - acc: 0.6070 - val_loss: 1.2906 - val_acc: 0.5676\n",
      "Epoch 360/1000\n",
      "7048/7048 [==============================] - 0s 53us/step - loss: 1.0900 - acc: 0.6067 - val_loss: 1.3057 - val_acc: 0.5497\n",
      "Epoch 361/1000\n",
      "7048/7048 [==============================] - ETA: 0s - loss: 1.1062 - acc: 0.607 - 0s 50us/step - loss: 1.1041 - acc: 0.6090 - val_loss: 1.2904 - val_acc: 0.5663\n",
      "Epoch 362/1000\n",
      "7048/7048 [==============================] - 0s 52us/step - loss: 1.1039 - acc: 0.6031 - val_loss: 1.3215 - val_acc: 0.5561\n",
      "Epoch 363/1000\n",
      "7048/7048 [==============================] - 0s 49us/step - loss: 1.0955 - acc: 0.6036 - val_loss: 1.3087 - val_acc: 0.5625\n",
      "Epoch 364/1000\n",
      "7048/7048 [==============================] - 0s 49us/step - loss: 1.0906 - acc: 0.6136 - val_loss: 1.3071 - val_acc: 0.5676\n",
      "Epoch 365/1000\n",
      "7048/7048 [==============================] - 0s 51us/step - loss: 1.1271 - acc: 0.5955 - val_loss: 1.3264 - val_acc: 0.5612\n",
      "Epoch 366/1000\n",
      "7048/7048 [==============================] - 0s 54us/step - loss: 1.0965 - acc: 0.6041 - val_loss: 1.3042 - val_acc: 0.5676\n",
      "Epoch 367/1000\n",
      "7048/7048 [==============================] - 0s 55us/step - loss: 1.0800 - acc: 0.6124 - val_loss: 1.2997 - val_acc: 0.5676\n",
      "Epoch 368/1000\n",
      "7048/7048 [==============================] - 0s 51us/step - loss: 1.0872 - acc: 0.6064 - val_loss: 1.3095 - val_acc: 0.5587\n",
      "Epoch 369/1000\n",
      "7048/7048 [==============================] - 0s 53us/step - loss: 1.0931 - acc: 0.6036 - val_loss: 1.3064 - val_acc: 0.5561\n",
      "Epoch 370/1000\n",
      "7048/7048 [==============================] - 0s 55us/step - loss: 1.0785 - acc: 0.6156 - val_loss: 1.3231 - val_acc: 0.5536\n",
      "Epoch 371/1000\n",
      "7048/7048 [==============================] - 0s 53us/step - loss: 1.0975 - acc: 0.6087 - val_loss: 1.3324 - val_acc: 0.5510\n",
      "Epoch 372/1000\n",
      "7048/7048 [==============================] - 0s 51us/step - loss: 1.1043 - acc: 0.6063 - val_loss: 1.3217 - val_acc: 0.5663\n",
      "Epoch 373/1000\n",
      "7048/7048 [==============================] - 0s 53us/step - loss: 1.0846 - acc: 0.6104 - val_loss: 1.3227 - val_acc: 0.5561\n",
      "Epoch 374/1000\n",
      "7048/7048 [==============================] - 0s 57us/step - loss: 1.0709 - acc: 0.6142 - val_loss: 1.3399 - val_acc: 0.5561\n",
      "Epoch 375/1000\n",
      "7048/7048 [==============================] - 0s 57us/step - loss: 1.0887 - acc: 0.6087 - val_loss: 1.2979 - val_acc: 0.5663\n",
      "Epoch 376/1000\n",
      "7048/7048 [==============================] - 0s 57us/step - loss: 1.0958 - acc: 0.6112 - val_loss: 1.3160 - val_acc: 0.5676\n",
      "Epoch 377/1000\n",
      "7048/7048 [==============================] - 0s 65us/step - loss: 1.0850 - acc: 0.6071 - val_loss: 1.3175 - val_acc: 0.5523\n",
      "Epoch 378/1000\n",
      "7048/7048 [==============================] - 1s 72us/step - loss: 1.0803 - acc: 0.6127 - val_loss: 1.2811 - val_acc: 0.5536\n",
      "Epoch 379/1000\n",
      "7048/7048 [==============================] - 1s 87us/step - loss: 1.0855 - acc: 0.6078 - val_loss: 1.3143 - val_acc: 0.5714\n",
      "Epoch 380/1000\n",
      "7048/7048 [==============================] - 1s 84us/step - loss: 1.0748 - acc: 0.6101 - val_loss: 1.3147 - val_acc: 0.5599\n",
      "Epoch 381/1000\n",
      "7048/7048 [==============================] - 1s 74us/step - loss: 1.0817 - acc: 0.6163 - val_loss: 1.3158 - val_acc: 0.5625\n",
      "Epoch 382/1000\n",
      "7048/7048 [==============================] - 1s 76us/step - loss: 1.0871 - acc: 0.6110 - val_loss: 1.3107 - val_acc: 0.5651\n",
      "Epoch 383/1000\n",
      "7048/7048 [==============================] - 1s 76us/step - loss: 1.0953 - acc: 0.6083 - val_loss: 1.2886 - val_acc: 0.5612\n",
      "Epoch 384/1000\n",
      "7048/7048 [==============================] - 0s 62us/step - loss: 1.0798 - acc: 0.6124 - val_loss: 1.3127 - val_acc: 0.5587\n",
      "Epoch 385/1000\n",
      "7048/7048 [==============================] - 0s 62us/step - loss: 1.0886 - acc: 0.6145 - val_loss: 1.3132 - val_acc: 0.5612\n",
      "Epoch 386/1000\n",
      "7048/7048 [==============================] - 0s 62us/step - loss: 1.0816 - acc: 0.6124 - val_loss: 1.3051 - val_acc: 0.5523\n",
      "Epoch 387/1000\n",
      "7048/7048 [==============================] - 0s 60us/step - loss: 1.0805 - acc: 0.6081 - val_loss: 1.3378 - val_acc: 0.5561\n",
      "Epoch 388/1000\n",
      "7048/7048 [==============================] - 0s 61us/step - loss: 1.0789 - acc: 0.6148 - val_loss: 1.3052 - val_acc: 0.5727\n",
      "Epoch 389/1000\n",
      "7048/7048 [==============================] - 0s 63us/step - loss: 1.0897 - acc: 0.6058 - val_loss: 1.2865 - val_acc: 0.5829\n",
      "Epoch 390/1000\n",
      "7048/7048 [==============================] - 0s 61us/step - loss: 1.0826 - acc: 0.6112 - val_loss: 1.2947 - val_acc: 0.5714\n",
      "Epoch 391/1000\n",
      "7048/7048 [==============================] - 0s 55us/step - loss: 1.0771 - acc: 0.6141 - val_loss: 1.2985 - val_acc: 0.5714\n",
      "Epoch 392/1000\n",
      "7048/7048 [==============================] - 0s 57us/step - loss: 1.0821 - acc: 0.6043 - val_loss: 1.3000 - val_acc: 0.5625\n",
      "Epoch 393/1000\n",
      "7048/7048 [==============================] - 0s 58us/step - loss: 1.1051 - acc: 0.6084 - val_loss: 1.3277 - val_acc: 0.5663\n",
      "Epoch 394/1000\n",
      "7048/7048 [==============================] - 0s 57us/step - loss: 1.0746 - acc: 0.6114 - val_loss: 1.3043 - val_acc: 0.5663\n",
      "Epoch 395/1000\n",
      "7048/7048 [==============================] - 0s 61us/step - loss: 1.0757 - acc: 0.6127 - val_loss: 1.3234 - val_acc: 0.5536\n",
      "Epoch 396/1000\n",
      "7048/7048 [==============================] - 0s 58us/step - loss: 1.0678 - acc: 0.6212 - val_loss: 1.3028 - val_acc: 0.5714\n",
      "Epoch 397/1000\n",
      "7048/7048 [==============================] - 0s 58us/step - loss: 1.0860 - acc: 0.6070 - val_loss: 1.3094 - val_acc: 0.5612\n",
      "Epoch 398/1000\n",
      "7048/7048 [==============================] - 0s 59us/step - loss: 1.0940 - acc: 0.6027 - val_loss: 1.3204 - val_acc: 0.5638\n",
      "Epoch 399/1000\n",
      "7048/7048 [==============================] - 0s 58us/step - loss: 1.0839 - acc: 0.6108 - val_loss: 1.3088 - val_acc: 0.5778\n",
      "Epoch 400/1000\n",
      "7048/7048 [==============================] - 0s 62us/step - loss: 1.0930 - acc: 0.6119 - val_loss: 1.3100 - val_acc: 0.5536\n",
      "Epoch 401/1000\n",
      "7048/7048 [==============================] - 0s 61us/step - loss: 1.0835 - acc: 0.6075 - val_loss: 1.3148 - val_acc: 0.5625\n",
      "Epoch 402/1000\n",
      "7048/7048 [==============================] - 0s 66us/step - loss: 1.1586 - acc: 0.5915 - val_loss: 1.2979 - val_acc: 0.5625\n",
      "Epoch 403/1000\n",
      "7048/7048 [==============================] - 1s 79us/step - loss: 1.0946 - acc: 0.6030 - val_loss: 1.3011 - val_acc: 0.5625\n",
      "Epoch 404/1000\n",
      "7048/7048 [==============================] - 1s 73us/step - loss: 1.0720 - acc: 0.6172 - val_loss: 1.3195 - val_acc: 0.5740\n",
      "Epoch 405/1000\n",
      "7048/7048 [==============================] - 0s 70us/step - loss: 1.0760 - acc: 0.6182 - val_loss: 1.3001 - val_acc: 0.5599\n",
      "Epoch 406/1000\n",
      "7048/7048 [==============================] - 0s 69us/step - loss: 1.0764 - acc: 0.6117 - val_loss: 1.3027 - val_acc: 0.5612\n",
      "Epoch 407/1000\n",
      "7048/7048 [==============================] - 0s 61us/step - loss: 1.0935 - acc: 0.6088 - val_loss: 1.3003 - val_acc: 0.5599\n",
      "Epoch 408/1000\n",
      "7048/7048 [==============================] - 0s 60us/step - loss: 1.0709 - acc: 0.6162 - val_loss: 1.3189 - val_acc: 0.5625\n",
      "Epoch 409/1000\n",
      "7048/7048 [==============================] - 0s 61us/step - loss: 1.0703 - acc: 0.6161 - val_loss: 1.3089 - val_acc: 0.5599\n",
      "Epoch 410/1000\n",
      "7048/7048 [==============================] - 0s 60us/step - loss: 1.0692 - acc: 0.6117 - val_loss: 1.3272 - val_acc: 0.5638\n",
      "Epoch 411/1000\n",
      "7048/7048 [==============================] - 0s 61us/step - loss: 1.0880 - acc: 0.6088 - val_loss: 1.3152 - val_acc: 0.5587\n",
      "Epoch 412/1000\n",
      "7048/7048 [==============================] - 0s 70us/step - loss: 1.0733 - acc: 0.6165 - val_loss: 1.2825 - val_acc: 0.5651\n",
      "Epoch 413/1000\n",
      "7048/7048 [==============================] - 0s 54us/step - loss: 1.0674 - acc: 0.6166 - val_loss: 1.3126 - val_acc: 0.5689\n",
      "Epoch 414/1000\n",
      "7048/7048 [==============================] - 0s 54us/step - loss: 1.0711 - acc: 0.6141 - val_loss: 1.3356 - val_acc: 0.5625\n",
      "Epoch 415/1000\n",
      "7048/7048 [==============================] - 0s 53us/step - loss: 1.0845 - acc: 0.6148 - val_loss: 1.3083 - val_acc: 0.5599\n",
      "Epoch 416/1000\n",
      "7048/7048 [==============================] - 0s 54us/step - loss: 1.0704 - acc: 0.6152 - val_loss: 1.3263 - val_acc: 0.5485\n",
      "Epoch 417/1000\n",
      "7048/7048 [==============================] - 0s 53us/step - loss: 1.0961 - acc: 0.6019 - val_loss: 1.3088 - val_acc: 0.5625\n",
      "Epoch 418/1000\n",
      "7048/7048 [==============================] - 0s 53us/step - loss: 1.0854 - acc: 0.6091 - val_loss: 1.3281 - val_acc: 0.5702\n",
      "Epoch 419/1000\n",
      "7048/7048 [==============================] - 0s 53us/step - loss: 1.0684 - acc: 0.6185 - val_loss: 1.3368 - val_acc: 0.5727\n",
      "Epoch 420/1000\n",
      "7048/7048 [==============================] - 0s 53us/step - loss: 1.0706 - acc: 0.6169 - val_loss: 1.3345 - val_acc: 0.5727\n",
      "Epoch 421/1000\n",
      "7048/7048 [==============================] - 0s 54us/step - loss: 1.0722 - acc: 0.6142 - val_loss: 1.3319 - val_acc: 0.5651\n",
      "Epoch 422/1000\n",
      "7048/7048 [==============================] - 0s 53us/step - loss: 1.0770 - acc: 0.6093 - val_loss: 1.3216 - val_acc: 0.5510\n",
      "Epoch 423/1000\n",
      "7048/7048 [==============================] - 0s 53us/step - loss: 1.0843 - acc: 0.6083 - val_loss: 1.3169 - val_acc: 0.5523\n",
      "Epoch 424/1000\n",
      "7048/7048 [==============================] - 0s 61us/step - loss: 1.0804 - acc: 0.6131 - val_loss: 1.3395 - val_acc: 0.5485\n",
      "Epoch 425/1000\n",
      "7048/7048 [==============================] - 0s 60us/step - loss: 1.1164 - acc: 0.5983 - val_loss: 1.2856 - val_acc: 0.5638\n",
      "Epoch 426/1000\n",
      "7048/7048 [==============================] - 0s 66us/step - loss: 1.0695 - acc: 0.6165 - val_loss: 1.3458 - val_acc: 0.5561\n",
      "Epoch 427/1000\n",
      "7048/7048 [==============================] - 0s 62us/step - loss: 1.0732 - acc: 0.6148 - val_loss: 1.3125 - val_acc: 0.5663\n",
      "Epoch 428/1000\n",
      "7048/7048 [==============================] - 0s 56us/step - loss: 1.0572 - acc: 0.6179 - val_loss: 1.3305 - val_acc: 0.5651\n",
      "Epoch 429/1000\n",
      "7048/7048 [==============================] - 0s 60us/step - loss: 1.0829 - acc: 0.6129 - val_loss: 1.2946 - val_acc: 0.5651\n",
      "Epoch 430/1000\n",
      "7048/7048 [==============================] - 0s 59us/step - loss: 1.0556 - acc: 0.6189 - val_loss: 1.3039 - val_acc: 0.5663\n",
      "Epoch 431/1000\n",
      "7048/7048 [==============================] - 0s 71us/step - loss: 1.0706 - acc: 0.6183 - val_loss: 1.3132 - val_acc: 0.5510\n",
      "Epoch 432/1000\n",
      "7048/7048 [==============================] - 0s 69us/step - loss: 1.0734 - acc: 0.6135 - val_loss: 1.3142 - val_acc: 0.5625\n",
      "Epoch 433/1000\n",
      "7048/7048 [==============================] - 0s 55us/step - loss: 1.0628 - acc: 0.6176 - val_loss: 1.3173 - val_acc: 0.5625\n",
      "Epoch 434/1000\n",
      "7048/7048 [==============================] - 0s 71us/step - loss: 1.0592 - acc: 0.6205 - val_loss: 1.3167 - val_acc: 0.5651\n",
      "Epoch 435/1000\n",
      "7048/7048 [==============================] - 0s 66us/step - loss: 1.0747 - acc: 0.6118 - val_loss: 1.3468 - val_acc: 0.5472\n",
      "Epoch 436/1000\n",
      "7048/7048 [==============================] - 0s 61us/step - loss: 1.0593 - acc: 0.6159 - val_loss: 1.3424 - val_acc: 0.5485\n",
      "Epoch 437/1000\n",
      "7048/7048 [==============================] - 0s 58us/step - loss: 1.0809 - acc: 0.6129 - val_loss: 1.3388 - val_acc: 0.5536\n",
      "Epoch 438/1000\n",
      "7048/7048 [==============================] - 0s 57us/step - loss: 1.0795 - acc: 0.6142 - val_loss: 1.3207 - val_acc: 0.5536\n",
      "Epoch 439/1000\n",
      "7048/7048 [==============================] - 0s 56us/step - loss: 1.1839 - acc: 0.5756 - val_loss: 1.3557 - val_acc: 0.5459\n",
      "Epoch 440/1000\n",
      "7048/7048 [==============================] - 0s 56us/step - loss: 1.1401 - acc: 0.5927 - val_loss: 1.3146 - val_acc: 0.5561\n",
      "Epoch 441/1000\n",
      "7048/7048 [==============================] - 0s 55us/step - loss: 1.0887 - acc: 0.6029 - val_loss: 1.3308 - val_acc: 0.5599\n",
      "Epoch 442/1000\n",
      "7048/7048 [==============================] - 0s 55us/step - loss: 1.1362 - acc: 0.5898 - val_loss: 1.3076 - val_acc: 0.5574\n",
      "Epoch 443/1000\n",
      "7048/7048 [==============================] - 0s 56us/step - loss: 1.0820 - acc: 0.6047 - val_loss: 1.3313 - val_acc: 0.5612\n",
      "Epoch 444/1000\n",
      "7048/7048 [==============================] - 0s 57us/step - loss: 1.0971 - acc: 0.6067 - val_loss: 1.3093 - val_acc: 0.5638\n",
      "Epoch 445/1000\n",
      "7048/7048 [==============================] - 0s 55us/step - loss: 1.0919 - acc: 0.6075 - val_loss: 1.3249 - val_acc: 0.5638\n",
      "Epoch 446/1000\n",
      "7048/7048 [==============================] - 0s 64us/step - loss: 1.0717 - acc: 0.6093 - val_loss: 1.3377 - val_acc: 0.5536\n",
      "Epoch 447/1000\n",
      "7048/7048 [==============================] - 0s 61us/step - loss: 1.1293 - acc: 0.6006 - val_loss: 1.3187 - val_acc: 0.5676\n",
      "Epoch 448/1000\n",
      "7048/7048 [==============================] - 0s 61us/step - loss: 1.0898 - acc: 0.6110 - val_loss: 1.3070 - val_acc: 0.5599\n",
      "Epoch 449/1000\n",
      "7048/7048 [==============================] - 0s 59us/step - loss: 1.0941 - acc: 0.6098 - val_loss: 1.3180 - val_acc: 0.5561\n",
      "Epoch 450/1000\n",
      "7048/7048 [==============================] - 0s 60us/step - loss: 1.0812 - acc: 0.6135 - val_loss: 1.3392 - val_acc: 0.5689\n",
      "Epoch 451/1000\n",
      "7048/7048 [==============================] - 0s 62us/step - loss: 1.0749 - acc: 0.6176 - val_loss: 1.3343 - val_acc: 0.5702\n",
      "Epoch 452/1000\n",
      "7048/7048 [==============================] - 0s 58us/step - loss: 1.0734 - acc: 0.6128 - val_loss: 1.3397 - val_acc: 0.5625\n",
      "Epoch 453/1000\n",
      "7048/7048 [==============================] - 0s 66us/step - loss: 1.0755 - acc: 0.6108 - val_loss: 1.3169 - val_acc: 0.5561\n",
      "Epoch 454/1000\n",
      "7048/7048 [==============================] - 0s 65us/step - loss: 1.0829 - acc: 0.6152 - val_loss: 1.3126 - val_acc: 0.5765\n",
      "Epoch 455/1000\n",
      "7048/7048 [==============================] - 0s 52us/step - loss: 1.0732 - acc: 0.6202 - val_loss: 1.3256 - val_acc: 0.5651\n",
      "Epoch 456/1000\n",
      "7048/7048 [==============================] - 0s 49us/step - loss: 1.0637 - acc: 0.6121 - val_loss: 1.3260 - val_acc: 0.5663\n",
      "Epoch 457/1000\n",
      "7048/7048 [==============================] - 0s 56us/step - loss: 1.1507 - acc: 0.5860 - val_loss: 1.3185 - val_acc: 0.5536\n",
      "Epoch 458/1000\n",
      "7048/7048 [==============================] - 0s 64us/step - loss: 1.0772 - acc: 0.6149 - val_loss: 1.3340 - val_acc: 0.5523\n",
      "Epoch 459/1000\n",
      "7048/7048 [==============================] - 0s 64us/step - loss: 1.0769 - acc: 0.6100 - val_loss: 1.3312 - val_acc: 0.5344\n",
      "Epoch 460/1000\n",
      "7048/7048 [==============================] - 0s 54us/step - loss: 1.0839 - acc: 0.6068 - val_loss: 1.3285 - val_acc: 0.5536\n",
      "Epoch 461/1000\n",
      "7048/7048 [==============================] - 0s 66us/step - loss: 1.0731 - acc: 0.6141 - val_loss: 1.3116 - val_acc: 0.5599\n",
      "Epoch 462/1000\n",
      "7048/7048 [==============================] - 0s 54us/step - loss: 1.0722 - acc: 0.6192 - val_loss: 1.3434 - val_acc: 0.5612\n",
      "Epoch 463/1000\n",
      "7048/7048 [==============================] - 0s 49us/step - loss: 1.0560 - acc: 0.6163 - val_loss: 1.3196 - val_acc: 0.5599\n",
      "Epoch 464/1000\n",
      "7048/7048 [==============================] - 0s 60us/step - loss: 1.0642 - acc: 0.6171 - val_loss: 1.3271 - val_acc: 0.5599\n",
      "Epoch 465/1000\n",
      "7048/7048 [==============================] - 0s 60us/step - loss: 1.0653 - acc: 0.6193 - val_loss: 1.3087 - val_acc: 0.5638\n",
      "Epoch 466/1000\n",
      "7048/7048 [==============================] - 0s 54us/step - loss: 1.0678 - acc: 0.6169 - val_loss: 1.3305 - val_acc: 0.5638\n",
      "Epoch 467/1000\n",
      "7048/7048 [==============================] - 0s 58us/step - loss: 1.0629 - acc: 0.6209 - val_loss: 1.3222 - val_acc: 0.5651\n",
      "Epoch 468/1000\n",
      "7048/7048 [==============================] - 0s 64us/step - loss: 1.0636 - acc: 0.6188 - val_loss: 1.3512 - val_acc: 0.5510\n",
      "Epoch 469/1000\n",
      "7048/7048 [==============================] - 1s 75us/step - loss: 1.0716 - acc: 0.6087 - val_loss: 1.3251 - val_acc: 0.5599\n",
      "Epoch 470/1000\n",
      "7048/7048 [==============================] - 1s 72us/step - loss: 1.0643 - acc: 0.6200 - val_loss: 1.3194 - val_acc: 0.5625\n",
      "Epoch 471/1000\n",
      "7048/7048 [==============================] - 1s 78us/step - loss: 1.0557 - acc: 0.6234 - val_loss: 1.3095 - val_acc: 0.5625\n",
      "Epoch 472/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7048/7048 [==============================] - 0s 57us/step - loss: 1.0580 - acc: 0.6223 - val_loss: 1.3031 - val_acc: 0.5599\n",
      "Epoch 473/1000\n",
      "7048/7048 [==============================] - 0s 54us/step - loss: 1.0677 - acc: 0.6149 - val_loss: 1.3240 - val_acc: 0.5561\n",
      "Epoch 474/1000\n",
      "7048/7048 [==============================] - 0s 52us/step - loss: 1.0609 - acc: 0.6154 - val_loss: 1.3094 - val_acc: 0.5599\n",
      "Epoch 475/1000\n",
      "7048/7048 [==============================] - 0s 58us/step - loss: 1.0925 - acc: 0.6136 - val_loss: 1.3352 - val_acc: 0.5536\n",
      "Epoch 476/1000\n",
      "7048/7048 [==============================] - 0s 67us/step - loss: 1.0659 - acc: 0.6134 - val_loss: 1.3232 - val_acc: 0.5523\n",
      "Epoch 477/1000\n",
      "7048/7048 [==============================] - 0s 63us/step - loss: 1.0462 - acc: 0.6246 - val_loss: 1.3344 - val_acc: 0.5561\n",
      "Epoch 478/1000\n",
      "7048/7048 [==============================] - 0s 55us/step - loss: 1.0620 - acc: 0.6196 - val_loss: 1.3325 - val_acc: 0.5561\n",
      "Epoch 479/1000\n",
      "7048/7048 [==============================] - 0s 52us/step - loss: 1.0502 - acc: 0.6200 - val_loss: 1.3362 - val_acc: 0.5497\n",
      "Epoch 480/1000\n",
      "7048/7048 [==============================] - 0s 49us/step - loss: 1.0616 - acc: 0.6161 - val_loss: 1.3540 - val_acc: 0.5574\n",
      "Epoch 481/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 1.0689 - acc: 0.6162 - val_loss: 1.3255 - val_acc: 0.5599\n",
      "Epoch 482/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 1.0521 - acc: 0.6169 - val_loss: 1.3476 - val_acc: 0.5587\n",
      "Epoch 483/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 1.0437 - acc: 0.6215 - val_loss: 1.3310 - val_acc: 0.5510\n",
      "Epoch 484/1000\n",
      "7048/7048 [==============================] - 0s 51us/step - loss: 1.0465 - acc: 0.6244 - val_loss: 1.3535 - val_acc: 0.5561\n",
      "Epoch 485/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 1.0630 - acc: 0.6172 - val_loss: 1.3315 - val_acc: 0.5638\n",
      "Epoch 486/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 1.0605 - acc: 0.6154 - val_loss: 1.3257 - val_acc: 0.5561\n",
      "Epoch 487/1000\n",
      "7048/7048 [==============================] - 0s 51us/step - loss: 1.0540 - acc: 0.6171 - val_loss: 1.3416 - val_acc: 0.5663\n",
      "Epoch 488/1000\n",
      "7048/7048 [==============================] - 0s 55us/step - loss: 1.1010 - acc: 0.6112 - val_loss: 1.3107 - val_acc: 0.5536\n",
      "Epoch 489/1000\n",
      "7048/7048 [==============================] - 0s 49us/step - loss: 1.0560 - acc: 0.6243 - val_loss: 1.3231 - val_acc: 0.5587\n",
      "Epoch 490/1000\n",
      "7048/7048 [==============================] - 0s 55us/step - loss: 1.0623 - acc: 0.6180 - val_loss: 1.3175 - val_acc: 0.5612\n",
      "Epoch 491/1000\n",
      "7048/7048 [==============================] - 0s 49us/step - loss: 1.0466 - acc: 0.6240 - val_loss: 1.3126 - val_acc: 0.5536\n",
      "Epoch 492/1000\n",
      "7048/7048 [==============================] - 0s 56us/step - loss: 1.0593 - acc: 0.6179 - val_loss: 1.3245 - val_acc: 0.5612\n",
      "Epoch 493/1000\n",
      "7048/7048 [==============================] - 0s 49us/step - loss: 1.0492 - acc: 0.6207 - val_loss: 1.3297 - val_acc: 0.5485\n",
      "Epoch 494/1000\n",
      "7048/7048 [==============================] - 0s 51us/step - loss: 1.0484 - acc: 0.6209 - val_loss: 1.3383 - val_acc: 0.5612\n",
      "Epoch 495/1000\n",
      "7048/7048 [==============================] - 0s 53us/step - loss: 1.0463 - acc: 0.6291 - val_loss: 1.3562 - val_acc: 0.5561\n",
      "Epoch 496/1000\n",
      "7048/7048 [==============================] - 0s 57us/step - loss: 1.0448 - acc: 0.6230 - val_loss: 1.3461 - val_acc: 0.5497\n",
      "Epoch 497/1000\n",
      "7048/7048 [==============================] - 0s 54us/step - loss: 1.0508 - acc: 0.6155 - val_loss: 1.3529 - val_acc: 0.5523\n",
      "Epoch 498/1000\n",
      "7048/7048 [==============================] - 0s 51us/step - loss: 1.0614 - acc: 0.6223 - val_loss: 1.3490 - val_acc: 0.5727\n",
      "Epoch 499/1000\n",
      "7048/7048 [==============================] - 0s 49us/step - loss: 1.0670 - acc: 0.6129 - val_loss: 1.3337 - val_acc: 0.5485\n",
      "Epoch 500/1000\n",
      "7048/7048 [==============================] - 0s 48us/step - loss: 1.0481 - acc: 0.6171 - val_loss: 1.3391 - val_acc: 0.5574\n",
      "Epoch 501/1000\n",
      "7048/7048 [==============================] - 0s 49us/step - loss: 1.0482 - acc: 0.6205 - val_loss: 1.3694 - val_acc: 0.5485\n",
      "Epoch 502/1000\n",
      "7048/7048 [==============================] - 0s 49us/step - loss: 1.0603 - acc: 0.6115 - val_loss: 1.3538 - val_acc: 0.5472\n",
      "Epoch 503/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 1.0371 - acc: 0.6277 - val_loss: 1.3307 - val_acc: 0.5472\n",
      "Epoch 504/1000\n",
      "7048/7048 [==============================] - 0s 49us/step - loss: 1.0926 - acc: 0.6111 - val_loss: 1.3341 - val_acc: 0.5523\n",
      "Epoch 505/1000\n",
      "7048/7048 [==============================] - 0s 49us/step - loss: 1.0605 - acc: 0.6166 - val_loss: 1.3417 - val_acc: 0.5485\n",
      "Epoch 506/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 1.0434 - acc: 0.6291 - val_loss: 1.3676 - val_acc: 0.5587\n",
      "Epoch 507/1000\n",
      "7048/7048 [==============================] - 0s 49us/step - loss: 1.0609 - acc: 0.6219 - val_loss: 1.3138 - val_acc: 0.5638\n",
      "Epoch 508/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 1.1212 - acc: 0.6007 - val_loss: 1.3258 - val_acc: 0.5574\n",
      "Epoch 509/1000\n",
      "7048/7048 [==============================] - 0s 49us/step - loss: 1.0581 - acc: 0.6209 - val_loss: 1.3192 - val_acc: 0.5625\n",
      "Epoch 510/1000\n",
      "7048/7048 [==============================] - 0s 51us/step - loss: 1.0613 - acc: 0.6193 - val_loss: 1.3539 - val_acc: 0.5651\n",
      "Epoch 511/1000\n",
      "7048/7048 [==============================] - 0s 53us/step - loss: 1.0606 - acc: 0.6210 - val_loss: 1.3441 - val_acc: 0.5459\n",
      "Epoch 512/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 1.0526 - acc: 0.6190 - val_loss: 1.3519 - val_acc: 0.5523\n",
      "Epoch 513/1000\n",
      "7048/7048 [==============================] - 0s 52us/step - loss: 1.0530 - acc: 0.6257 - val_loss: 1.3346 - val_acc: 0.5625\n",
      "Epoch 514/1000\n",
      "7048/7048 [==============================] - 0s 57us/step - loss: 1.0441 - acc: 0.6254 - val_loss: 1.3547 - val_acc: 0.5434\n",
      "Epoch 515/1000\n",
      "7048/7048 [==============================] - 0s 52us/step - loss: 1.0483 - acc: 0.6244 - val_loss: 1.3284 - val_acc: 0.5702\n",
      "Epoch 516/1000\n",
      "7048/7048 [==============================] - 0s 53us/step - loss: 1.0545 - acc: 0.6210 - val_loss: 1.3273 - val_acc: 0.5599\n",
      "Epoch 517/1000\n",
      "7048/7048 [==============================] - 0s 52us/step - loss: 1.0473 - acc: 0.6241 - val_loss: 1.3363 - val_acc: 0.5548\n",
      "Epoch 518/1000\n",
      "7048/7048 [==============================] - 0s 52us/step - loss: 1.0450 - acc: 0.6297 - val_loss: 1.3303 - val_acc: 0.5663\n",
      "Epoch 519/1000\n",
      "7048/7048 [==============================] - 0s 54us/step - loss: 1.0687 - acc: 0.6215 - val_loss: 1.3475 - val_acc: 0.5574\n",
      "Epoch 520/1000\n",
      "7048/7048 [==============================] - 0s 53us/step - loss: 1.0323 - acc: 0.6295 - val_loss: 1.3464 - val_acc: 0.5561\n",
      "Epoch 521/1000\n",
      "7048/7048 [==============================] - 0s 67us/step - loss: 1.0521 - acc: 0.6243 - val_loss: 1.3606 - val_acc: 0.5472\n",
      "Epoch 522/1000\n",
      "7048/7048 [==============================] - 0s 69us/step - loss: 1.0355 - acc: 0.6287 - val_loss: 1.3497 - val_acc: 0.5497\n",
      "Epoch 523/1000\n",
      "7048/7048 [==============================] - 1s 73us/step - loss: 1.0510 - acc: 0.6230 - val_loss: 1.3666 - val_acc: 0.5497\n",
      "Epoch 524/1000\n",
      "7048/7048 [==============================] - 0s 71us/step - loss: 1.0575 - acc: 0.6196 - val_loss: 1.3433 - val_acc: 0.5651\n",
      "Epoch 525/1000\n",
      "7048/7048 [==============================] - 1s 79us/step - loss: 1.0451 - acc: 0.6274 - val_loss: 1.3286 - val_acc: 0.5638\n",
      "Epoch 526/1000\n",
      "7048/7048 [==============================] - 0s 62us/step - loss: 1.0413 - acc: 0.6239 - val_loss: 1.3554 - val_acc: 0.5625\n",
      "Epoch 527/1000\n",
      "7048/7048 [==============================] - 0s 61us/step - loss: 1.0342 - acc: 0.6263 - val_loss: 1.3567 - val_acc: 0.5676\n",
      "Epoch 528/1000\n",
      "7048/7048 [==============================] - 1s 71us/step - loss: 1.0555 - acc: 0.6217 - val_loss: 1.3226 - val_acc: 0.5599\n",
      "Epoch 529/1000\n",
      "7048/7048 [==============================] - 1s 82us/step - loss: 1.0464 - acc: 0.6250 - val_loss: 1.3404 - val_acc: 0.5510\n",
      "Epoch 530/1000\n",
      "7048/7048 [==============================] - 1s 77us/step - loss: 1.0418 - acc: 0.6277 - val_loss: 1.3447 - val_acc: 0.5561\n",
      "Epoch 531/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7048/7048 [==============================] - 1s 72us/step - loss: 1.0572 - acc: 0.6251 - val_loss: 1.3557 - val_acc: 0.5472\n",
      "Epoch 532/1000\n",
      "7048/7048 [==============================] - 0s 69us/step - loss: 1.0488 - acc: 0.6212 - val_loss: 1.3188 - val_acc: 0.5395\n",
      "Epoch 533/1000\n",
      "7048/7048 [==============================] - 0s 68us/step - loss: 1.0388 - acc: 0.6200 - val_loss: 1.3482 - val_acc: 0.5689\n",
      "Epoch 534/1000\n",
      "7048/7048 [==============================] - 0s 64us/step - loss: 1.0337 - acc: 0.6297 - val_loss: 1.3608 - val_acc: 0.5536\n",
      "Epoch 535/1000\n",
      "7048/7048 [==============================] - 0s 55us/step - loss: 1.0455 - acc: 0.6202 - val_loss: 1.3231 - val_acc: 0.5689\n",
      "Epoch 536/1000\n",
      "7048/7048 [==============================] - 0s 55us/step - loss: 1.0406 - acc: 0.6215 - val_loss: 1.3516 - val_acc: 0.5497\n",
      "Epoch 537/1000\n",
      "7048/7048 [==============================] - 0s 54us/step - loss: 1.0362 - acc: 0.6293 - val_loss: 1.3440 - val_acc: 0.5510\n",
      "Epoch 538/1000\n",
      "7048/7048 [==============================] - 0s 56us/step - loss: 1.0540 - acc: 0.6249 - val_loss: 1.3659 - val_acc: 0.5472\n",
      "Epoch 539/1000\n",
      "7048/7048 [==============================] - 0s 56us/step - loss: 1.0418 - acc: 0.6281 - val_loss: 1.3451 - val_acc: 0.5625\n",
      "Epoch 540/1000\n",
      "7048/7048 [==============================] - 0s 55us/step - loss: 1.0777 - acc: 0.6146 - val_loss: 1.3349 - val_acc: 0.5599\n",
      "Epoch 541/1000\n",
      "7048/7048 [==============================] - 0s 55us/step - loss: 1.0475 - acc: 0.6294 - val_loss: 1.3251 - val_acc: 0.5561\n",
      "Epoch 542/1000\n",
      "7048/7048 [==============================] - 0s 54us/step - loss: 1.0477 - acc: 0.6278 - val_loss: 1.3256 - val_acc: 0.5587\n",
      "Epoch 543/1000\n",
      "7048/7048 [==============================] - 0s 54us/step - loss: 1.0422 - acc: 0.6277 - val_loss: 1.3738 - val_acc: 0.5472\n",
      "Epoch 544/1000\n",
      "7048/7048 [==============================] - 0s 56us/step - loss: 1.0487 - acc: 0.6298 - val_loss: 1.3359 - val_acc: 0.5510\n",
      "Epoch 545/1000\n",
      "7048/7048 [==============================] - 0s 56us/step - loss: 1.0443 - acc: 0.6254 - val_loss: 1.3432 - val_acc: 0.5612\n",
      "Epoch 546/1000\n",
      "7048/7048 [==============================] - 0s 56us/step - loss: 1.0476 - acc: 0.6205 - val_loss: 1.3489 - val_acc: 0.5497\n",
      "Epoch 547/1000\n",
      "7048/7048 [==============================] - 0s 64us/step - loss: 1.0446 - acc: 0.6195 - val_loss: 1.3296 - val_acc: 0.5612\n",
      "Epoch 548/1000\n",
      "7048/7048 [==============================] - 1s 75us/step - loss: 1.0773 - acc: 0.6131 - val_loss: 1.3325 - val_acc: 0.5536\n",
      "Epoch 549/1000\n",
      "7048/7048 [==============================] - 1s 72us/step - loss: 1.0484 - acc: 0.6274 - val_loss: 1.3251 - val_acc: 0.5599\n",
      "Epoch 550/1000\n",
      "7048/7048 [==============================] - 0s 71us/step - loss: 1.0360 - acc: 0.6280 - val_loss: 1.3820 - val_acc: 0.5561\n",
      "Epoch 551/1000\n",
      "7048/7048 [==============================] - 0s 70us/step - loss: 1.0380 - acc: 0.6276 - val_loss: 1.3447 - val_acc: 0.5485\n",
      "Epoch 552/1000\n",
      "7048/7048 [==============================] - 1s 71us/step - loss: 1.0308 - acc: 0.6324 - val_loss: 1.3492 - val_acc: 0.5638\n",
      "Epoch 553/1000\n",
      "7048/7048 [==============================] - 1s 80us/step - loss: 1.0510 - acc: 0.6259 - val_loss: 1.3166 - val_acc: 0.5587\n",
      "Epoch 554/1000\n",
      "7048/7048 [==============================] - 0s 71us/step - loss: 1.0408 - acc: 0.6233 - val_loss: 1.3469 - val_acc: 0.5625\n",
      "Epoch 555/1000\n",
      "7048/7048 [==============================] - 0s 63us/step - loss: 1.0523 - acc: 0.6215 - val_loss: 1.3439 - val_acc: 0.5587\n",
      "Epoch 556/1000\n",
      "7048/7048 [==============================] - 0s 69us/step - loss: 1.0389 - acc: 0.6253 - val_loss: 1.3479 - val_acc: 0.5574\n",
      "Epoch 557/1000\n",
      "7048/7048 [==============================] - 0s 70us/step - loss: 1.0231 - acc: 0.6341 - val_loss: 1.3978 - val_acc: 0.5459\n",
      "Epoch 558/1000\n",
      "7048/7048 [==============================] - 0s 60us/step - loss: 1.0472 - acc: 0.6277 - val_loss: 1.3422 - val_acc: 0.5702\n",
      "Epoch 559/1000\n",
      "7048/7048 [==============================] - 0s 55us/step - loss: 1.0281 - acc: 0.6291 - val_loss: 1.3573 - val_acc: 0.5574\n",
      "Epoch 560/1000\n",
      "7048/7048 [==============================] - 0s 58us/step - loss: 1.0457 - acc: 0.6200 - val_loss: 1.3475 - val_acc: 0.5587\n",
      "Epoch 561/1000\n",
      "7048/7048 [==============================] - 0s 57us/step - loss: 1.0332 - acc: 0.6261 - val_loss: 1.3406 - val_acc: 0.5599\n",
      "Epoch 562/1000\n",
      "7048/7048 [==============================] - 0s 55us/step - loss: 1.0316 - acc: 0.6301 - val_loss: 1.3439 - val_acc: 0.5625\n",
      "Epoch 563/1000\n",
      "7048/7048 [==============================] - 0s 54us/step - loss: 1.0398 - acc: 0.6268 - val_loss: 1.3352 - val_acc: 0.5625\n",
      "Epoch 564/1000\n",
      "7048/7048 [==============================] - 0s 54us/step - loss: 1.0405 - acc: 0.6260 - val_loss: 1.3332 - val_acc: 0.5625\n",
      "Epoch 565/1000\n",
      "7048/7048 [==============================] - 0s 55us/step - loss: 1.0444 - acc: 0.6267 - val_loss: 1.3353 - val_acc: 0.5548\n",
      "Epoch 566/1000\n",
      "7048/7048 [==============================] - 0s 54us/step - loss: 1.0363 - acc: 0.6285 - val_loss: 1.3120 - val_acc: 0.5740\n",
      "Epoch 567/1000\n",
      "7048/7048 [==============================] - 0s 53us/step - loss: 1.0256 - acc: 0.6351 - val_loss: 1.3490 - val_acc: 0.5561\n",
      "Epoch 568/1000\n",
      "7048/7048 [==============================] - 0s 54us/step - loss: 1.0333 - acc: 0.6318 - val_loss: 1.3295 - val_acc: 0.5587\n",
      "Epoch 569/1000\n",
      "7048/7048 [==============================] - 0s 55us/step - loss: 1.0543 - acc: 0.6216 - val_loss: 1.3312 - val_acc: 0.5714\n",
      "Epoch 570/1000\n",
      "7048/7048 [==============================] - 0s 55us/step - loss: 1.0514 - acc: 0.6210 - val_loss: 1.3397 - val_acc: 0.5561\n",
      "Epoch 571/1000\n",
      "7048/7048 [==============================] - 0s 55us/step - loss: 1.0325 - acc: 0.6274 - val_loss: 1.3547 - val_acc: 0.5702\n",
      "Epoch 572/1000\n",
      "7048/7048 [==============================] - 0s 54us/step - loss: 1.0427 - acc: 0.6220 - val_loss: 1.3487 - val_acc: 0.5434\n",
      "Epoch 573/1000\n",
      "7048/7048 [==============================] - 0s 54us/step - loss: 1.0339 - acc: 0.6276 - val_loss: 1.3370 - val_acc: 0.5612\n",
      "Epoch 574/1000\n",
      "7048/7048 [==============================] - 1s 72us/step - loss: 1.0389 - acc: 0.6277 - val_loss: 1.3634 - val_acc: 0.5510\n",
      "Epoch 575/1000\n",
      "7048/7048 [==============================] - 1s 76us/step - loss: 1.0309 - acc: 0.6253 - val_loss: 1.3552 - val_acc: 0.5472\n",
      "Epoch 576/1000\n",
      "7048/7048 [==============================] - 1s 73us/step - loss: 1.0385 - acc: 0.6210 - val_loss: 1.3572 - val_acc: 0.5612\n",
      "Epoch 577/1000\n",
      "7048/7048 [==============================] - 0s 56us/step - loss: 1.0412 - acc: 0.6254 - val_loss: 1.3771 - val_acc: 0.5459\n",
      "Epoch 578/1000\n",
      "7048/7048 [==============================] - 0s 57us/step - loss: 1.2210 - acc: 0.5760 - val_loss: 1.3049 - val_acc: 0.5638\n",
      "Epoch 579/1000\n",
      "7048/7048 [==============================] - 0s 61us/step - loss: 1.0873 - acc: 0.6083 - val_loss: 1.3298 - val_acc: 0.5472\n",
      "Epoch 580/1000\n",
      "7048/7048 [==============================] - 0s 61us/step - loss: 1.0639 - acc: 0.6200 - val_loss: 1.3229 - val_acc: 0.5510\n",
      "Epoch 581/1000\n",
      "7048/7048 [==============================] - 0s 56us/step - loss: 1.0525 - acc: 0.6234 - val_loss: 1.3293 - val_acc: 0.5459\n",
      "Epoch 582/1000\n",
      "7048/7048 [==============================] - 0s 63us/step - loss: 1.0525 - acc: 0.6179 - val_loss: 1.3305 - val_acc: 0.5651\n",
      "Epoch 583/1000\n",
      "7048/7048 [==============================] - 0s 60us/step - loss: 1.0592 - acc: 0.6159 - val_loss: 1.3401 - val_acc: 0.5638\n",
      "Epoch 584/1000\n",
      "7048/7048 [==============================] - 2s 230us/step - loss: 1.0365 - acc: 0.6263 - val_loss: 1.3559 - val_acc: 0.5536 0s - loss: 1.0153 - ETA: 0s - loss: 1.0275 - acc: \n",
      "Epoch 585/1000\n",
      "7048/7048 [==============================] - 1s 201us/step - loss: 1.0424 - acc: 0.6345 - val_loss: 1.3294 - val_acc: 0.5612\n",
      "Epoch 586/1000\n",
      "7048/7048 [==============================] - 0s 54us/step - loss: 1.0307 - acc: 0.6314 - val_loss: 1.3761 - val_acc: 0.5574\n",
      "Epoch 587/1000\n",
      "7048/7048 [==============================] - 0s 55us/step - loss: 1.0436 - acc: 0.6312 - val_loss: 1.3337 - val_acc: 0.5497\n",
      "Epoch 588/1000\n",
      "7048/7048 [==============================] - 0s 66us/step - loss: 1.0376 - acc: 0.6259 - val_loss: 1.3774 - val_acc: 0.5612\n",
      "Epoch 589/1000\n",
      "7048/7048 [==============================] - 1s 76us/step - loss: 1.0433 - acc: 0.6291 - val_loss: 1.3585 - val_acc: 0.5472\n",
      "Epoch 590/1000\n",
      "7048/7048 [==============================] - 1s 76us/step - loss: 1.0409 - acc: 0.6294 - val_loss: 1.3435 - val_acc: 0.5638\n",
      "Epoch 591/1000\n",
      "7048/7048 [==============================] - 1s 73us/step - loss: 1.0259 - acc: 0.6321 - val_loss: 1.3600 - val_acc: 0.5472\n",
      "Epoch 592/1000\n",
      "7048/7048 [==============================] - 1s 71us/step - loss: 1.0758 - acc: 0.6149 - val_loss: 1.3355 - val_acc: 0.5548\n",
      "Epoch 593/1000\n",
      "7048/7048 [==============================] - 0s 67us/step - loss: 1.0337 - acc: 0.6287 - val_loss: 1.3300 - val_acc: 0.5612\n",
      "Epoch 594/1000\n",
      "7048/7048 [==============================] - 1s 72us/step - loss: 1.0254 - acc: 0.6327 - val_loss: 1.3621 - val_acc: 0.5523\n",
      "Epoch 595/1000\n",
      "7048/7048 [==============================] - 1s 75us/step - loss: 1.0346 - acc: 0.6234 - val_loss: 1.3578 - val_acc: 0.5485\n",
      "Epoch 596/1000\n",
      "7048/7048 [==============================] - 0s 60us/step - loss: 1.0155 - acc: 0.6365 - val_loss: 1.3544 - val_acc: 0.5459\n",
      "Epoch 597/1000\n",
      "7048/7048 [==============================] - 0s 57us/step - loss: 1.0304 - acc: 0.6288 - val_loss: 1.3536 - val_acc: 0.5497\n",
      "Epoch 598/1000\n",
      "7048/7048 [==============================] - 0s 57us/step - loss: 1.0301 - acc: 0.6301 - val_loss: 1.3658 - val_acc: 0.5485\n",
      "Epoch 599/1000\n",
      "7048/7048 [==============================] - 0s 56us/step - loss: 1.0205 - acc: 0.6334 - val_loss: 1.3733 - val_acc: 0.5497\n",
      "Epoch 600/1000\n",
      "7048/7048 [==============================] - 0s 55us/step - loss: 1.0299 - acc: 0.6254 - val_loss: 1.3566 - val_acc: 0.5625\n",
      "Epoch 601/1000\n",
      "7048/7048 [==============================] - 0s 55us/step - loss: 1.0270 - acc: 0.6325 - val_loss: 1.3713 - val_acc: 0.5587\n",
      "Epoch 602/1000\n",
      "7048/7048 [==============================] - 0s 59us/step - loss: 1.0662 - acc: 0.6183 - val_loss: 1.3684 - val_acc: 0.5638\n",
      "Epoch 603/1000\n",
      "7048/7048 [==============================] - 0s 59us/step - loss: 1.0471 - acc: 0.6206 - val_loss: 1.3446 - val_acc: 0.5536\n",
      "Epoch 604/1000\n",
      "7048/7048 [==============================] - 0s 56us/step - loss: 1.0252 - acc: 0.6325 - val_loss: 1.3577 - val_acc: 0.5574\n",
      "Epoch 605/1000\n",
      "7048/7048 [==============================] - 0s 56us/step - loss: 1.0276 - acc: 0.6277 - val_loss: 1.3484 - val_acc: 0.5651\n",
      "Epoch 606/1000\n",
      "7048/7048 [==============================] - 0s 56us/step - loss: 1.0325 - acc: 0.6278 - val_loss: 1.3299 - val_acc: 0.5536\n",
      "Epoch 607/1000\n",
      "7048/7048 [==============================] - 0s 54us/step - loss: 1.0361 - acc: 0.6302 - val_loss: 1.3494 - val_acc: 0.5459\n",
      "Epoch 608/1000\n",
      "7048/7048 [==============================] - 0s 67us/step - loss: 1.0244 - acc: 0.6270 - val_loss: 1.3272 - val_acc: 0.5497\n",
      "Epoch 609/1000\n",
      "7048/7048 [==============================] - 0s 55us/step - loss: 1.0244 - acc: 0.6368 - val_loss: 1.3527 - val_acc: 0.5510\n",
      "Epoch 610/1000\n",
      "7048/7048 [==============================] - 0s 64us/step - loss: 1.0198 - acc: 0.6348 - val_loss: 1.3497 - val_acc: 0.5370\n",
      "Epoch 611/1000\n",
      "7048/7048 [==============================] - 0s 68us/step - loss: 1.0248 - acc: 0.6305 - val_loss: 1.3384 - val_acc: 0.5561\n",
      "Epoch 612/1000\n",
      "7048/7048 [==============================] - 0s 55us/step - loss: 1.0323 - acc: 0.6325 - val_loss: 1.3461 - val_acc: 0.5561\n",
      "Epoch 613/1000\n",
      "7048/7048 [==============================] - 0s 61us/step - loss: 1.0316 - acc: 0.6277 - val_loss: 1.3561 - val_acc: 0.5561\n",
      "Epoch 614/1000\n",
      "7048/7048 [==============================] - 0s 59us/step - loss: 1.0395 - acc: 0.6237 - val_loss: 1.3257 - val_acc: 0.5651\n",
      "Epoch 615/1000\n",
      "7048/7048 [==============================] - 0s 64us/step - loss: 1.0177 - acc: 0.6322 - val_loss: 1.3415 - val_acc: 0.5497\n",
      "Epoch 616/1000\n",
      "7048/7048 [==============================] - 0s 60us/step - loss: 1.0266 - acc: 0.6345 - val_loss: 1.3609 - val_acc: 0.5472\n",
      "Epoch 617/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 1.0187 - acc: 0.6368 - val_loss: 1.3492 - val_acc: 0.5676\n",
      "Epoch 618/1000\n",
      "7048/7048 [==============================] - 0s 49us/step - loss: 1.0120 - acc: 0.6324 - val_loss: 1.3486 - val_acc: 0.5651\n",
      "Epoch 619/1000\n",
      "7048/7048 [==============================] - 0s 49us/step - loss: 1.0307 - acc: 0.6284 - val_loss: 1.3273 - val_acc: 0.5587\n",
      "Epoch 620/1000\n",
      "7048/7048 [==============================] - 0s 51us/step - loss: 1.0185 - acc: 0.6344 - val_loss: 1.3494 - val_acc: 0.5612\n",
      "Epoch 621/1000\n",
      "7048/7048 [==============================] - 0s 49us/step - loss: 1.0320 - acc: 0.6297 - val_loss: 1.3515 - val_acc: 0.5625\n",
      "Epoch 622/1000\n",
      "7048/7048 [==============================] - 0s 57us/step - loss: 1.0135 - acc: 0.6328 - val_loss: 1.3507 - val_acc: 0.5472\n",
      "Epoch 623/1000\n",
      "7048/7048 [==============================] - 0s 57us/step - loss: 1.0202 - acc: 0.6345 - val_loss: 1.3673 - val_acc: 0.5548\n",
      "Epoch 624/1000\n",
      "7048/7048 [==============================] - 0s 52us/step - loss: 1.0301 - acc: 0.6270 - val_loss: 1.3595 - val_acc: 0.5485\n",
      "Epoch 625/1000\n",
      "7048/7048 [==============================] - 0s 55us/step - loss: 1.0455 - acc: 0.6261 - val_loss: 1.3642 - val_acc: 0.5434\n",
      "Epoch 626/1000\n",
      "7048/7048 [==============================] - 0s 51us/step - loss: 1.0343 - acc: 0.6288 - val_loss: 1.3542 - val_acc: 0.5497\n",
      "Epoch 627/1000\n",
      "7048/7048 [==============================] - 0s 52us/step - loss: 1.0156 - acc: 0.6327 - val_loss: 1.3632 - val_acc: 0.5523\n",
      "Epoch 628/1000\n",
      "7048/7048 [==============================] - 0s 66us/step - loss: 1.0196 - acc: 0.6315 - val_loss: 1.3536 - val_acc: 0.5599\n",
      "Epoch 629/1000\n",
      "7048/7048 [==============================] - 0s 69us/step - loss: 1.0268 - acc: 0.6297 - val_loss: 1.3789 - val_acc: 0.5472\n",
      "Epoch 630/1000\n",
      "7048/7048 [==============================] - 0s 67us/step - loss: 1.0102 - acc: 0.6345 - val_loss: 1.3339 - val_acc: 0.5536\n",
      "Epoch 631/1000\n",
      "7048/7048 [==============================] - 0s 69us/step - loss: 1.0076 - acc: 0.6337 - val_loss: 1.3589 - val_acc: 0.5536\n",
      "Epoch 632/1000\n",
      "7048/7048 [==============================] - 0s 67us/step - loss: 1.0373 - acc: 0.6283 - val_loss: 1.3554 - val_acc: 0.5459\n",
      "Epoch 633/1000\n",
      "7048/7048 [==============================] - 0s 66us/step - loss: 1.0229 - acc: 0.6331 - val_loss: 1.3671 - val_acc: 0.5599\n",
      "Epoch 634/1000\n",
      "7048/7048 [==============================] - 0s 63us/step - loss: 1.0295 - acc: 0.6301 - val_loss: 1.3629 - val_acc: 0.5523\n",
      "Epoch 635/1000\n",
      "7048/7048 [==============================] - 0s 54us/step - loss: 1.0286 - acc: 0.6317 - val_loss: 1.3629 - val_acc: 0.5459\n",
      "Epoch 636/1000\n",
      "7048/7048 [==============================] - 0s 67us/step - loss: 1.0138 - acc: 0.6297 - val_loss: 1.3500 - val_acc: 0.5536\n",
      "Epoch 637/1000\n",
      "7048/7048 [==============================] - 0s 53us/step - loss: 1.0215 - acc: 0.6302 - val_loss: 1.3388 - val_acc: 0.5702\n",
      "Epoch 638/1000\n",
      "7048/7048 [==============================] - 0s 53us/step - loss: 1.0152 - acc: 0.6290 - val_loss: 1.3652 - val_acc: 0.5548\n",
      "Epoch 639/1000\n",
      "7048/7048 [==============================] - 0s 56us/step - loss: 1.0219 - acc: 0.6349 - val_loss: 1.3853 - val_acc: 0.5497\n",
      "Epoch 640/1000\n",
      "7048/7048 [==============================] - 0s 53us/step - loss: 1.0489 - acc: 0.6212 - val_loss: 1.3734 - val_acc: 0.5485\n",
      "Epoch 641/1000\n",
      "7048/7048 [==============================] - 0s 55us/step - loss: 1.0307 - acc: 0.6312 - val_loss: 1.3459 - val_acc: 0.5446\n",
      "Epoch 642/1000\n",
      "7048/7048 [==============================] - 0s 57us/step - loss: 1.0374 - acc: 0.6190 - val_loss: 1.3529 - val_acc: 0.5421\n",
      "Epoch 643/1000\n",
      "7048/7048 [==============================] - 0s 54us/step - loss: 1.0323 - acc: 0.6283 - val_loss: 1.3454 - val_acc: 0.5446\n",
      "Epoch 644/1000\n",
      "7048/7048 [==============================] - 0s 53us/step - loss: 1.0274 - acc: 0.6277 - val_loss: 1.3599 - val_acc: 0.5587\n",
      "Epoch 645/1000\n",
      "7048/7048 [==============================] - 0s 55us/step - loss: 1.0223 - acc: 0.6328 - val_loss: 1.3846 - val_acc: 0.5574\n",
      "Epoch 646/1000\n",
      "7048/7048 [==============================] - 0s 53us/step - loss: 1.0087 - acc: 0.6366 - val_loss: 1.3799 - val_acc: 0.5548\n",
      "Epoch 647/1000\n",
      "7048/7048 [==============================] - 0s 54us/step - loss: 1.0152 - acc: 0.6416 - val_loss: 1.3566 - val_acc: 0.5536\n",
      "Epoch 648/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7048/7048 [==============================] - 0s 54us/step - loss: 1.0226 - acc: 0.6345 - val_loss: 1.3499 - val_acc: 0.5587\n",
      "Epoch 649/1000\n",
      "7048/7048 [==============================] - 0s 54us/step - loss: 1.0211 - acc: 0.6322 - val_loss: 1.3237 - val_acc: 0.5599\n",
      "Epoch 650/1000\n",
      "7048/7048 [==============================] - 0s 53us/step - loss: 1.0424 - acc: 0.6227 - val_loss: 1.3475 - val_acc: 0.5536\n",
      "Epoch 651/1000\n",
      "7048/7048 [==============================] - 0s 54us/step - loss: 1.0314 - acc: 0.6232 - val_loss: 1.3356 - val_acc: 0.5523\n",
      "Epoch 652/1000\n",
      "7048/7048 [==============================] - 0s 57us/step - loss: 1.0162 - acc: 0.6329 - val_loss: 1.3532 - val_acc: 0.5472\n",
      "Epoch 653/1000\n",
      "7048/7048 [==============================] - 0s 53us/step - loss: 1.0209 - acc: 0.6361 - val_loss: 1.3571 - val_acc: 0.5574\n",
      "Epoch 654/1000\n",
      "7048/7048 [==============================] - 0s 53us/step - loss: 1.0235 - acc: 0.6300 - val_loss: 1.3504 - val_acc: 0.5574\n",
      "Epoch 655/1000\n",
      "7048/7048 [==============================] - 0s 54us/step - loss: 1.0193 - acc: 0.6331 - val_loss: 1.3418 - val_acc: 0.5548\n",
      "Epoch 656/1000\n",
      "7048/7048 [==============================] - 0s 53us/step - loss: 1.0209 - acc: 0.6331 - val_loss: 1.3563 - val_acc: 0.5383\n",
      "Epoch 657/1000\n",
      "7048/7048 [==============================] - 0s 54us/step - loss: 1.0439 - acc: 0.6271 - val_loss: 1.3345 - val_acc: 0.5434\n",
      "Epoch 658/1000\n",
      "7048/7048 [==============================] - 0s 70us/step - loss: 1.0168 - acc: 0.6337 - val_loss: 1.3579 - val_acc: 0.5446\n",
      "Epoch 659/1000\n",
      "7048/7048 [==============================] - 1s 82us/step - loss: 1.0222 - acc: 0.6281 - val_loss: 1.3688 - val_acc: 0.5587\n",
      "Epoch 660/1000\n",
      "7048/7048 [==============================] - 1s 72us/step - loss: 1.0275 - acc: 0.6348 - val_loss: 1.3575 - val_acc: 0.5523\n",
      "Epoch 661/1000\n",
      "7048/7048 [==============================] - 1s 80us/step - loss: 1.0215 - acc: 0.6318 - val_loss: 1.3736 - val_acc: 0.5561\n",
      "Epoch 662/1000\n",
      "7048/7048 [==============================] - 0s 57us/step - loss: 1.0147 - acc: 0.6349 - val_loss: 1.3709 - val_acc: 0.5574\n",
      "Epoch 663/1000\n",
      "7048/7048 [==============================] - 0s 71us/step - loss: 1.0250 - acc: 0.6386 - val_loss: 1.3505 - val_acc: 0.5510\n",
      "Epoch 664/1000\n",
      "7048/7048 [==============================] - 1s 78us/step - loss: 1.0250 - acc: 0.6266 - val_loss: 1.3543 - val_acc: 0.5663\n",
      "Epoch 665/1000\n",
      "7048/7048 [==============================] - 1s 73us/step - loss: 1.0133 - acc: 0.6399 - val_loss: 1.3876 - val_acc: 0.5625\n",
      "Epoch 666/1000\n",
      "7048/7048 [==============================] - 0s 52us/step - loss: 1.0018 - acc: 0.6429 - val_loss: 1.3757 - val_acc: 0.5421\n",
      "Epoch 667/1000\n",
      "7048/7048 [==============================] - 0s 54us/step - loss: 1.0044 - acc: 0.6375 - val_loss: 1.3785 - val_acc: 0.5446\n",
      "Epoch 668/1000\n",
      "7048/7048 [==============================] - 0s 55us/step - loss: 1.0400 - acc: 0.6227 - val_loss: 1.3731 - val_acc: 0.5408\n",
      "Epoch 669/1000\n",
      "7048/7048 [==============================] - 0s 53us/step - loss: 1.0275 - acc: 0.6287 - val_loss: 1.3538 - val_acc: 0.5638\n",
      "Epoch 670/1000\n",
      "7048/7048 [==============================] - 0s 54us/step - loss: 1.0154 - acc: 0.6342 - val_loss: 1.3367 - val_acc: 0.5536\n",
      "Epoch 671/1000\n",
      "7048/7048 [==============================] - 0s 52us/step - loss: 1.0158 - acc: 0.6346 - val_loss: 1.3577 - val_acc: 0.5574\n",
      "Epoch 672/1000\n",
      "7048/7048 [==============================] - 0s 51us/step - loss: 1.0349 - acc: 0.6310 - val_loss: 1.3457 - val_acc: 0.5587\n",
      "Epoch 673/1000\n",
      "7048/7048 [==============================] - 0s 53us/step - loss: 1.0080 - acc: 0.6412 - val_loss: 1.3695 - val_acc: 0.5446\n",
      "Epoch 674/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 1.0264 - acc: 0.6355 - val_loss: 1.3353 - val_acc: 0.5587\n",
      "Epoch 675/1000\n",
      "7048/7048 [==============================] - 0s 49us/step - loss: 1.0145 - acc: 0.6368 - val_loss: 1.3670 - val_acc: 0.5319\n",
      "Epoch 676/1000\n",
      "7048/7048 [==============================] - 0s 48us/step - loss: 1.0149 - acc: 0.6324 - val_loss: 1.3499 - val_acc: 0.5625\n",
      "Epoch 677/1000\n",
      "7048/7048 [==============================] - 0s 49us/step - loss: 1.0118 - acc: 0.6368 - val_loss: 1.3676 - val_acc: 0.5459\n",
      "Epoch 678/1000\n",
      "7048/7048 [==============================] - 0s 49us/step - loss: 1.0191 - acc: 0.6298 - val_loss: 1.3407 - val_acc: 0.5523\n",
      "Epoch 679/1000\n",
      "7048/7048 [==============================] - 0s 49us/step - loss: 1.0134 - acc: 0.6354 - val_loss: 1.3772 - val_acc: 0.5510\n",
      "Epoch 680/1000\n",
      "7048/7048 [==============================] - 0s 54us/step - loss: 1.0094 - acc: 0.6317 - val_loss: 1.3762 - val_acc: 0.5587\n",
      "Epoch 681/1000\n",
      "7048/7048 [==============================] - 0s 52us/step - loss: 1.0104 - acc: 0.6341 - val_loss: 1.3768 - val_acc: 0.5536\n",
      "Epoch 682/1000\n",
      "7048/7048 [==============================] - 0s 56us/step - loss: 1.0002 - acc: 0.6368 - val_loss: 1.3419 - val_acc: 0.5536\n",
      "Epoch 683/1000\n",
      "7048/7048 [==============================] - 0s 53us/step - loss: 1.0050 - acc: 0.6388 - val_loss: 1.3532 - val_acc: 0.5421\n",
      "Epoch 684/1000\n",
      "7048/7048 [==============================] - 0s 52us/step - loss: 1.0064 - acc: 0.6371 - val_loss: 1.3753 - val_acc: 0.5459\n",
      "Epoch 685/1000\n",
      "7048/7048 [==============================] - 0s 56us/step - loss: 1.0245 - acc: 0.6297 - val_loss: 1.3599 - val_acc: 0.5599\n",
      "Epoch 686/1000\n",
      "7048/7048 [==============================] - 0s 62us/step - loss: 1.0012 - acc: 0.6406 - val_loss: 1.3492 - val_acc: 0.5523\n",
      "Epoch 687/1000\n",
      "7048/7048 [==============================] - 0s 68us/step - loss: 1.0098 - acc: 0.6295 - val_loss: 1.3731 - val_acc: 0.5446\n",
      "Epoch 688/1000\n",
      "7048/7048 [==============================] - 0s 57us/step - loss: 1.0068 - acc: 0.6382 - val_loss: 1.3692 - val_acc: 0.5459\n",
      "Epoch 689/1000\n",
      "7048/7048 [==============================] - 0s 59us/step - loss: 1.0192 - acc: 0.6337 - val_loss: 1.3952 - val_acc: 0.5510\n",
      "Epoch 690/1000\n",
      "7048/7048 [==============================] - 0s 55us/step - loss: 1.0108 - acc: 0.6368 - val_loss: 1.3453 - val_acc: 0.5523\n",
      "Epoch 691/1000\n",
      "7048/7048 [==============================] - 0s 51us/step - loss: 1.0132 - acc: 0.6364 - val_loss: 1.3654 - val_acc: 0.5497\n",
      "Epoch 692/1000\n",
      "7048/7048 [==============================] - 0s 51us/step - loss: 1.0110 - acc: 0.6364 - val_loss: 1.3723 - val_acc: 0.5574\n",
      "Epoch 693/1000\n",
      "7048/7048 [==============================] - 0s 53us/step - loss: 1.0037 - acc: 0.6416 - val_loss: 1.3645 - val_acc: 0.5523\n",
      "Epoch 694/1000\n",
      "7048/7048 [==============================] - 0s 49us/step - loss: 1.0284 - acc: 0.6315 - val_loss: 1.3649 - val_acc: 0.5510\n",
      "Epoch 695/1000\n",
      "7048/7048 [==============================] - 0s 62us/step - loss: 1.0111 - acc: 0.6302 - val_loss: 1.3768 - val_acc: 0.5599\n",
      "Epoch 696/1000\n",
      "7048/7048 [==============================] - 0s 60us/step - loss: 1.0150 - acc: 0.6300 - val_loss: 1.3640 - val_acc: 0.5485\n",
      "Epoch 697/1000\n",
      "7048/7048 [==============================] - 0s 61us/step - loss: 1.0157 - acc: 0.6327 - val_loss: 1.3729 - val_acc: 0.5523\n",
      "Epoch 698/1000\n",
      "7048/7048 [==============================] - 0s 54us/step - loss: 1.0175 - acc: 0.6329 - val_loss: 1.3645 - val_acc: 0.5536\n",
      "Epoch 699/1000\n",
      "7048/7048 [==============================] - 0s 58us/step - loss: 0.9949 - acc: 0.6433 - val_loss: 1.3900 - val_acc: 0.5344\n",
      "Epoch 700/1000\n",
      "7048/7048 [==============================] - 1s 77us/step - loss: 1.0231 - acc: 0.6305 - val_loss: 1.3558 - val_acc: 0.5485\n",
      "Epoch 701/1000\n",
      "7048/7048 [==============================] - 0s 65us/step - loss: 1.0203 - acc: 0.6334 - val_loss: 1.3683 - val_acc: 0.5408\n",
      "Epoch 702/1000\n",
      "7048/7048 [==============================] - 0s 66us/step - loss: 1.0040 - acc: 0.6379 - val_loss: 1.3773 - val_acc: 0.5574\n",
      "Epoch 703/1000\n",
      "7048/7048 [==============================] - 0s 57us/step - loss: 1.0108 - acc: 0.6379 - val_loss: 1.3550 - val_acc: 0.5536\n",
      "Epoch 704/1000\n",
      "7048/7048 [==============================] - 0s 61us/step - loss: 1.0170 - acc: 0.6362 - val_loss: 1.3868 - val_acc: 0.5472\n",
      "Epoch 705/1000\n",
      "7048/7048 [==============================] - 0s 53us/step - loss: 0.9927 - acc: 0.6437 - val_loss: 1.3789 - val_acc: 0.5434\n",
      "Epoch 706/1000\n",
      "7048/7048 [==============================] - 0s 67us/step - loss: 1.0129 - acc: 0.6354 - val_loss: 1.3621 - val_acc: 0.5510\n",
      "Epoch 707/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7048/7048 [==============================] - 0s 61us/step - loss: 1.0028 - acc: 0.6361 - val_loss: 1.3587 - val_acc: 0.5625\n",
      "Epoch 708/1000\n",
      "7048/7048 [==============================] - 0s 55us/step - loss: 1.0178 - acc: 0.6341 - val_loss: 1.3605 - val_acc: 0.5651\n",
      "Epoch 709/1000\n",
      "7048/7048 [==============================] - 0s 52us/step - loss: 1.0052 - acc: 0.6349 - val_loss: 1.3563 - val_acc: 0.5497\n",
      "Epoch 710/1000\n",
      "7048/7048 [==============================] - 0s 55us/step - loss: 1.0144 - acc: 0.6385 - val_loss: 1.3657 - val_acc: 0.5485\n",
      "Epoch 711/1000\n",
      "7048/7048 [==============================] - 0s 54us/step - loss: 1.0107 - acc: 0.6369 - val_loss: 1.3787 - val_acc: 0.5587\n",
      "Epoch 712/1000\n",
      "7048/7048 [==============================] - 0s 59us/step - loss: 1.0287 - acc: 0.6287 - val_loss: 1.3695 - val_acc: 0.5510\n",
      "Epoch 713/1000\n",
      "7048/7048 [==============================] - 0s 54us/step - loss: 1.0136 - acc: 0.6402 - val_loss: 1.3712 - val_acc: 0.5485\n",
      "Epoch 714/1000\n",
      "7048/7048 [==============================] - 0s 53us/step - loss: 1.0083 - acc: 0.6386 - val_loss: 1.3925 - val_acc: 0.5408\n",
      "Epoch 715/1000\n",
      "7048/7048 [==============================] - 0s 53us/step - loss: 1.0065 - acc: 0.6434 - val_loss: 1.4129 - val_acc: 0.5357\n",
      "Epoch 716/1000\n",
      "7048/7048 [==============================] - 0s 53us/step - loss: 1.0107 - acc: 0.6371 - val_loss: 1.4052 - val_acc: 0.5332\n",
      "Epoch 717/1000\n",
      "7048/7048 [==============================] - 0s 53us/step - loss: 1.0202 - acc: 0.6311 - val_loss: 1.3767 - val_acc: 0.5536\n",
      "Epoch 718/1000\n",
      "7048/7048 [==============================] - 0s 53us/step - loss: 1.0065 - acc: 0.6331 - val_loss: 1.3729 - val_acc: 0.5523\n",
      "Epoch 719/1000\n",
      "7048/7048 [==============================] - 0s 53us/step - loss: 0.9941 - acc: 0.6405 - val_loss: 1.3948 - val_acc: 0.5523\n",
      "Epoch 720/1000\n",
      "7048/7048 [==============================] - 0s 52us/step - loss: 0.9974 - acc: 0.6388 - val_loss: 1.3611 - val_acc: 0.5561\n",
      "Epoch 721/1000\n",
      "7048/7048 [==============================] - 0s 56us/step - loss: 1.0450 - acc: 0.6285 - val_loss: 1.3543 - val_acc: 0.5561\n",
      "Epoch 722/1000\n",
      "7048/7048 [==============================] - 0s 54us/step - loss: 1.0111 - acc: 0.6355 - val_loss: 1.3363 - val_acc: 0.5587\n",
      "Epoch 723/1000\n",
      "7048/7048 [==============================] - 0s 52us/step - loss: 0.9825 - acc: 0.6417 - val_loss: 1.3747 - val_acc: 0.5408\n",
      "Epoch 724/1000\n",
      "7048/7048 [==============================] - 0s 49us/step - loss: 1.0062 - acc: 0.6392 - val_loss: 1.3902 - val_acc: 0.5434\n",
      "Epoch 725/1000\n",
      "7048/7048 [==============================] - 1s 97us/step - loss: 1.0136 - acc: 0.6362 - val_loss: 1.3895 - val_acc: 0.5421\n",
      "Epoch 726/1000\n",
      "7048/7048 [==============================] - 0s 59us/step - loss: 0.9936 - acc: 0.6427 - val_loss: 1.3803 - val_acc: 0.5497\n",
      "Epoch 727/1000\n",
      "7048/7048 [==============================] - 0s 71us/step - loss: 0.9904 - acc: 0.6446 - val_loss: 1.3634 - val_acc: 0.5446\n",
      "Epoch 728/1000\n",
      "7048/7048 [==============================] - 0s 54us/step - loss: 0.9961 - acc: 0.6375 - val_loss: 1.3807 - val_acc: 0.5434\n",
      "Epoch 729/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 1.0029 - acc: 0.6349 - val_loss: 1.3655 - val_acc: 0.5421\n",
      "Epoch 730/1000\n",
      "7048/7048 [==============================] - 0s 51us/step - loss: 1.0009 - acc: 0.6407 - val_loss: 1.3970 - val_acc: 0.5459\n",
      "Epoch 731/1000\n",
      "7048/7048 [==============================] - 0s 51us/step - loss: 1.0160 - acc: 0.6325 - val_loss: 1.3844 - val_acc: 0.5574\n",
      "Epoch 732/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 1.0144 - acc: 0.6325 - val_loss: 1.3675 - val_acc: 0.5408\n",
      "Epoch 733/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9995 - acc: 0.6400 - val_loss: 1.3734 - val_acc: 0.5370\n",
      "Epoch 734/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 1.0111 - acc: 0.6366 - val_loss: 1.3722 - val_acc: 0.5383\n",
      "Epoch 735/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 1.0000 - acc: 0.6356 - val_loss: 1.3654 - val_acc: 0.5230\n",
      "Epoch 736/1000\n",
      "7048/7048 [==============================] - 0s 51us/step - loss: 1.0574 - acc: 0.6180 - val_loss: 1.3500 - val_acc: 0.5459\n",
      "Epoch 737/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9966 - acc: 0.6422 - val_loss: 1.3726 - val_acc: 0.5434\n",
      "Epoch 738/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 1.0120 - acc: 0.6346 - val_loss: 1.4112 - val_acc: 0.5446\n",
      "Epoch 739/1000\n",
      "7048/7048 [==============================] - 0s 51us/step - loss: 1.0033 - acc: 0.6412 - val_loss: 1.3990 - val_acc: 0.5281\n",
      "Epoch 740/1000\n",
      "7048/7048 [==============================] - 0s 51us/step - loss: 1.0061 - acc: 0.6375 - val_loss: 1.3587 - val_acc: 0.5408\n",
      "Epoch 741/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 1.0208 - acc: 0.6385 - val_loss: 1.3926 - val_acc: 0.5548\n",
      "Epoch 742/1000\n",
      "7048/7048 [==============================] - 0s 52us/step - loss: 1.0047 - acc: 0.6362 - val_loss: 1.3526 - val_acc: 0.5408\n",
      "Epoch 743/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 1.0103 - acc: 0.6362 - val_loss: 1.3560 - val_acc: 0.5497\n",
      "Epoch 744/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 1.0052 - acc: 0.6410 - val_loss: 1.3942 - val_acc: 0.5357\n",
      "Epoch 745/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9948 - acc: 0.6426 - val_loss: 1.3526 - val_acc: 0.5383\n",
      "Epoch 746/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 1.0247 - acc: 0.6373 - val_loss: 1.3693 - val_acc: 0.5459\n",
      "Epoch 747/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9885 - acc: 0.6464 - val_loss: 1.3729 - val_acc: 0.5434\n",
      "Epoch 748/1000\n",
      "7048/7048 [==============================] - 0s 51us/step - loss: 0.9990 - acc: 0.6423 - val_loss: 1.3576 - val_acc: 0.5510\n",
      "Epoch 749/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 1.0000 - acc: 0.6395 - val_loss: 1.3836 - val_acc: 0.5408\n",
      "Epoch 750/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9920 - acc: 0.6434 - val_loss: 1.3726 - val_acc: 0.5421\n",
      "Epoch 751/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 1.0186 - acc: 0.6285 - val_loss: 1.3702 - val_acc: 0.5485\n",
      "Epoch 752/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9926 - acc: 0.6403 - val_loss: 1.3997 - val_acc: 0.5421\n",
      "Epoch 753/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9981 - acc: 0.6405 - val_loss: 1.3858 - val_acc: 0.5370\n",
      "Epoch 754/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9922 - acc: 0.6372 - val_loss: 1.3727 - val_acc: 0.5395\n",
      "Epoch 755/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 1.0037 - acc: 0.6436 - val_loss: 1.3650 - val_acc: 0.5421\n",
      "Epoch 756/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9981 - acc: 0.6320 - val_loss: 1.3826 - val_acc: 0.5370\n",
      "Epoch 757/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 1.0005 - acc: 0.6365 - val_loss: 1.3721 - val_acc: 0.5306\n",
      "Epoch 758/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 1.0151 - acc: 0.6317 - val_loss: 1.3675 - val_acc: 0.5446\n",
      "Epoch 759/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 1.0183 - acc: 0.6345 - val_loss: 1.3936 - val_acc: 0.5383\n",
      "Epoch 760/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 1.0216 - acc: 0.6337 - val_loss: 1.3678 - val_acc: 0.5383\n",
      "Epoch 761/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 1.0011 - acc: 0.6348 - val_loss: 1.3810 - val_acc: 0.5421\n",
      "Epoch 762/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9973 - acc: 0.6383 - val_loss: 1.3863 - val_acc: 0.5332\n",
      "Epoch 763/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 1.0073 - acc: 0.6345 - val_loss: 1.3682 - val_acc: 0.5434\n",
      "Epoch 764/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9961 - acc: 0.6447 - val_loss: 1.3588 - val_acc: 0.5395\n",
      "Epoch 765/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 1.0050 - acc: 0.6396 - val_loss: 1.4032 - val_acc: 0.5395\n",
      "Epoch 766/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7048/7048 [==============================] - 0s 50us/step - loss: 1.0014 - acc: 0.6405 - val_loss: 1.3766 - val_acc: 0.5383\n",
      "Epoch 767/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9948 - acc: 0.6415 - val_loss: 1.3945 - val_acc: 0.5383\n",
      "Epoch 768/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 1.0187 - acc: 0.6392 - val_loss: 1.3850 - val_acc: 0.5306\n",
      "Epoch 769/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9934 - acc: 0.6415 - val_loss: 1.3969 - val_acc: 0.5344\n",
      "Epoch 770/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9970 - acc: 0.6400 - val_loss: 1.3776 - val_acc: 0.5446\n",
      "Epoch 771/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9881 - acc: 0.6460 - val_loss: 1.3717 - val_acc: 0.5459\n",
      "Epoch 772/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9956 - acc: 0.6388 - val_loss: 1.3729 - val_acc: 0.5485\n",
      "Epoch 773/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 1.0245 - acc: 0.6291 - val_loss: 1.3758 - val_acc: 0.5599\n",
      "Epoch 774/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 1.0671 - acc: 0.6210 - val_loss: 1.3523 - val_acc: 0.5395\n",
      "Epoch 775/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 1.0318 - acc: 0.6237 - val_loss: 1.4092 - val_acc: 0.5485\n",
      "Epoch 776/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9992 - acc: 0.6345 - val_loss: 1.3930 - val_acc: 0.5395\n",
      "Epoch 777/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9877 - acc: 0.6403 - val_loss: 1.3929 - val_acc: 0.5357\n",
      "Epoch 778/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9944 - acc: 0.6406 - val_loss: 1.3876 - val_acc: 0.5523\n",
      "Epoch 779/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 1.0032 - acc: 0.6345 - val_loss: 1.3841 - val_acc: 0.5408\n",
      "Epoch 780/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9858 - acc: 0.6412 - val_loss: 1.3936 - val_acc: 0.5434\n",
      "Epoch 781/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9862 - acc: 0.6437 - val_loss: 1.3783 - val_acc: 0.5446\n",
      "Epoch 782/1000\n",
      "7048/7048 [==============================] - 0s 52us/step - loss: 0.9995 - acc: 0.6393 - val_loss: 1.3953 - val_acc: 0.5357\n",
      "Epoch 783/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 1.0187 - acc: 0.6293 - val_loss: 1.3840 - val_acc: 0.5485\n",
      "Epoch 784/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 1.0015 - acc: 0.6406 - val_loss: 1.3907 - val_acc: 0.5485\n",
      "Epoch 785/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9846 - acc: 0.6410 - val_loss: 1.3981 - val_acc: 0.5383\n",
      "Epoch 786/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9887 - acc: 0.6403 - val_loss: 1.3642 - val_acc: 0.5408\n",
      "Epoch 787/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 1.0573 - acc: 0.6145 - val_loss: 1.3468 - val_acc: 0.5472\n",
      "Epoch 788/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 1.0066 - acc: 0.6396 - val_loss: 1.4048 - val_acc: 0.5332\n",
      "Epoch 789/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 1.0372 - acc: 0.6260 - val_loss: 1.3755 - val_acc: 0.5421\n",
      "Epoch 790/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9897 - acc: 0.6454 - val_loss: 1.3760 - val_acc: 0.5485\n",
      "Epoch 791/1000\n",
      "7048/7048 [==============================] - 0s 51us/step - loss: 0.9909 - acc: 0.6382 - val_loss: 1.3884 - val_acc: 0.5459\n",
      "Epoch 792/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9928 - acc: 0.6395 - val_loss: 1.3740 - val_acc: 0.5485\n",
      "Epoch 793/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9979 - acc: 0.6427 - val_loss: 1.3746 - val_acc: 0.5446\n",
      "Epoch 794/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9784 - acc: 0.6442 - val_loss: 1.3934 - val_acc: 0.5408\n",
      "Epoch 795/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9799 - acc: 0.6498 - val_loss: 1.4286 - val_acc: 0.5561\n",
      "Epoch 796/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9858 - acc: 0.6443 - val_loss: 1.3998 - val_acc: 0.5548\n",
      "Epoch 797/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9832 - acc: 0.6444 - val_loss: 1.4047 - val_acc: 0.5485\n",
      "Epoch 798/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9996 - acc: 0.6398 - val_loss: 1.4217 - val_acc: 0.5421\n",
      "Epoch 799/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 1.0078 - acc: 0.6392 - val_loss: 1.3889 - val_acc: 0.5485\n",
      "Epoch 800/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9986 - acc: 0.6423 - val_loss: 1.3802 - val_acc: 0.5408\n",
      "Epoch 801/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9868 - acc: 0.6398 - val_loss: 1.3988 - val_acc: 0.5395\n",
      "Epoch 802/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9839 - acc: 0.6409 - val_loss: 1.3816 - val_acc: 0.5421\n",
      "Epoch 803/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 1.0007 - acc: 0.6337 - val_loss: 1.3766 - val_acc: 0.5421\n",
      "Epoch 804/1000\n",
      "7048/7048 [==============================] - 0s 53us/step - loss: 0.9931 - acc: 0.6398 - val_loss: 1.3985 - val_acc: 0.5408\n",
      "Epoch 805/1000\n",
      "7048/7048 [==============================] - 0s 57us/step - loss: 0.9940 - acc: 0.6388 - val_loss: 1.3910 - val_acc: 0.5421\n",
      "Epoch 806/1000\n",
      "7048/7048 [==============================] - 0s 54us/step - loss: 0.9804 - acc: 0.6413 - val_loss: 1.4253 - val_acc: 0.5383\n",
      "Epoch 807/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9791 - acc: 0.6471 - val_loss: 1.4089 - val_acc: 0.5344\n",
      "Epoch 808/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9734 - acc: 0.6486 - val_loss: 1.3856 - val_acc: 0.5510\n",
      "Epoch 809/1000\n",
      "7048/7048 [==============================] - 0s 56us/step - loss: 1.0082 - acc: 0.6359 - val_loss: 1.3854 - val_acc: 0.5434\n",
      "Epoch 810/1000\n",
      "7048/7048 [==============================] - 0s 56us/step - loss: 0.9948 - acc: 0.6429 - val_loss: 1.4092 - val_acc: 0.5357\n",
      "Epoch 811/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9832 - acc: 0.6470 - val_loss: 1.3830 - val_acc: 0.5523\n",
      "Epoch 812/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9756 - acc: 0.6481 - val_loss: 1.3885 - val_acc: 0.5434\n",
      "Epoch 813/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9819 - acc: 0.6518 - val_loss: 1.4171 - val_acc: 0.5383\n",
      "Epoch 814/1000\n",
      "7048/7048 [==============================] - 0s 51us/step - loss: 1.0076 - acc: 0.6342 - val_loss: 1.4036 - val_acc: 0.5485\n",
      "Epoch 815/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9815 - acc: 0.6422 - val_loss: 1.4058 - val_acc: 0.5383\n",
      "Epoch 816/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9886 - acc: 0.6429 - val_loss: 1.4088 - val_acc: 0.5357\n",
      "Epoch 817/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9855 - acc: 0.6454 - val_loss: 1.3859 - val_acc: 0.5408\n",
      "Epoch 818/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9873 - acc: 0.6444 - val_loss: 1.3769 - val_acc: 0.5370\n",
      "Epoch 819/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9958 - acc: 0.6429 - val_loss: 1.3804 - val_acc: 0.5434\n",
      "Epoch 820/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9774 - acc: 0.6498 - val_loss: 1.3642 - val_acc: 0.5434\n",
      "Epoch 821/1000\n",
      "7048/7048 [==============================] - 0s 51us/step - loss: 0.9899 - acc: 0.6416 - val_loss: 1.3948 - val_acc: 0.5459\n",
      "Epoch 822/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9946 - acc: 0.6436 - val_loss: 1.4176 - val_acc: 0.5561\n",
      "Epoch 823/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9813 - acc: 0.6447 - val_loss: 1.4225 - val_acc: 0.5395\n",
      "Epoch 824/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9922 - acc: 0.6426 - val_loss: 1.3902 - val_acc: 0.5421\n",
      "Epoch 825/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9749 - acc: 0.6493 - val_loss: 1.3962 - val_acc: 0.5536\n",
      "Epoch 826/1000\n",
      "7048/7048 [==============================] - 0s 51us/step - loss: 0.9754 - acc: 0.6507 - val_loss: 1.4057 - val_acc: 0.5421\n",
      "Epoch 827/1000\n",
      "7048/7048 [==============================] - 0s 51us/step - loss: 0.9809 - acc: 0.6505 - val_loss: 1.4075 - val_acc: 0.5561\n",
      "Epoch 828/1000\n",
      "7048/7048 [==============================] - 0s 51us/step - loss: 0.9841 - acc: 0.6478 - val_loss: 1.3876 - val_acc: 0.5459\n",
      "Epoch 829/1000\n",
      "7048/7048 [==============================] - 0s 51us/step - loss: 0.9938 - acc: 0.6409 - val_loss: 1.3931 - val_acc: 0.5421\n",
      "Epoch 830/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 1.0015 - acc: 0.6432 - val_loss: 1.4243 - val_acc: 0.5446\n",
      "Epoch 831/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9835 - acc: 0.6470 - val_loss: 1.4140 - val_acc: 0.5497\n",
      "Epoch 832/1000\n",
      "7048/7048 [==============================] - 0s 52us/step - loss: 0.9899 - acc: 0.6426 - val_loss: 1.3858 - val_acc: 0.5434\n",
      "Epoch 833/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9910 - acc: 0.6469 - val_loss: 1.3962 - val_acc: 0.5497\n",
      "Epoch 834/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9876 - acc: 0.6447 - val_loss: 1.4009 - val_acc: 0.5383\n",
      "Epoch 835/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 1.0042 - acc: 0.6388 - val_loss: 1.3958 - val_acc: 0.5485\n",
      "Epoch 836/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9965 - acc: 0.6419 - val_loss: 1.4234 - val_acc: 0.5434\n",
      "Epoch 837/1000\n",
      "7048/7048 [==============================] - 0s 51us/step - loss: 0.9896 - acc: 0.6419 - val_loss: 1.4340 - val_acc: 0.5510\n",
      "Epoch 838/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9871 - acc: 0.6440 - val_loss: 1.4277 - val_acc: 0.5536\n",
      "Epoch 839/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9879 - acc: 0.6399 - val_loss: 1.4510 - val_acc: 0.5485\n",
      "Epoch 840/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9879 - acc: 0.6491 - val_loss: 1.4217 - val_acc: 0.5510\n",
      "Epoch 841/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 1.0008 - acc: 0.6436 - val_loss: 1.3974 - val_acc: 0.5421\n",
      "Epoch 842/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 1.0061 - acc: 0.6423 - val_loss: 1.4113 - val_acc: 0.5421\n",
      "Epoch 843/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9861 - acc: 0.6473 - val_loss: 1.4110 - val_acc: 0.5408\n",
      "Epoch 844/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9896 - acc: 0.6420 - val_loss: 1.4113 - val_acc: 0.5383\n",
      "Epoch 845/1000\n",
      "7048/7048 [==============================] - 0s 53us/step - loss: 1.0557 - acc: 0.6216 - val_loss: 1.4125 - val_acc: 0.5408\n",
      "Epoch 846/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 1.0000 - acc: 0.6410 - val_loss: 1.4143 - val_acc: 0.5408\n",
      "Epoch 847/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9850 - acc: 0.6500 - val_loss: 1.4078 - val_acc: 0.5523\n",
      "Epoch 848/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9787 - acc: 0.6476 - val_loss: 1.4280 - val_acc: 0.5459\n",
      "Epoch 849/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9815 - acc: 0.6433 - val_loss: 1.4159 - val_acc: 0.5421\n",
      "Epoch 850/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9889 - acc: 0.6385 - val_loss: 1.3788 - val_acc: 0.5548\n",
      "Epoch 851/1000\n",
      "7048/7048 [==============================] - 0s 55us/step - loss: 0.9821 - acc: 0.6444 - val_loss: 1.4197 - val_acc: 0.5408\n",
      "Epoch 852/1000\n",
      "7048/7048 [==============================] - 0s 51us/step - loss: 1.0337 - acc: 0.6287 - val_loss: 1.4193 - val_acc: 0.5357\n",
      "Epoch 853/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 1.0096 - acc: 0.6365 - val_loss: 1.4074 - val_acc: 0.5306\n",
      "Epoch 854/1000\n",
      "7048/7048 [==============================] - 0s 51us/step - loss: 0.9921 - acc: 0.6473 - val_loss: 1.4088 - val_acc: 0.5459\n",
      "Epoch 855/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9838 - acc: 0.6433 - val_loss: 1.4007 - val_acc: 0.5383\n",
      "Epoch 856/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 1.0166 - acc: 0.6392 - val_loss: 1.3655 - val_acc: 0.5421\n",
      "Epoch 857/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9889 - acc: 0.6419 - val_loss: 1.4001 - val_acc: 0.5459\n",
      "Epoch 858/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9883 - acc: 0.6420 - val_loss: 1.3887 - val_acc: 0.5332\n",
      "Epoch 859/1000\n",
      "7048/7048 [==============================] - 0s 51us/step - loss: 0.9672 - acc: 0.6495 - val_loss: 1.4108 - val_acc: 0.5319\n",
      "Epoch 860/1000\n",
      "7048/7048 [==============================] - 0s 51us/step - loss: 0.9708 - acc: 0.6515 - val_loss: 1.3920 - val_acc: 0.5357\n",
      "Epoch 861/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9858 - acc: 0.6440 - val_loss: 1.3809 - val_acc: 0.5574\n",
      "Epoch 862/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 1.0397 - acc: 0.6290 - val_loss: 1.3786 - val_acc: 0.5574\n",
      "Epoch 863/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9752 - acc: 0.6470 - val_loss: 1.3874 - val_acc: 0.5446\n",
      "Epoch 864/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9669 - acc: 0.6527 - val_loss: 1.4142 - val_acc: 0.5459\n",
      "Epoch 865/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 1.0323 - acc: 0.6288 - val_loss: 1.3832 - val_acc: 0.5536\n",
      "Epoch 866/1000\n",
      "7048/7048 [==============================] - 0s 51us/step - loss: 0.9949 - acc: 0.6399 - val_loss: 1.3719 - val_acc: 0.5357\n",
      "Epoch 867/1000\n",
      "7048/7048 [==============================] - 0s 51us/step - loss: 0.9772 - acc: 0.6483 - val_loss: 1.4022 - val_acc: 0.5357\n",
      "Epoch 868/1000\n",
      "7048/7048 [==============================] - 0s 51us/step - loss: 0.9832 - acc: 0.6490 - val_loss: 1.4087 - val_acc: 0.5485\n",
      "Epoch 869/1000\n",
      "7048/7048 [==============================] - 0s 52us/step - loss: 1.0062 - acc: 0.6413 - val_loss: 1.3738 - val_acc: 0.5446\n",
      "Epoch 870/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9754 - acc: 0.6484 - val_loss: 1.4259 - val_acc: 0.5357\n",
      "Epoch 871/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9626 - acc: 0.6532 - val_loss: 1.4247 - val_acc: 0.5383\n",
      "Epoch 872/1000\n",
      "7048/7048 [==============================] - 0s 51us/step - loss: 0.9685 - acc: 0.6522 - val_loss: 1.3997 - val_acc: 0.5472\n",
      "Epoch 873/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9806 - acc: 0.6459 - val_loss: 1.4224 - val_acc: 0.5319\n",
      "Epoch 874/1000\n",
      "7048/7048 [==============================] - 0s 56us/step - loss: 0.9744 - acc: 0.6476 - val_loss: 1.3838 - val_acc: 0.5357\n",
      "Epoch 875/1000\n",
      "7048/7048 [==============================] - 0s 67us/step - loss: 0.9845 - acc: 0.6490 - val_loss: 1.4142 - val_acc: 0.5395\n",
      "Epoch 876/1000\n",
      "7048/7048 [==============================] - 0s 52us/step - loss: 0.9752 - acc: 0.6532 - val_loss: 1.4267 - val_acc: 0.5395\n",
      "Epoch 877/1000\n",
      "7048/7048 [==============================] - 0s 52us/step - loss: 0.9717 - acc: 0.6530 - val_loss: 1.4193 - val_acc: 0.5395\n",
      "Epoch 878/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9642 - acc: 0.6501 - val_loss: 1.4099 - val_acc: 0.5459\n",
      "Epoch 879/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9777 - acc: 0.6500 - val_loss: 1.4084 - val_acc: 0.5548\n",
      "Epoch 880/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9853 - acc: 0.6461 - val_loss: 1.3974 - val_acc: 0.5306\n",
      "Epoch 881/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9859 - acc: 0.6434 - val_loss: 1.4092 - val_acc: 0.5421\n",
      "Epoch 882/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9789 - acc: 0.6447 - val_loss: 1.4068 - val_acc: 0.5383\n",
      "Epoch 883/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9778 - acc: 0.6422 - val_loss: 1.4053 - val_acc: 0.5536\n",
      "Epoch 884/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7048/7048 [==============================] - 0s 51us/step - loss: 0.9835 - acc: 0.6510 - val_loss: 1.3991 - val_acc: 0.5306\n",
      "Epoch 885/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9855 - acc: 0.6461 - val_loss: 1.3845 - val_acc: 0.5536\n",
      "Epoch 886/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 1.0195 - acc: 0.6354 - val_loss: 1.3989 - val_acc: 0.5293\n",
      "Epoch 887/1000\n",
      "7048/7048 [==============================] - 0s 51us/step - loss: 0.9885 - acc: 0.6459 - val_loss: 1.3955 - val_acc: 0.5497\n",
      "Epoch 888/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9722 - acc: 0.6419 - val_loss: 1.4030 - val_acc: 0.5395\n",
      "Epoch 889/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9819 - acc: 0.6493 - val_loss: 1.4007 - val_acc: 0.5485\n",
      "Epoch 890/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9777 - acc: 0.6459 - val_loss: 1.4001 - val_acc: 0.5408\n",
      "Epoch 891/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9695 - acc: 0.6524 - val_loss: 1.4174 - val_acc: 0.5395\n",
      "Epoch 892/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9948 - acc: 0.6446 - val_loss: 1.4035 - val_acc: 0.5472\n",
      "Epoch 893/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9768 - acc: 0.6409 - val_loss: 1.4121 - val_acc: 0.5344\n",
      "Epoch 894/1000\n",
      "7048/7048 [==============================] - 0s 51us/step - loss: 0.9904 - acc: 0.6459 - val_loss: 1.3989 - val_acc: 0.5510\n",
      "Epoch 895/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9812 - acc: 0.6501 - val_loss: 1.3848 - val_acc: 0.5548\n",
      "Epoch 896/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9695 - acc: 0.6505 - val_loss: 1.4004 - val_acc: 0.5293\n",
      "Epoch 897/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9971 - acc: 0.6420 - val_loss: 1.3783 - val_acc: 0.5497\n",
      "Epoch 898/1000\n",
      "7048/7048 [==============================] - 0s 56us/step - loss: 0.9881 - acc: 0.6451 - val_loss: 1.4037 - val_acc: 0.5472\n",
      "Epoch 899/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9802 - acc: 0.6494 - val_loss: 1.4016 - val_acc: 0.5536\n",
      "Epoch 900/1000\n",
      "7048/7048 [==============================] - 0s 56us/step - loss: 0.9890 - acc: 0.6463 - val_loss: 1.4094 - val_acc: 0.5383\n",
      "Epoch 901/1000\n",
      "7048/7048 [==============================] - 0s 51us/step - loss: 0.9700 - acc: 0.6494 - val_loss: 1.3999 - val_acc: 0.5446\n",
      "Epoch 902/1000\n",
      "7048/7048 [==============================] - 0s 56us/step - loss: 0.9874 - acc: 0.6442 - val_loss: 1.3747 - val_acc: 0.5497\n",
      "Epoch 903/1000\n",
      "7048/7048 [==============================] - 0s 62us/step - loss: 0.9742 - acc: 0.6522 - val_loss: 1.4162 - val_acc: 0.5446\n",
      "Epoch 904/1000\n",
      "7048/7048 [==============================] - 0s 51us/step - loss: 0.9641 - acc: 0.6542 - val_loss: 1.4393 - val_acc: 0.5434\n",
      "Epoch 905/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9778 - acc: 0.6521 - val_loss: 1.3968 - val_acc: 0.5548\n",
      "Epoch 906/1000\n",
      "7048/7048 [==============================] - 0s 51us/step - loss: 0.9618 - acc: 0.6514 - val_loss: 1.4116 - val_acc: 0.5561\n",
      "Epoch 907/1000\n",
      "7048/7048 [==============================] - 0s 51us/step - loss: 0.9683 - acc: 0.6503 - val_loss: 1.4092 - val_acc: 0.5536\n",
      "Epoch 908/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9736 - acc: 0.6507 - val_loss: 1.4199 - val_acc: 0.5421\n",
      "Epoch 909/1000\n",
      "7048/7048 [==============================] - 0s 52us/step - loss: 0.9830 - acc: 0.6454 - val_loss: 1.4032 - val_acc: 0.5497\n",
      "Epoch 910/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9882 - acc: 0.6456 - val_loss: 1.4003 - val_acc: 0.5459\n",
      "Epoch 911/1000\n",
      "7048/7048 [==============================] - 0s 51us/step - loss: 0.9740 - acc: 0.6474 - val_loss: 1.4181 - val_acc: 0.5536\n",
      "Epoch 912/1000\n",
      "7048/7048 [==============================] - 0s 52us/step - loss: 0.9664 - acc: 0.6494 - val_loss: 1.4335 - val_acc: 0.5485\n",
      "Epoch 913/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9776 - acc: 0.6515 - val_loss: 1.3885 - val_acc: 0.5268\n",
      "Epoch 914/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9743 - acc: 0.6444 - val_loss: 1.3830 - val_acc: 0.5332\n",
      "Epoch 915/1000\n",
      "7048/7048 [==============================] - 0s 51us/step - loss: 0.9936 - acc: 0.6437 - val_loss: 1.4021 - val_acc: 0.5485\n",
      "Epoch 916/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9682 - acc: 0.6518 - val_loss: 1.4163 - val_acc: 0.5421\n",
      "Epoch 917/1000\n",
      "7048/7048 [==============================] - 0s 51us/step - loss: 0.9716 - acc: 0.6461 - val_loss: 1.4103 - val_acc: 0.5434\n",
      "Epoch 918/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9855 - acc: 0.6517 - val_loss: 1.4124 - val_acc: 0.5434\n",
      "Epoch 919/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9886 - acc: 0.6425 - val_loss: 1.4149 - val_acc: 0.5485\n",
      "Epoch 920/1000\n",
      "7048/7048 [==============================] - 0s 51us/step - loss: 0.9971 - acc: 0.6417 - val_loss: 1.3997 - val_acc: 0.5434\n",
      "Epoch 921/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 1.0647 - acc: 0.6263 - val_loss: 1.3967 - val_acc: 0.5383\n",
      "Epoch 922/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9943 - acc: 0.6416 - val_loss: 1.4223 - val_acc: 0.5434\n",
      "Epoch 923/1000\n",
      "7048/7048 [==============================] - 0s 51us/step - loss: 0.9961 - acc: 0.6400 - val_loss: 1.3847 - val_acc: 0.5497\n",
      "Epoch 924/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 1.0039 - acc: 0.6400 - val_loss: 1.4196 - val_acc: 0.5561\n",
      "Epoch 925/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9886 - acc: 0.6434 - val_loss: 1.4211 - val_acc: 0.5459\n",
      "Epoch 926/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9875 - acc: 0.6427 - val_loss: 1.4299 - val_acc: 0.5408\n",
      "Epoch 927/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9680 - acc: 0.6497 - val_loss: 1.4326 - val_acc: 0.5357\n",
      "Epoch 928/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9647 - acc: 0.6498 - val_loss: 1.4358 - val_acc: 0.5459\n",
      "Epoch 929/1000\n",
      "7048/7048 [==============================] - 0s 51us/step - loss: 0.9673 - acc: 0.6511 - val_loss: 1.4422 - val_acc: 0.5446\n",
      "Epoch 930/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9688 - acc: 0.6555 - val_loss: 1.4400 - val_acc: 0.5332\n",
      "Epoch 931/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9794 - acc: 0.6491 - val_loss: 1.4350 - val_acc: 0.5357\n",
      "Epoch 932/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9815 - acc: 0.6494 - val_loss: 1.3898 - val_acc: 0.5510\n",
      "Epoch 933/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9633 - acc: 0.6555 - val_loss: 1.3984 - val_acc: 0.5370\n",
      "Epoch 934/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9800 - acc: 0.6466 - val_loss: 1.3875 - val_acc: 0.5395\n",
      "Epoch 935/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9817 - acc: 0.6451 - val_loss: 1.4252 - val_acc: 0.5395\n",
      "Epoch 936/1000\n",
      "7048/7048 [==============================] - ETA: 0s - loss: 0.9670 - acc: 0.648 - 0s 50us/step - loss: 0.9667 - acc: 0.6487 - val_loss: 1.4262 - val_acc: 0.5357\n",
      "Epoch 937/1000\n",
      "7048/7048 [==============================] - 0s 51us/step - loss: 0.9967 - acc: 0.6426 - val_loss: 1.4453 - val_acc: 0.5255\n",
      "Epoch 938/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9765 - acc: 0.6487 - val_loss: 1.4178 - val_acc: 0.5268\n",
      "Epoch 939/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9753 - acc: 0.6504 - val_loss: 1.4162 - val_acc: 0.5395\n",
      "Epoch 940/1000\n",
      "7048/7048 [==============================] - 0s 51us/step - loss: 0.9706 - acc: 0.6480 - val_loss: 1.4102 - val_acc: 0.5344\n",
      "Epoch 941/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9713 - acc: 0.6477 - val_loss: 1.4090 - val_acc: 0.5268\n",
      "Epoch 942/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9813 - acc: 0.6517 - val_loss: 1.4036 - val_acc: 0.5344\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 943/1000\n",
      "7048/7048 [==============================] - 0s 51us/step - loss: 0.9687 - acc: 0.6539 - val_loss: 1.3973 - val_acc: 0.5561\n",
      "Epoch 944/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9639 - acc: 0.6495 - val_loss: 1.4116 - val_acc: 0.5497\n",
      "Epoch 945/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9665 - acc: 0.6511 - val_loss: 1.4298 - val_acc: 0.5497\n",
      "Epoch 946/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9598 - acc: 0.6512 - val_loss: 1.4237 - val_acc: 0.5344\n",
      "Epoch 947/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9684 - acc: 0.6481 - val_loss: 1.4226 - val_acc: 0.5446\n",
      "Epoch 948/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9779 - acc: 0.6451 - val_loss: 1.4242 - val_acc: 0.5332\n",
      "Epoch 949/1000\n",
      "7048/7048 [==============================] - 0s 53us/step - loss: 0.9755 - acc: 0.6459 - val_loss: 1.4413 - val_acc: 0.5459\n",
      "Epoch 950/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9716 - acc: 0.6471 - val_loss: 1.3980 - val_acc: 0.5357\n",
      "Epoch 951/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9796 - acc: 0.6481 - val_loss: 1.3971 - val_acc: 0.5370\n",
      "Epoch 952/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9678 - acc: 0.6527 - val_loss: 1.4408 - val_acc: 0.5344\n",
      "Epoch 953/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9740 - acc: 0.6527 - val_loss: 1.4226 - val_acc: 0.5434\n",
      "Epoch 954/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9638 - acc: 0.6548 - val_loss: 1.3863 - val_acc: 0.5408\n",
      "Epoch 955/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9731 - acc: 0.6443 - val_loss: 1.4081 - val_acc: 0.5408\n",
      "Epoch 956/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9778 - acc: 0.6522 - val_loss: 1.4075 - val_acc: 0.5434\n",
      "Epoch 957/1000\n",
      "7048/7048 [==============================] - 0s 51us/step - loss: 0.9788 - acc: 0.6494 - val_loss: 1.4194 - val_acc: 0.5293\n",
      "Epoch 958/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9877 - acc: 0.6442 - val_loss: 1.4011 - val_acc: 0.5408\n",
      "Epoch 959/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9689 - acc: 0.6524 - val_loss: 1.4017 - val_acc: 0.5395\n",
      "Epoch 960/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9761 - acc: 0.6491 - val_loss: 1.4023 - val_acc: 0.5408\n",
      "Epoch 961/1000\n",
      "7048/7048 [==============================] - 0s 51us/step - loss: 0.9672 - acc: 0.6508 - val_loss: 1.3986 - val_acc: 0.5485\n",
      "Epoch 962/1000\n",
      "7048/7048 [==============================] - 0s 51us/step - loss: 0.9828 - acc: 0.6494 - val_loss: 1.3925 - val_acc: 0.5357\n",
      "Epoch 963/1000\n",
      "7048/7048 [==============================] - 0s 54us/step - loss: 0.9724 - acc: 0.6551 - val_loss: 1.4014 - val_acc: 0.5536\n",
      "Epoch 964/1000\n",
      "7048/7048 [==============================] - 0s 59us/step - loss: 0.9813 - acc: 0.6443 - val_loss: 1.3887 - val_acc: 0.5395\n",
      "Epoch 965/1000\n",
      "7048/7048 [==============================] - 0s 56us/step - loss: 0.9729 - acc: 0.6532 - val_loss: 1.4001 - val_acc: 0.5370\n",
      "Epoch 966/1000\n",
      "7048/7048 [==============================] - 0s 61us/step - loss: 0.9645 - acc: 0.6548 - val_loss: 1.3961 - val_acc: 0.5408\n",
      "Epoch 967/1000\n",
      "7048/7048 [==============================] - 0s 54us/step - loss: 0.9529 - acc: 0.6528 - val_loss: 1.4284 - val_acc: 0.5485\n",
      "Epoch 968/1000\n",
      "7048/7048 [==============================] - 0s 52us/step - loss: 0.9734 - acc: 0.6505 - val_loss: 1.3934 - val_acc: 0.5395\n",
      "Epoch 969/1000\n",
      "7048/7048 [==============================] - 0s 51us/step - loss: 0.9611 - acc: 0.6524 - val_loss: 1.4176 - val_acc: 0.5408\n",
      "Epoch 970/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9596 - acc: 0.6518 - val_loss: 1.4033 - val_acc: 0.5510\n",
      "Epoch 971/1000\n",
      "7048/7048 [==============================] - 0s 51us/step - loss: 0.9780 - acc: 0.6494 - val_loss: 1.3965 - val_acc: 0.5434\n",
      "Epoch 972/1000\n",
      "7048/7048 [==============================] - 0s 51us/step - loss: 0.9678 - acc: 0.6542 - val_loss: 1.4272 - val_acc: 0.5459\n",
      "Epoch 973/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9738 - acc: 0.6497 - val_loss: 1.4124 - val_acc: 0.5446\n",
      "Epoch 974/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9774 - acc: 0.6530 - val_loss: 1.3987 - val_acc: 0.5548\n",
      "Epoch 975/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9603 - acc: 0.6503 - val_loss: 1.4044 - val_acc: 0.5344\n",
      "Epoch 976/1000\n",
      "7048/7048 [==============================] - 0s 51us/step - loss: 0.9608 - acc: 0.6588 - val_loss: 1.4397 - val_acc: 0.5421\n",
      "Epoch 977/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9620 - acc: 0.6579 - val_loss: 1.4237 - val_acc: 0.5472\n",
      "Epoch 978/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 1.0276 - acc: 0.6305 - val_loss: 1.3989 - val_acc: 0.5459\n",
      "Epoch 979/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 1.0448 - acc: 0.6243 - val_loss: 1.4044 - val_acc: 0.5370\n",
      "Epoch 980/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 1.0061 - acc: 0.6356 - val_loss: 1.4080 - val_acc: 0.5510\n",
      "Epoch 981/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9957 - acc: 0.6332 - val_loss: 1.4092 - val_acc: 0.5434\n",
      "Epoch 982/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 1.0198 - acc: 0.6337 - val_loss: 1.3944 - val_acc: 0.5332\n",
      "Epoch 983/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9788 - acc: 0.6454 - val_loss: 1.4270 - val_acc: 0.5459\n",
      "Epoch 984/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9717 - acc: 0.6517 - val_loss: 1.4356 - val_acc: 0.5383\n",
      "Epoch 985/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9972 - acc: 0.6415 - val_loss: 1.4151 - val_acc: 0.5472\n",
      "Epoch 986/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9662 - acc: 0.6541 - val_loss: 1.4152 - val_acc: 0.5434\n",
      "Epoch 987/1000\n",
      "7048/7048 [==============================] - 0s 51us/step - loss: 0.9670 - acc: 0.6493 - val_loss: 1.3967 - val_acc: 0.5485\n",
      "Epoch 988/1000\n",
      "7048/7048 [==============================] - 0s 52us/step - loss: 0.9580 - acc: 0.6569 - val_loss: 1.4092 - val_acc: 0.5383\n",
      "Epoch 989/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9861 - acc: 0.6417 - val_loss: 1.4177 - val_acc: 0.5370\n",
      "Epoch 990/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9995 - acc: 0.6410 - val_loss: 1.3990 - val_acc: 0.5548\n",
      "Epoch 991/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9715 - acc: 0.6514 - val_loss: 1.4178 - val_acc: 0.5523\n",
      "Epoch 992/1000\n",
      "7048/7048 [==============================] - 0s 58us/step - loss: 0.9656 - acc: 0.6535 - val_loss: 1.4082 - val_acc: 0.5383\n",
      "Epoch 993/1000\n",
      "7048/7048 [==============================] - 0s 61us/step - loss: 0.9742 - acc: 0.6507 - val_loss: 1.3871 - val_acc: 0.5472\n",
      "Epoch 994/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9811 - acc: 0.6491 - val_loss: 1.3877 - val_acc: 0.5204\n",
      "Epoch 995/1000\n",
      "7048/7048 [==============================] - 0s 51us/step - loss: 0.9809 - acc: 0.6480 - val_loss: 1.4121 - val_acc: 0.5497\n",
      "Epoch 996/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9635 - acc: 0.6511 - val_loss: 1.4067 - val_acc: 0.5395\n",
      "Epoch 997/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9645 - acc: 0.6582 - val_loss: 1.4098 - val_acc: 0.5434\n",
      "Epoch 998/1000\n",
      "7048/7048 [==============================] - 0s 52us/step - loss: 0.9560 - acc: 0.6610 - val_loss: 1.4029 - val_acc: 0.5446\n",
      "Epoch 999/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9711 - acc: 0.6498 - val_loss: 1.3937 - val_acc: 0.5523\n",
      "Epoch 1000/1000\n",
      "7048/7048 [==============================] - 0s 50us/step - loss: 0.9638 - acc: 0.6545 - val_loss: 1.4152 - val_acc: 0.5548\n"
     ]
    }
   ],
   "source": [
    "history = model.fit( X_train, Y_train, epochs = 1000, batch_size=128, validation_split=0.1, verbose = 1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1958/1958 [==============================] - 0s 67us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.4720978298518461, 0.5306435138200228]"
      ]
     },
     "execution_count": 562,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['AGG ASSAULT', 'AUTO THEFT', 'BURGLARY-NONRES',\n",
       "       'BURGLARY-RESIDENCE', 'HOMICIDE', 'LARCENY-FROM VEHICLE',\n",
       "       'LARCENY-NON VEHICLE', 'MANSLAUGHTER', 'ROBBERY-COMMERCIAL',\n",
       "       'ROBBERY-PEDESTRIAN', 'ROBBERY-RESIDENCE'], dtype=object)"
      ]
     },
     "execution_count": 650,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 669,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])"
      ]
     },
     "execution_count": 669,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying Sample Input to get the probability of each crime category\n",
    "Testing our model output with sample input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 668,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of AGG ASSAULT: 50%\n",
      "Probability of AUTO THEFT: 12%\n",
      "Probability of BURGLARY-NONRES: 0%\n",
      "Probability of BURGLARY-RESIDENCE: 0%\n",
      "Probability of HOMICIDE: 2%\n",
      "Probability of LARCENY-FROM VEHICLE: 13%\n",
      "Probability of LARCENY-NON VEHICLE: 4%\n",
      "Probability of MANSLAUGHTER: 0%\n",
      "Probability of ROBBERY-COMMERCIAL: 0%\n",
      "Probability of ROBBERY-PEDESTRIAN: 15%\n",
      "Probability of ROBBERY-RESIDENCE: 0%\n"
     ]
    }
   ],
   "source": [
    "from math import floor \n",
    "\n",
    "test = np.array([[-0.4121499 , 0.06029446, 0.95146566, -0.92644004, -0.28378506, -0.99773083, -0.356867 , -0.65815827, 1.20975734, -0.60228777, -0.16221292, -0.03503205, -0.0285977 , -0.07019344, -0.18794012, 2.46034837, -0.11509898, -0.0101072 , -0.01429447, -0.05059806, -0.57396909, -0.08111905, -0.04045981, -0.5 , -0.15476262, -0.21847463, -0.18382306, -0.33691584, -0.0429185 , -0.05450695, -0.09302925, -0.0202175 , -0.07236489, -0.0202175 , -0.01429447, -0.05059806, -0.0101072 , -0.05355618, -0.10209893, -0.06719131, -0.11509898, -0.2015037 , -0.0751646 , -0.21459713, -0.0798333 , -0.05636093, -0.10412256, -0.03503205, -0.0202175 , -0.0429185 , -0.0101072 , -0.05059806, -0.0202175 , -0.03353892, -0.0285977 , -0.06485029, -0.0101072 , -0.01750797, -0.06795405, -0.09898922, -0.01429447, -0.01429447, -0.02476381, -0.01750797, -0.07164824, -0.03784281, -0.0285977 , -0.0474579 , -0.0202175 , -0.0101072 , -0.01429447, -0.03646438, -0.04409682, -0.12558227, -0.03503205, -0.0452447 , -0.0952381 , -0.06795405, -0.0101072 , -0.01750797, -0.01429447, -0.0202175 , -0.0202175 , -0.0285977 , -0.022605 , -0.03646438, -0.03197647, -0.03784281, -0.06945485, -0.09076855, -0.03917304, -0.09076855, -0.03353892, -0.0101072 , -0.05450695, -0.0952381 , -0.07307466, -0.0429185 , -0.07652689, -0.0429185 , -0.05059806, -0.03646438, -0.03503205, -0.07164824, -0.0101072 , -0.0285977 , -0.11600671, -0.0101072 , -0.07377774, -0.0101072 , -0.04852699, -0.03197647, -0.0572657 , -0.08111905, -0.03197647, -0.02476381, -0.02674934, -0.06485029, -0.02476381, -0.0607519 , -0.29668091, -0.04409682, -0.01429447, -0.1031155 , -0.04409682, -0.06945485, -0.13253581, -0.0101072 , -0.0285977 , -0.10158699, -0.022605 , -0.022605 , -0.0285977 , -0.06324231, -0.01429447, -0.022605 , -0.06485029, -0.01429447, -0.03197647, -0.07447433, -0.09190561, -0.1288982 , -0.01429447, -0.02476381, -0.12175082, -0.0202175 , -0.12218211, -0.08903662, -0.0202175 , -0.0836328 , -0.04409682, -0.01429447, -0.03197647, -0.022605 , -0.04636439, -0.12766418, -0.01429447, -0.07719923, -0.12131808, -0.03197647, -0.0101072 , -0.0452447 , -0.01429447, -0.07719923, -0.03197647, -0.05059806, -0.0101072 , -0.0285977 , -0.0952381 , -0.0202175 , -0.04852699, -0.1433032 , -0.03784281, -0.15954003, -0.10854367, -0.08111905, -0.0101072 , -0.02476381, -0.08175454, -0.03503205, -0.01750797, 8.07147128, -0.01429447, -0.01750797, -0.26973904, -0.01429447, -0.03784281, -0.10902433, -0.06242305, -0.0101072 , -0.0101072 , -0.0101072 , -0.13954598, -0.0452447 , -0.0101072 , -0.03503205, -0.1031155 , -0.16744367, -0.03197647, -0.03503205, -0.0709245 , -0.05258843, -0.022605 , -0.01750797, -0.05059806, -0.05059806, -0.05160274, -0.03353892, -0.08111905, -0.09469054, -0.0202175 , -0.08547092, -0.06242305, -0.10950298, -0.04045981, -0.02674934, -0.08903662, -0.09578263, -0.03646438, -0.03646438, -0.0101072 , -0.08301131, -0.01429447, -0.03197647, -0.0202175 , -0.0572657 , -0.02674934, -0.0429185 , -0.01750797, -0.0554416 , -0.0101072 , -0.07307466, -0.07786589, -0.06870847, -0.09632419, -0.0101072 , -0.07447433, -0.02476381, -0.10055562, -0.11957238, -0.01750797, -0.0709245 , -0.030334 , -0.05450695, -0.08607515, -0.030334 , -0.08301131, -0.0866753 , -0.1444128 , -0.11186698, -0.05355618, -0.0202175 , -0.15232315, -0.05258843, -0.04957324, -0.01750797, -0.0452447 , -0.0202175 , -0.01429447, -0.01750797, -0.1000361 , -0.03503205, -0.0101072 , -0.03503205, -0.06641998, -0.030334 , -0.03784281, -0.0285977 , -0.03784281, -0.0101072 ]])\n",
    "test = scaler.transform(test)\n",
    "ans = model.predict(test)\n",
    "\n",
    "for i, n in enumerate(ans[0]):\n",
    "  print(\"Probability of \" + str(labels[i]) + \": \" + str(floor(n*100)) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classification Model\n",
    "Trying Random forest classsifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 714,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df[['Latitude','Longitude','Shift Occurrence','Location Type','Neighborhood','Hour','Minute','Month','DayOfWeek','Day']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 715,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\IKNOOR\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\Users\\IKNOOR\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "df2['Shift Occurrence']=le.fit_transform(df2['Shift Occurrence']) \n",
    "df2['Neighborhood']=le.fit_transform(df2['Neighborhood']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 716,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Shift Occurrence</th>\n",
       "      <th>Location Type</th>\n",
       "      <th>Neighborhood</th>\n",
       "      <th>Hour</th>\n",
       "      <th>Minute</th>\n",
       "      <th>Month</th>\n",
       "      <th>DayOfWeek</th>\n",
       "      <th>Day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>33.75194</td>\n",
       "      <td>-84.38964</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>76</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33.68077</td>\n",
       "      <td>-84.49370</td>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>100</td>\n",
       "      <td>17</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Latitude  Longitude  Shift Occurrence Location Type  Neighborhood  Hour  \\\n",
       "0  33.75194  -84.38964                 2            13            76     0   \n",
       "2  33.68077  -84.49370                 1            26           100    17   \n",
       "\n",
       "   Minute  Month  DayOfWeek  Day  \n",
       "0      20      1          1    1  \n",
       "2      40      1          1    1  "
      ]
     },
     "execution_count": 716,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 717,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(df2, integer_encoded, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 718,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6],\n",
       "       [6],\n",
       "       [6],\n",
       "       ...,\n",
       "       [4],\n",
       "       [0],\n",
       "       [2]])"
      ]
     },
     "execution_count": 718,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "integer_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 719,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Shift Occurrence</th>\n",
       "      <th>Location Type</th>\n",
       "      <th>Neighborhood</th>\n",
       "      <th>Hour</th>\n",
       "      <th>Minute</th>\n",
       "      <th>Month</th>\n",
       "      <th>DayOfWeek</th>\n",
       "      <th>Day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>33.75194</td>\n",
       "      <td>-84.38964</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>76</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33.68077</td>\n",
       "      <td>-84.49370</td>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>100</td>\n",
       "      <td>17</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33.71744</td>\n",
       "      <td>-84.36818</td>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>28</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Latitude  Longitude  Shift Occurrence Location Type  Neighborhood  Hour  \\\n",
       "0  33.75194  -84.38964                 2            13            76     0   \n",
       "2  33.68077  -84.49370                 1            26           100    17   \n",
       "3  33.71744  -84.36818                 2            23            28     4   \n",
       "\n",
       "   Minute  Month  DayOfWeek  Day  \n",
       "0      20      1          1    1  \n",
       "2      40      1          1    1  \n",
       "3      15      1          1    1  "
      ]
     },
     "execution_count": 719,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 720,
   "metadata": {},
   "outputs": [],
   "source": [
    "trees = 1000\n",
    "depth = 100\n",
    "clf = RandomForestClassifier(n_estimators=trees, max_depth=depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training on 100 trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 721,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\IKNOOR\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=100, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=1000, n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 721,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 722,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5],\n",
       "       [5],\n",
       "       [5],\n",
       "       ...,\n",
       "       [6],\n",
       "       [5],\n",
       "       [3]])"
      ]
     },
     "execution_count": 722,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 723,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 0, ..., 6, 0, 5])"
      ]
     },
     "execution_count": 723,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peds = clf.predict(X_test)\n",
    "peds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 724,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5510725229826353"
      ]
     },
     "execution_count": 724,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying outher models and calculating test accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Light GBM Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 728,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\IKNOOR\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:219: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\IKNOOR\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:252: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "model =LGBMClassifier(objective='multiclass', num_class=39).fit(X_train.values, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 729,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5510725229826353"
      ]
     },
     "execution_count": 729,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Gradient Boosting Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\IKNOOR\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=10,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "              n_iter_no_change=None, presort='auto', random_state=None,\n",
       "              subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "              verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 617,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # build gradient boosting trees model\n",
    "trees = 100\n",
    "depth = 10\n",
    "clf = GradientBoostingClassifier(n_estimators=trees, max_depth=depth)\n",
    "clf.fit(X_train, Y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5398365679264555"
      ]
     },
     "execution_count": 618,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) K-Nearest Neighbour Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 670,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df[['Beat','NPU','UCR #','IBR Code','Latitude','Longitude','Shift Occurrence','Location Type','Neighborhood','Hour','Minute','Month','DayOfWeek','Day']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 671,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Shift Occurrence</th>\n",
       "      <th>Neighborhood</th>\n",
       "      <th>Hour</th>\n",
       "      <th>DayOfWeek</th>\n",
       "      <th>Day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>33.75194</td>\n",
       "      <td>-84.38964</td>\n",
       "      <td>Morning Watch</td>\n",
       "      <td>Downtown</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Latitude  Longitude Shift Occurrence Neighborhood  Hour  DayOfWeek  Day\n",
       "0  33.75194  -84.38964    Morning Watch     Downtown     0          1    1"
      ]
     },
     "execution_count": 671,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 672,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3=pd.get_dummies(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 673,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Hour</th>\n",
       "      <th>DayOfWeek</th>\n",
       "      <th>Day</th>\n",
       "      <th>Shift Occurrence_Day Watch</th>\n",
       "      <th>Shift Occurrence_Evening Watch</th>\n",
       "      <th>Shift Occurrence_Morning Watch</th>\n",
       "      <th>Shift Occurrence_Unknown</th>\n",
       "      <th>Neighborhood_Adair Park</th>\n",
       "      <th>...</th>\n",
       "      <th>Neighborhood_Westview</th>\n",
       "      <th>Neighborhood_Westwood Terrace</th>\n",
       "      <th>Neighborhood_Whitewater Creek</th>\n",
       "      <th>Neighborhood_Whittier Mill Village</th>\n",
       "      <th>Neighborhood_Wildwood (NPU-C)</th>\n",
       "      <th>Neighborhood_Wildwood (NPU-H)</th>\n",
       "      <th>Neighborhood_Wilson Mill Meadows</th>\n",
       "      <th>Neighborhood_Wisteria Gardens</th>\n",
       "      <th>Neighborhood_Woodland Hills</th>\n",
       "      <th>Neighborhood_Wyngate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>33.75194</td>\n",
       "      <td>-84.38964</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 235 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Latitude  Longitude  Hour  DayOfWeek  Day  Shift Occurrence_Day Watch  \\\n",
       "0  33.75194  -84.38964     0          1    1                           0   \n",
       "\n",
       "   Shift Occurrence_Evening Watch  Shift Occurrence_Morning Watch  \\\n",
       "0                               0                               1   \n",
       "\n",
       "   Shift Occurrence_Unknown  Neighborhood_Adair Park  ...  \\\n",
       "0                         0                        0  ...   \n",
       "\n",
       "   Neighborhood_Westview  Neighborhood_Westwood Terrace  \\\n",
       "0                      0                              0   \n",
       "\n",
       "   Neighborhood_Whitewater Creek  Neighborhood_Whittier Mill Village  \\\n",
       "0                              0                                   0   \n",
       "\n",
       "   Neighborhood_Wildwood (NPU-C)  Neighborhood_Wildwood (NPU-H)  \\\n",
       "0                              0                              0   \n",
       "\n",
       "   Neighborhood_Wilson Mill Meadows  Neighborhood_Wisteria Gardens  \\\n",
       "0                                 0                              0   \n",
       "\n",
       "   Neighborhood_Woodland Hills  Neighborhood_Wyngate  \n",
       "0                            0                     0  \n",
       "\n",
       "[1 rows x 235 columns]"
      ]
     },
     "execution_count": 673,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 674,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\IKNOOR\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:645: DataConversionWarning: Data with input dtype uint8, int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StandardScaler(copy=True, with_mean=True, with_std=True)"
      ]
     },
     "execution_count": 674,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 675,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\IKNOOR\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DataConversionWarning: Data with input dtype uint8, int64, float64 were all converted to float64 by StandardScaler.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df3=scaler.transform(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 676,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.21172121,  0.2868266 , -2.00717897, ..., -0.0285977 ,\n",
       "        -0.03784281, -0.0101072 ],\n",
       "       [-1.78617048, -2.05873898,  0.50766897, ..., -0.0285977 ,\n",
       "        -0.03784281, -0.0101072 ],\n",
       "       [-0.97494307,  0.77054597, -1.41545005, ..., -0.0285977 ,\n",
       "        -0.03784281, -0.0101072 ],\n",
       "       ...,\n",
       "       [ 0.91165288, -0.9258529 ,  0.2118045 , ..., -0.0285977 ,\n",
       "        -0.03784281, -0.0101072 ],\n",
       "       [-1.61472209, -3.01152638, -1.41545005, ..., -0.0285977 ,\n",
       "        -0.03784281, -0.0101072 ],\n",
       "       [ 0.77051214,  0.73944006, -1.71131451, ..., -0.0285977 ,\n",
       "        -0.03784281, -0.0101072 ]])"
      ]
     },
     "execution_count": 676,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 677,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(df3, integer_encoded, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 678,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 678,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "classifier = KNeighborsClassifier()\n",
    "classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying different values of of n_neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_scores = []\n",
    "test_scores = []\n",
    "for k in range(1, 10, 2):\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train, Y_train)\n",
    "    train_score = knn.score(X_train, Y_train)\n",
    "    test_score = knn.score(X_test, Y_test)\n",
    "    train_scores.append(train_score)\n",
    "    test_scores.append(test_score)\n",
    "    \n",
    "plt.plot(range(1, 10, 2), train_scores, marker='o')\n",
    "plt.plot(range(1, 10, 2), test_scores, marker=\"x\")\n",
    "plt.xlabel(\"k neighbors\")\n",
    "plt.ylabel(\"Testing Accuracy Score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
